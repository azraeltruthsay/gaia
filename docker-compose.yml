# GAIA Service-Oriented Architecture - Master Composition
#
# Services:
#   - gaia-core:  The Brain   - Cognitive loop and reasoning (CPU, delegates to gaia-prime)
#   - gaia-prime: The Voice   - Standalone vLLM inference server (GPU)
#   - gaia-web:   The Face    - UI and API gateway
#   - gaia-study: The Subconscious - Background processing (GPU, SOLE WRITER)
#   - gaia-mcp:   The Hands   - Sandboxed tool execution
#
# Volume Access:
#   - knowledge/:       RO for all, RW for gaia-study
#   - vector_store/:    RO for gaia-core, RW for gaia-study (SOLE WRITER)
#   - gaia-models/:     RO for gaia-core, RW for gaia-study (LoRA adapters)
#   - gaia-common/:     RO for all (shared library)
#
# Usage:
#   docker compose up -d              # Start all services
#   docker compose up gaia-web        # Start web + dependencies
#   docker compose logs -f gaia-core  # Follow core logs
#   docker compose down               # Stop all services
#
# Modular Swap (inject candidate into live flow):
#   # Swap MCP: live core -> candidate MCP
#   MCP_ENDPOINT=http://gaia-mcp-candidate:8765/jsonrpc docker compose up -d gaia-core
#
#   # Swap Core: live web -> candidate core
#   CORE_ENDPOINT=http://gaia-core-candidate:6415 docker compose up -d gaia-web
#
#   # Swap Study: live core -> candidate study
#   STUDY_ENDPOINT=http://gaia-study-candidate:8766 docker compose up -d gaia-core

services:
  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-orchestrator: The Coordinator - GPU and container lifecycle management
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-orchestrator:
    build:
      context: .
      dockerfile: ./gaia-orchestrator/Dockerfile
    image: gaia-orchestrator:latest
    container_name: gaia-orchestrator
    hostname: gaia-orchestrator
    restart: unless-stopped

    volumes:
      # Docker socket for container management
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Shared state for persistence
      - gaia-shared:/shared:rw
      # Project root for compose file access (read-only)
      - .:/gaia/GAIA_Project:ro

    environment:
      - ORCHESTRATOR_HOST=0.0.0.0
      - ORCHESTRATOR_PORT=6410
      - ORCHESTRATOR_STATE_DIR=/shared/orchestrator
      - ORCHESTRATOR_COMPOSE_FILE_LIVE=/gaia/GAIA_Project/docker-compose.yml
      - ORCHESTRATOR_COMPOSE_FILE_CANDIDATE=/gaia/GAIA_Project/docker-compose.candidate.yml
      # Service endpoints (inside Docker network)
      - ORCHESTRATOR_CORE_URL=http://gaia-core:6415
      - ORCHESTRATOR_WEB_URL=http://gaia-web:6414
      - ORCHESTRATOR_STUDY_URL=http://gaia-study:8766
      - ORCHESTRATOR_MCP_URL=http://gaia-mcp:8765
      - ORCHESTRATOR_PRIME_URL=http://gaia-prime:7777
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "6410:6410"

    networks:
      - gaia-net

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6410/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-prime: The Voice - Standalone vLLM inference server (GPU)
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-prime:
    build:
      context: ./gaia-prime
      dockerfile: Dockerfile
    image: localhost:5000/gaia-prime:local
    container_name: gaia-prime
    hostname: gaia-prime
    restart: unless-stopped

    volumes:
      - /mnt/gaia_warm_pool:/models:ro

    environment:
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_FLASH_ATTN_VERSION=2
      - TORCH_CUDA_ARCH_LIST=12.0+PTX
      # Required for vLLM sleep mode (/sleep and /wake_up endpoints)
      - VLLM_SERVER_DEV_MODE=1

    ports:
      - "7777:7777"

    command: >
      python -m vllm.entrypoints.openai.api_server
      --model ${PRIME_MODEL_PATH:-/models/Qwen3-8B-AWQ}
      --host 0.0.0.0
      --port 7777
      --gpu-memory-utilization 0.70
      --max-model-len 8192
      --max-num-seqs 4
      --trust-remote-code
      --dtype float16
      --quantization awq
      --enforce-eager
      --enable-lora
      --max-loras 4
      --max-lora-rank 64
      --enable-sleep-mode
      --enable-prefix-caching
      --kv-offloading-backend native
      --kv-offloading-size 8
      --disable-hybrid-kv-cache-manager

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7777/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

    networks:
      - gaia-net

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-core: The Brain - Cognitive loop and reasoning (CPU, delegates to gaia-prime)
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-core:
    build:
      context: .
      dockerfile: ./gaia-core/Dockerfile
    image: localhost:5000/gaia-core:local
    container_name: gaia-core
    hostname: gaia-core
    restart: unless-stopped
    user: "${UID:-1000}:${GID:-1000}"

    volumes:
      # Source code (development mode)
      - ./gaia-core:/app:rw
      # Shared library (development mode - mount for live reload)
      - ./gaia-common:/gaia-common:ro
      # Knowledge base (read-write for conversation curator)
      - ./knowledge:/knowledge:rw
      # Vector store (read-only - gaia-study is SOLE WRITER)
      - ./knowledge/vector_store:/vector_store:ro
      # Model files (read-only)
      - ./gaia-models:/models:ro
      # Shared state
      - gaia-shared:/shared:rw

    environment:
      - PYTHONPATH=/app:/gaia-common
      - GAIA_SERVICE=core
      - GAIA_ENV=${GAIA_ENV:-development}
      # Paths
      - KNOWLEDGE_DIR=/knowledge
      - VECTOR_STORE_PATH=/vector_store
      - MODELS_DIR=/models
      - SHARED_DIR=/shared
      # Service endpoints (configurable for candidate injection)
      - MCP_ENDPOINT=${MCP_ENDPOINT:-http://gaia-mcp:8765/jsonrpc}
      - STUDY_ENDPOINT=${STUDY_ENDPOINT:-http://gaia-study:8766}
      # CPU-only mode — GPU inference offloaded to gaia-prime
      - GAIA_BACKEND=${GAIA_BACKEND:-gpu_prime}
      - GAIA_FORCE_CPU=1
      - N_GPU_LAYERS=0
      - GAIA_ALLOW_PRIME_LOAD=${GAIA_ALLOW_PRIME_LOAD:-1}
      # Remote vLLM inference via gaia-prime
      - PRIME_ENDPOINT=${PRIME_ENDPOINT:-http://gaia-prime:7777}
      - PRIME_MODEL=${PRIME_MODEL:-/models/Qwen3-8B-AWQ}
      # Groq API fallback (free tier)
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=${GROQ_MODEL:-llama-3.3-70b-versatile}
      # Model loading - disabled by default, models load on first use
      - GAIA_AUTOLOAD_MODELS=${GAIA_AUTOLOAD_MODELS:-0}
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "6415:6415"

    networks:
      - gaia-net

    depends_on:
      gaia-mcp:
        condition: service_healthy
      gaia-prime:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6415/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-web: The Face - UI and API gateway
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-web:
    build:
      context: .
      dockerfile: ./gaia-web/Dockerfile
    image: localhost:5000/gaia-web:local
    container_name: gaia-web
    hostname: gaia-web
    restart: unless-stopped

    env_file:
      - .env.discord

    volumes:
      # Source code (development mode)
      - ./gaia-web:/app:rw
      # Shared library (development mode)
      - ./gaia-common:/gaia-common:ro
      # Knowledge base (read-only, for static content)
      - ./knowledge:/knowledge:ro
      # Static assets (if needed)
      - ./gaia-web/static:/app/static:ro

    environment:
      - PYTHONPATH=/app:/gaia-common
      - GAIA_SERVICE=web
      - GAIA_ENV=${GAIA_ENV:-development}
      # Service endpoints (configurable for candidate injection)
      - CORE_ENDPOINT=${CORE_ENDPOINT:-http://gaia-core:6415}
      - MCP_ENDPOINT=${MCP_ENDPOINT:-http://gaia-mcp:8765/jsonrpc}
      # Web server settings
      - HOST=0.0.0.0
      - PORT=6414
      # Discord integration (Unified Interface Gateway)
      # DISCORD_BOT_TOKEN loaded from env_file: .env.discord
      - ENABLE_DISCORD=${ENABLE_DISCORD:-1}
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "6414:6414"

    networks:
      - gaia-net

    depends_on:
      gaia-core:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6414/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-study: The Subconscious - Background processing
  # SOLE WRITER to vector store and LoRA adapters
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-study:
    build:
      context: .
      dockerfile: ./gaia-study/Dockerfile
    image: localhost:5000/gaia-study:local
    container_name: gaia-study
    hostname: gaia-study
    restart: unless-stopped
    user: "${UID:-1000}:${GID:-1000}"

    volumes:
      # Source code (development mode)
      - ./gaia-study:/app:rw
      # Shared library (development mode)
      - ./gaia-common:/gaia-common:ro
      # Knowledge base (FULL ACCESS for indexing)
      - ./knowledge:/knowledge:rw
      # Vector store (SOLE WRITER)
      - ./knowledge/vector_store:/vector_store:rw
      # Model files (WRITE for LoRA adapters)
      - ./gaia-models:/models:rw
      # Shared state
      - gaia-shared:/shared:rw

    environment:
      - GAIA_SERVICE=study
      - GAIA_ENV=${GAIA_ENV:-development}
      # Paths
      - KNOWLEDGE_DIR=/knowledge
      - VECTOR_STORE_PATH=/vector_store
      - MODELS_DIR=/models
      - SHARED_DIR=/shared
      # GPU settings for embedding
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      # Embedding model
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "8766:8766"

    networks:
      - gaia-net

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8766/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-mcp: The Hands - Sandboxed tool execution
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-mcp:
    build:
      context: .
      dockerfile: ./gaia-mcp/Dockerfile
    image: localhost:5000/gaia-mcp:local
    container_name: gaia-mcp
    hostname: gaia-mcp
    restart: unless-stopped
    user: "${UID:-1000}:${GID:-1000}"

    volumes:
      # Source code (development mode)
      - ./gaia-mcp:/app:rw
      # Shared library (development mode)
      - ./gaia-common:/gaia-common:ro
      # Knowledge base (read-only for tool reference)
      - ./knowledge:/knowledge:rw
      # Model files (read-only for embeddings)
      - ./gaia-models:/models:ro
      # Sandboxed workspace (isolated)
      - gaia-sandbox:/sandbox:rw

    environment:
      - GAIA_SERVICE=mcp
      - MODELS_DIR=/models
      - GAIA_ENV=${GAIA_ENV:-development}
      # Paths
      - SANDBOX_ROOT=/sandbox
      - KNOWLEDGE_DIR=/knowledge
      # Security
      - MCP_APPROVAL_REQUIRED=${MCP_APPROVAL_REQUIRED:-true}
      - MCP_APPROVAL_TTL=900
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "8765:8765"

    networks:
      - gaia-net

    # Security hardening
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8765/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s




# ═══════════════════════════════════════════════════════════════════════════
# Networks
# ═══════════════════════════════════════════════════════════════════════════
networks:
  gaia-net:
    name: gaia-network
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

# ═══════════════════════════════════════════════════════════════════════════
# Volumes
# ═══════════════════════════════════════════════════════════════════════════
volumes:
  # Shared state between services (sessions, packets, etc.)
  gaia-shared:
    name: gaia-shared

  # Isolated sandbox for MCP tool execution
  gaia-sandbox:
    name: gaia-sandbox
