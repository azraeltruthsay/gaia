# GAIA Service-Oriented Architecture - Master Composition
#
# Services:
#   - gaia-core:  The Brain   - Cognitive loop and reasoning (CPU, delegates to gaia-prime)
#   - gaia-prime: The Voice   - Standalone vLLM inference server (GPU)
#   - gaia-web:   The Face    - UI and API gateway
#   - gaia-study: The Subconscious - Background processing (GPU, SOLE WRITER)
#   - gaia-mcp:   The Hands   - Sandboxed tool execution
#
# Volume Access:
#   - knowledge/:       RO for all, RW for gaia-study
#   - vector_store/:    RO for gaia-core, RW for gaia-study (SOLE WRITER)
#   - gaia-models/:     RO for gaia-core, RW for gaia-study (LoRA adapters)
#   - gaia-common/:     RO for all (shared library)
#
# Usage:
#   docker compose up -d              # Start all services
#   docker compose up gaia-web        # Start web + dependencies
#   docker compose logs -f gaia-core  # Follow core logs
#   docker compose down               # Stop all services
#
# Modular Swap (inject candidate into live flow):
#   # Swap MCP: live core -> candidate MCP
#   MCP_ENDPOINT=http://gaia-mcp-candidate:8765/jsonrpc docker compose up -d gaia-core
#
#   # Swap Core: live web -> candidate core
#   CORE_ENDPOINT=http://gaia-core-candidate:6415 docker compose up -d gaia-web
#
#   # Swap Study: live core -> candidate study
#   STUDY_ENDPOINT=http://gaia-study-candidate:8766 docker compose up -d gaia-core

services:
  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-orchestrator: The Coordinator - GPU and container lifecycle management
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-orchestrator:
    build:
      context: .
      dockerfile: ./gaia-orchestrator/Dockerfile
    image: gaia-orchestrator:latest
    container_name: gaia-orchestrator
    hostname: gaia-orchestrator
    restart: unless-stopped

    volumes:
      # Docker socket for container management
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Shared state for persistence
      - gaia-shared:/shared:rw
      # Project root for compose file access (read-only)
      - .:/gaia/GAIA_Project:ro

    # Grant access to the host Docker socket (GID 962 = docker group on host)
    group_add:
      - "962"

    environment:
      - ORCHESTRATOR_HOST=0.0.0.0
      - ORCHESTRATOR_PORT=6410
      - ORCHESTRATOR_STATE_DIR=/shared/orchestrator
      - ORCHESTRATOR_COMPOSE_FILE_LIVE=/gaia/GAIA_Project/docker-compose.yml
      - ORCHESTRATOR_COMPOSE_FILE_CANDIDATE=/gaia/GAIA_Project/docker-compose.candidate.yml
      # Service endpoints (inside Docker network)
      - ORCHESTRATOR_CORE_URL=http://gaia-core:6415
      - ORCHESTRATOR_WEB_URL=http://gaia-web:6414
      - ORCHESTRATOR_STUDY_URL=http://gaia-study:8766
      - ORCHESTRATOR_MCP_URL=http://gaia-mcp:8765
      - ORCHESTRATOR_PRIME_URL=http://gaia-prime:7777
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "6410:6410"

    networks:
      - gaia-net

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6410/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-prime: The Voice - Standalone vLLM inference server (GPU)
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-prime:
    build:
      context: ./gaia-prime
      dockerfile: Dockerfile
    image: localhost:5000/gaia-prime:local
    container_name: gaia-prime
    hostname: gaia-prime
    restart: unless-stopped

    volumes:
      - /mnt/gaia_warm_pool:/models:ro

    environment:
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_FLASH_ATTN_VERSION=2
      - TORCH_CUDA_ARCH_LIST=12.0+PTX
      # Required for vLLM sleep mode (/sleep and /wake_up endpoints)
      - VLLM_SERVER_DEV_MODE=1

    ports:
      - "7777:7777"

    command: >
      python -m vllm.entrypoints.openai.api_server
      --model ${PRIME_MODEL_PATH:-/models/Qwen3-8B-abliterated-AWQ}
      --host 0.0.0.0
      --port 7777
      --gpu-memory-utilization 0.65
      --max-model-len 16384
      --max-num-seqs 1
      --trust-remote-code
      --dtype auto
      --enforce-eager
      --enable-lora
      --max-loras 4
      --max-lora-rank 64
      --enable-sleep-mode
      --enable-prefix-caching
      --kv-offloading-backend native
      --kv-offloading-size 8
      --disable-hybrid-kv-cache-manager

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7777/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

    networks:
      - gaia-net

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-core: The Brain - Cognitive loop and reasoning (CPU, delegates to gaia-prime)
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-core:
    build:
      context: .
      dockerfile: ./gaia-core/Dockerfile
    image: localhost:5000/gaia-core:local
    container_name: gaia-core
    hostname: gaia-core
    restart: unless-stopped
    user: "${UID:-1000}:${GID:-1000}"
    # Allow time for cognitive checkpoint writes before SIGKILL (Phase 4.5 HA)
    stop_grace_period: 25s

    volumes:
      # Source code (development mode)
      - ./gaia-core:/app:rw
      # Shared library (development mode - mount for live reload)
      - ./gaia-common:/gaia-common:ro
      # Knowledge base (read-write for conversation curator)
      - ./knowledge:/knowledge:rw
      # Vector store (read-only - gaia-study is SOLE WRITER)
      - ./knowledge/vector_store:/vector_store:ro
      # Model files (read-only)
      - ./gaia-models:/models:ro
      # Logs (consolidated service logs)
      - ./logs:/logs:rw
      # Shared state
      - gaia-shared:/shared:rw

    environment:
      - PYTHONPATH=/app:/gaia-common
      - GAIA_SERVICE=core
      - GAIA_ENV=${GAIA_ENV:-development}
      # Paths
      - KNOWLEDGE_DIR=/knowledge
      - GAIA_BLUEPRINTS_ROOT=/knowledge/blueprints
      - VECTOR_STORE_PATH=/vector_store
      - MODELS_DIR=/models
      - SHARED_DIR=/shared
      # Service endpoints (configurable for candidate injection)
      - MCP_ENDPOINT=${MCP_ENDPOINT:-http://gaia-mcp:8765/jsonrpc}
      - STUDY_ENDPOINT=${STUDY_ENDPOINT:-http://gaia-study:8766}
      # CPU-only mode — GPU inference offloaded to gaia-prime
      - GAIA_BACKEND=${GAIA_BACKEND:-gpu_prime}
      - GAIA_FORCE_CPU=1
      - N_GPU_LAYERS=0
      - GAIA_ALLOW_PRIME_LOAD=${GAIA_ALLOW_PRIME_LOAD:-1}
      # Remote vLLM inference via gaia-prime
      - PRIME_ENDPOINT=${PRIME_ENDPOINT:-http://gaia-prime:7777}
      - PRIME_MODEL=${PRIME_MODEL:-/models/Qwen3-8B-abliterated-AWQ}
      # Lite model (CPU, GGUF via llama_cpp)
      - GAIA_LITE_GGUF=${GAIA_LITE_GGUF:-/models/Qwen3-8B-abliterated-Q4_K_M.gguf}
      # Groq API fallback (free tier)
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=${GROQ_MODEL:-llama-3.3-70b-versatile}
      # HA failover — candidate MCP as fallback
      - MCP_FALLBACK_ENDPOINT=${MCP_FALLBACK_ENDPOINT:-http://gaia-mcp-candidate:8765/jsonrpc}
      # Model loading - disabled by default, models load on first use
      - GAIA_AUTOLOAD_MODELS=${GAIA_AUTOLOAD_MODELS:-0}
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "6415:6415"

    networks:
      - gaia-net

    depends_on:
      gaia-mcp:
        condition: service_healthy
      gaia-prime:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6415/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-web: The Face - UI and API gateway
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-web:
    build:
      context: .
      dockerfile: ./gaia-web/Dockerfile
    image: localhost:5000/gaia-web:local
    container_name: gaia-web
    hostname: gaia-web
    restart: unless-stopped
    group_add:
      - "${DOCKER_GID:-962}"

    env_file:
      - .env.discord

    volumes:
      # Source code (development mode)
      - ./gaia-web:/app:rw
      # Shared library (development mode)
      - ./gaia-common:/gaia-common:ro
      # Knowledge base (read-only, for static content)
      - ./knowledge:/knowledge:ro
      # Static assets (if needed)
      - ./gaia-web/static:/app/static:ro
      # Docker socket for terminal access
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Logs (consolidated service logs)
      - ./logs:/logs:rw

    environment:
      - PYTHONPATH=/app:/gaia-common
      - GAIA_SERVICE=web
      - GAIA_ENV=${GAIA_ENV:-development}
      - GAIA_BLUEPRINTS_ROOT=/knowledge/blueprints
      - FILE_ROOTS=project:/app,knowledge:/knowledge
      # Service endpoints (configurable for candidate injection)
      - CORE_ENDPOINT=${CORE_ENDPOINT:-http://gaia-core:6415}
      - MCP_ENDPOINT=${MCP_ENDPOINT:-http://gaia-mcp:8765/jsonrpc}
      # Web server settings
      - HOST=0.0.0.0
      - PORT=6414
      # Discord integration (Unified Interface Gateway)
      # DISCORD_BOT_TOKEN loaded from env_file: .env.discord
      - ENABLE_DISCORD=${ENABLE_DISCORD:-1}
      # HA failover — candidate-core as fallback
      - CORE_FALLBACK_ENDPOINT=${CORE_FALLBACK_ENDPOINT:-http://gaia-core-candidate:6415}
      # Wiki proxy (internal, auto-resolves on gaia-net)
      - WIKI_ENDPOINT=${WIKI_ENDPOINT:-http://gaia-wiki:8080}
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "6414:6414"

    networks:
      - gaia-net

    depends_on:
      gaia-core:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6414/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-study: The Subconscious - Background processing
  # SOLE WRITER to vector store and LoRA adapters
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-study:
    build:
      context: .
      dockerfile: ./gaia-study/Dockerfile
    image: localhost:5000/gaia-study:local
    container_name: gaia-study
    hostname: gaia-study
    restart: unless-stopped
    user: "${UID:-1000}:${GID:-1000}"

    volumes:
      # Source code (development mode)
      - ./gaia-study:/app:rw
      # Shared library (development mode)
      - ./gaia-common:/gaia-common:ro
      # Knowledge base (FULL ACCESS for indexing)
      - ./knowledge:/knowledge:rw
      # Vector store (SOLE WRITER)
      - ./knowledge/vector_store:/vector_store:rw
      # Model files (WRITE for LoRA adapters)
      - ./gaia-models:/models:rw
      # Logs (consolidated service logs)
      - ./logs:/logs:rw
      # Shared state
      - gaia-shared:/shared:rw

    environment:
      - GAIA_SERVICE=study
      - GAIA_ENV=${GAIA_ENV:-development}
      # Paths
      - KNOWLEDGE_DIR=/knowledge
      - GAIA_BLUEPRINTS_ROOT=/knowledge/blueprints
      - VECTOR_STORE_PATH=/vector_store
      - MODELS_DIR=/models
      - SHARED_DIR=/shared
      - GAIA_LOG_DIR=/logs
      # GPU settings for embedding
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      # Embedding model
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "8766:8766"

    networks:
      - gaia-net

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8766/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-mcp: The Hands - Sandboxed tool execution
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-mcp:
    build:
      context: .
      dockerfile: ./gaia-mcp/Dockerfile
    image: localhost:5000/gaia-mcp:local
    container_name: gaia-mcp
    hostname: gaia-mcp
    restart: unless-stopped
    user: "${UID:-1000}:${GID:-1000}"

    volumes:
      # Source code (development mode)
      - ./gaia-mcp:/app:rw
      # Shared library (development mode)
      - ./gaia-common:/gaia-common:ro
      # Knowledge base (read-only for tool reference)
      - ./knowledge:/knowledge:rw
      # Model files (read-only for embeddings)
      - ./gaia-models:/models:ro
      # Sandboxed workspace (isolated)
      - gaia-sandbox:/sandbox:rw
      # Logs (for introspect_logs tool + own log persistence)
      - ./logs:/logs:rw
      # Candidates (for promotion readiness assessment)
      - ./candidates:/gaia/GAIA_Project/candidates:ro
      # Docker compose file (for compose_service check)
      - ./docker-compose.yml:/gaia/GAIA_Project/docker-compose.yml:ro
      # NotebookLM auth (Playwright storage state, read-only)
      - ${HOME}/.notebooklm:/auth/notebooklm:ro

    environment:
      - GAIA_SERVICE=mcp
      - MODELS_DIR=/models
      - GAIA_ENV=${GAIA_ENV:-development}
      # Paths
      - SANDBOX_ROOT=/sandbox
      - KNOWLEDGE_DIR=/knowledge
      - GAIA_BLUEPRINTS_ROOT=/knowledge/blueprints
      - GAIA_LOG_DIR=/logs
      # Security
      - MCP_APPROVAL_REQUIRED=${MCP_APPROVAL_REQUIRED:-true}
      - MCP_APPROVAL_TTL=900
      # External API keys (authenticated web_fetch)
      - KANKA_API_KEY=${KANKA_API_KEY:-}
      # NotebookLM auth
      - NOTEBOOKLM_HOME=/auth/notebooklm
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "8765:8765"

    networks:
      - gaia-net

    # Security hardening
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8765/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-audio: The Ears & Mouth - Half-duplex STT/TTS sensory service
  # GPU-capable container for Whisper STT and RealtimeTTS
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-audio:
    build:
      context: .
      dockerfile: ./gaia-audio/Dockerfile
    image: localhost:5000/gaia-audio:local
    container_name: gaia-audio
    hostname: gaia-audio
    restart: unless-stopped

    volumes:
      - ./gaia-audio:/app:rw
      - ./gaia-common:/app/gaia_common:ro
      - ./gaia-models/voice:/models/voice:ro
      - ./gaia-models/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf:/models/nano_refiner.gguf:ro

    environment:
      - PYTHONPATH=/app
      - GAIA_SERVICE=audio
      - GAIA_ENV=${GAIA_ENV:-production}
      - GAIA_CONSTANTS_PATH=/app/gaia_common/constants/gaia_constants.json
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - NANO_MODEL_PATH=/models/nano_refiner.gguf
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "8080:8080"

    networks:
      - gaia-net

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]


  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-wiki: The Library - Internal developer documentation
  # Accessible internally at http://gaia-wiki:8080
  # Optionally proxied by gaia-web at /wiki
  # NOT exposed publicly by default
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-wiki:
    build:
      context: .
      dockerfile: ./gaia-wiki/Dockerfile
    image: localhost:5000/gaia-wiki:local
    container_name: gaia-wiki
    hostname: gaia-wiki
    restart: unless-stopped

    volumes:
      # Live reload: mount docs for edit-without-rebuild
      - ./gaia-wiki/docs:/docs/docs:ro
      - ./gaia-wiki/mkdocs.yml:/docs/mkdocs.yml:ro
      # Auto-generated docs from blueprint YAML (written by gaia-core sleep task)
      - ./knowledge/wiki_auto:/docs/docs/auto:ro
      # Knowledge base: accessible for future auto-generated docs
      - ./knowledge:/knowledge:ro

    environment:
      - GAIA_SERVICE=wiki

    # No ports: block — internal only.
    # For local dev access, add temporarily: ports: ["8080:8080"]

    networks:
      - gaia-net

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

  # ═══════════════════════════════════════════════════════════════════════════
  # dozzle: The X-Ray - Real-time Docker log viewer
  # Lightweight container log viewer — reads directly from Docker socket
  # Provides instant visibility into all container stdout/stderr
  # Access at http://localhost:9999
  # ═══════════════════════════════════════════════════════════════════════════
  dozzle:
    image: amir20/dozzle:latest
    container_name: dozzle
    hostname: dozzle
    restart: unless-stopped

    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro

    environment:
      - DOZZLE_NO_ANALYTICS=true

    ports:
      - "9999:8080"

    networks:
      - gaia-net

  # ═══════════════════════════════════════════════════════════════════════════
  # gaia-doctor: The Immune System - Persistent HA watchdog
  # Monitors service health and auto-restarts crashed HA candidates
  # via docker compose with the HA overlay (preserves restart policies)
  # ═══════════════════════════════════════════════════════════════════════════
  gaia-doctor:
    build:
      context: .
      dockerfile: gaia-doctor/Dockerfile
    image: localhost:5000/gaia-doctor:local
    container_name: gaia-doctor
    hostname: gaia-doctor
    restart: unless-stopped

    volumes:
      # Docker socket (read-write — doctor needs to restart containers)
      - /var/run/docker.sock:/var/run/docker.sock
      # Docker CLI + compose plugin (bind-mounted from host, avoids installing full docker.io)
      - /usr/bin/docker:/usr/bin/docker:ro
      - /usr/lib/docker/cli-plugins/docker-compose:/usr/lib/docker/cli-plugins/docker-compose:ro
      # Shared state for status persistence and maintenance mode flag
      - gaia-shared:/shared:rw
      # Compose files for HA-overlay restarts
      - ./docker-compose.candidate.yml:/compose/docker-compose.candidate.yml:ro
      - ./docker-compose.ha.yml:/compose/docker-compose.ha.yml:ro

    group_add:
      - "${DOCKER_GID:-962}"

    environment:
      - POLL_INTERVAL=${DOCTOR_POLL_INTERVAL:-60}
      - FAILURE_THRESHOLD=${DOCTOR_FAILURE_THRESHOLD:-2}
      - RESTART_COOLDOWN=${DOCTOR_RESTART_COOLDOWN:-300}
      - COMPOSE_PROJECT_NAME=gaia_project
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

    ports:
      - "6419:6419"

    networks:
      - gaia-net

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6419/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s


# ═══════════════════════════════════════════════════════════════════════════
# Networks
# ═══════════════════════════════════════════════════════════════════════════
networks:
  gaia-net:
    name: gaia-network
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

# ═══════════════════════════════════════════════════════════════════════════
# Volumes
# ═══════════════════════════════════════════════════════════════════════════
volumes:
  # Shared state between services (sessions, packets, etc.)
  gaia-shared:
    name: gaia-shared

  # Isolated sandbox for MCP tool execution
  gaia-sandbox:
    name: gaia-sandbox
