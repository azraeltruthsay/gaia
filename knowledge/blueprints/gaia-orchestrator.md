# GAIA Service Blueprint: `gaia-orchestrator` (The Coordinator (Infrastructure))

> **Status:** ðŸŸ¢ LIVE Â· âš ï¸ genesis â€” awaiting validation  
> **Service version:** 0.5  
> **Blueprint version:** 0.2  
> **Generated by:** `manual_seed`  
> **Last reflected:** never  

## Purpose

Central resource broker for GPU allocation and container lifecycle in a single-GPU GAIA system. Enables gaia-prime (inference) and gaia-study (training) to share the GPU via coordinated handoffs. Provides real-time monitoring, container swapping, and graceful state recovery.


**Cognitive role:** The Coordinator

### Design Decisions

- CPU-only by design â€” async handoff orchestration without blocking cognition
- Single GPU multiplexing via container stop/start (not vLLM sleep) because --enforce-eager prevents full VRAM release
- VRAM cleanup uses pynvml if available; falls back to container status check on non-GPU hosts
- Atomic state persistence to /shared/orchestrator/state.json with rename to prevent corruption
- Lazy manager initialization â€” GPU, Docker, Handoff managers init only on first use
- Graceful service degradation â€” HTTP calls to study/prime that timeout log warnings but don't fail handoff
- WebSocket history buffer keeps last 100 notifications for new clients to catch up
- Docker compose subprocess (not Python SDK) for full compose support (profiles, env overrides)
- Handoff phases with progress tracking (6 phases: INITIATED -> RELEASING -> CLEANUP -> TRANSFER -> SIGNALING -> COMPLETED)

### âš ï¸ Open Questions

- Should GPU cleanup timeout be dynamic based on model size, or remain fixed at 30s?
- Should concurrent handoff requests queue, or immediately fail with 409 Conflict?
- Is prime health polling sufficient, or should orchestrator have explicit ownership tracking?

## Runtime

| Property | Value |
|----------|-------|
| Port | `6410` |
| Base image | `python:3.11-slim` |
| GPU | No |
| Health check | `curl -f http://localhost:6410/health` |
| Startup | `uvicorn gaia_orchestrator.main:app --host 0.0.0.0 --port 6410` |
| User | `${UID}:${GID}` |
| Dockerfile | `gaia-orchestrator/Dockerfile` |
| Compose service | `gaia-orchestrator` |

## Interfaces

### Inbound

- **`health`** âœ… `http_rest` `/health`  
  Container health check endpoint.
- **`root`** âœ… `http_rest` `/`  
  Root endpoint â€” full API overview for service discovery.
- **`status`** âœ… `http_rest` `/status`  
  Complete orchestrator state: GPU status, container statuses, active handoff.
- **`gpu_status`** âœ… `http_rest` `/gpu/status`  
  Current GPU ownership (owner, lease_id, acquired_at, queue).
- **`gpu_acquire`** âœ… `http_rest` `/gpu/acquire`  
  Request GPU ownership; queues if unavailable.
- **`gpu_release`** âœ… `http_rest` `/gpu/release`  
  Release GPU ownership; advances queue to next requester.
- **`gpu_wait`** âœ… `http_rest` `/gpu/wait`  
  Block and wait for GPU availability, then acquire.
- **`gpu_sleep`** âœ… `http_rest` `/gpu/sleep`  
  Release GPU for sleep â€” stops prime container, demotes gpu_prime. Called by gaia-core SleepCycleLoop.
- **`gpu_wake`** âœ… `http_rest` `/gpu/wake`  
  Reclaim GPU after wake â€” starts prime container, restores gpu_prime. Called by gaia-core SleepCycleLoop.
- **`containers_status`** âœ… `http_rest` `/containers/status`  
  Get all live + candidate service container states.
- **`containers_live_start`** âœ… `http_rest` `/containers/live/start`  
  Start live stack (gaia-core, gaia-web, gaia-mcp, gaia-study).
- **`containers_live_stop`** âœ… `http_rest` `/containers/live/stop`  
  Stop live stack and release GPU.
- **`containers_candidate_start`** âœ… `http_rest` `/containers/candidate/start`  
  Start candidate stack with --profile full.
- **`containers_candidate_stop`** âœ… `http_rest` `/containers/candidate/stop`  
  Stop candidate stack and release GPU if owned.
- **`containers_swap`** âœ… `http_rest` `/containers/swap`  
  Swap service between live/candidate; injects candidate into live traffic.
- **`handoff_prime_to_study`** âœ… `http_rest` `/handoff/prime-to-study`  
  Initiate GPU handoff from prime to study (5-phase: release -> cleanup -> transfer -> signal -> complete).
- **`handoff_study_to_prime`** âœ… `http_rest` `/handoff/study-to-prime`  
  Initiate GPU handoff from study back to prime.
- **`handoff_status`** âœ… `http_rest` `/handoff/{handoff_id}/status`  
  Query handoff status by ID.
- **`notify_oracle_fallback`** âœ… `http_rest` `/notify/oracle-fallback`  
  Receive Oracle inference fallback notifications.
- **`ws_notifications`** âœ… `websocket` `/ws/notifications`  
  Real-time notification stream; broadcasts to all connected WebSocket clients.

### Outbound

- **`core_gpu_release`** âœ… `http_rest` `/gpu/release`  
  Notify gaia-core to demote gpu_prime from model pool.
- **`core_gpu_reclaim`** âœ… `http_rest` `/gpu/reclaim`  
  Notify gaia-core to restore gpu_prime to model pool.
- **`core_study_handoff`** âœ… `http_rest` `/sleep/study-handoff`  
  Notify gaia-core sleep state machine of handoff completion.
- **`study_gpu_ready`** âœ… `http_rest` `/study/gpu-ready`  
  Signal gaia-study that GPU is available for training.
- **`study_gpu_release`** âœ… `http_rest` `/study/gpu-release`  
  Request gaia-study release GPU during study-to-prime handoff.
- **`prime_health_probe`** âœ… `http_rest` `/health`  
  Health probe to gaia-prime during container start (waits for model loaded).

## Service Dependencies

- **`gaia-core`** â€” gpu_negotiation Â· required
- **`gaia-prime`** â€” inference_container Â· optional (fallback: `â€”`)
- **`gaia-study`** â€” training_container Â· optional (fallback: `â€”`)

## Volume Dependencies

- **docker-socket** `ro` â†’ `/var/run/docker.sock` â€” Docker daemon access for container lifecycle management
- **gaia-shared** `rw` â†’ `/shared` â€” State persistence (/shared/orchestrator/state.json)
- **project-root** `ro` â†’ `/gaia/GAIA_Project` â€” Compose file access for live + candidate stacks

## Failure Modes

- ðŸ”´ **Docker daemon unreachable**
  â†’ Container start/stop endpoints return 500; GPU tracking still functional
- ðŸŸ  **gaia-core GPU release/reclaim fails**
  â†’ Logs error; GPU lease remains held; handoff may stall
- ðŸŸ  **gaia-prime health check timeout (120s)**
  â†’ GPU wake fails; study cycle exits with timeout
- ðŸŸ  **CUDA cleanup timeout (30s)**
  â†’ Handoff fails with timeout; GPU remains locked to previous owner
- ðŸŸ¡ **State file corruption on disk**
  â†’ Fresh state initialized on boot; handoff history lost but GPU ownership recovers
- ðŸŸ¡ **gaia-study unreachable during handoff**
  â†’ Signal calls log warnings but don't fail handoff; study may miss GPU-ready notification
- ðŸŸ¡ **Concurrent handoffs triggered**
  â†’ Handoff manager rejects 2nd handoff with 'already in progress' error

## Source Files

- `candidates/gaia-orchestrator/gaia_orchestrator/main.py` _entrypoint_ [python]
- `candidates/gaia-orchestrator/gaia_orchestrator/gpu_manager.py` _gpu_coordination_ [python]
- `candidates/gaia-orchestrator/gaia_orchestrator/docker_manager.py` _container_lifecycle_ [python]
- `candidates/gaia-orchestrator/gaia_orchestrator/handoff_manager.py` _handoff_protocol_ [python]
- `candidates/gaia-orchestrator/gaia_orchestrator/state.py` _state_persistence_ [python]
- `candidates/gaia-orchestrator/gaia_orchestrator/notification_manager.py` _websocket_broadcast_ [python]
- `candidates/gaia-orchestrator/gaia_orchestrator/config.py` _configuration_ [python]
- `candidates/gaia-orchestrator/gaia_orchestrator/models/schemas.py` _data_models_ [python]
- `candidates/gaia-orchestrator/Dockerfile` _build_config_ [dockerfile]

## Blueprint Confidence

| Section | Confidence |
|---------|------------|
| Runtime | ðŸŸ¢ ConfidenceLevel.HIGH |
| Contract | ðŸŸ¢ ConfidenceLevel.HIGH |
| Dependencies | ðŸŸ¢ ConfidenceLevel.HIGH |
| Failure Modes | ðŸŸ¡ ConfidenceLevel.MEDIUM |
| Intent | ðŸŸ¡ ConfidenceLevel.MEDIUM |

---
_Generated automatically from `gaia-orchestrator.yaml`. Do not edit this file directly._