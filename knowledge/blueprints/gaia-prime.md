# GAIA Service Blueprint: `gaia-prime` (The Voice (Inference))

> **Status:** ðŸŸ¢ LIVE Â· âš ï¸ genesis â€” awaiting validation  
> **Service version:** 0.5  
> **Blueprint version:** 0.2  
> **Generated by:** `manual_seed`  
> **Last reflected:** never  

## Purpose

GPU-accelerated LLM inference decoupled from the cognitive loop. Enables gaia-core (CPU-only) to delegate all LLM computation while maintaining control flow. Supports GPU time-sharing between inference (prime) and training (study) via orchestrator-managed container lifecycle.


**Cognitive role:** The Voice

### Design Decisions

- Standalone vLLM server â€” no custom wrapper code reduces maintenance and leverages community ecosystem
- Blackwell (sm_120) optimized via NGC PyTorch 25.03 + custom Float8 patches for RTX 5080
- LoRA support (4 concurrent adapters) allows persona/user-specific behavior without full finetune
- 8GB KV cache offloading buffer tolerates longer context despite --enforce-eager limitation
- Sleep/wake endpoints enable GPU handoff signaling; orchestrator uses container stop/start for full VRAM release
- Warm pool (/mnt/gaia_warm_pool tmpfs) seeded at boot reduces cold start from ~41s (NVMe) to ~37s (RAM)
- Prefix caching optimizes repeated long prefixes (system prompt + conversation context)
- Max context window 8192 tokens balances latency vs reasoning depth
- Guided decoding (JSON schema, regex) ensures reliable tool-calling output

### âš ï¸ Open Questions

- Should gpu_memory_utilization be dynamic based on session queue depth?
- Can prefix caching be leveraged more aggressively for knowledge retrieval patterns?
- Optimal LoRA rank for tier3_session ephemeral adapters to minimize performance impact?

## Runtime

| Property | Value |
|----------|-------|
| Port | `7777` |
| Base image | `nvcr.io/nvidia/pytorch:25.03-py3` |
| GPU | Yes |
| Health check | `curl -f http://localhost:7777/health` |
| Startup | `python -m vllm.entrypoints.openai.api_server --model /models/Qwen3-4B-Instruct-2507-heretic --host 0.0.0.0 --port 7777 --gpu-memory-utilization 0.70 --max-model-len 8192 --max-num-seqs 4 --trust-remote-code --dtype auto --enforce-eager --enable-lora --max-loras 4 --max-lora-rank 64 --enable-sleep-mode --enable-prefix-caching --kv-offloading-backend native --kv-offloading-size 8 --disable-hybrid-kv-cache-manager` |
| GPU count | `1` |
| Dockerfile | `gaia-prime/Dockerfile` |
| Compose service | `gaia-prime` |

## Interfaces

### Inbound

- **`health`** âœ… `http_rest` `/health`  
  Health check endpoint for container probes and orchestrator monitoring.
- **`models_list`** âœ… `http_rest` `/v1/models`  
  List models registered with vLLM server.
- **`chat_completions`** âœ… `http_rest` `/v1/chat/completions`  
  Chat completion â€” OpenAI-compatible API. Supports streaming, guided decoding (JSON/regex), and LoRA adapter selection.
- **`text_completions`** âœ… `http_rest` `/v1/completions`  
  Text completion â€” OpenAI-compatible API for raw text generation.
- **`vllm_sleep`** âœ… `http_rest` `/sleep`  
  vLLM sleep mode â€” offload KV cache to CPU RAM to reduce VRAM. Requires VLLM_SERVER_DEV_MODE=1.
- **`vllm_wake_up`** âœ… `http_rest` `/wake_up`  
  vLLM wake â€” restore KV cache from CPU RAM back to GPU VRAM.

## Volume Dependencies

- **gaia_warm_pool** `ro` â†’ `/models` â€” Model weights and LoRA adapters (warm pool seeded at boot by systemd)

## Failure Modes

- ðŸŸ¡ **gaia-prime unreachable**
  â†’ gaia-core falls back to Groq API, then local GGUF model
- ðŸ”´ **GPU out of memory (VRAM exhausted)**
  â†’ vLLM rejects request with 503; requires container restart
- ðŸ”´ **Model loading timeout (>120s)**
  â†’ Health check fails; orchestrator does not bring service online
- ðŸŸ¡ **vLLM process crash (OOM, CUDA error)**
  â†’ Container exits; restart policy=unless-stopped restarts automatically
- ðŸŸ¡ **LoRA adapter not found or mismatched**
  â†’ Request fails; gaia-core catches exception and retries with base model
- ðŸ”´ **CUDA architecture mismatch (wrong sm_ code)**
  â†’ Container fails during vLLM init; orchestrator detects health check failure

## Source Files

- `gaia-prime/Dockerfile` _build_config_ [dockerfile]
- `gaia-prime/patch_float8.sh` _build_helper_ [bash]
- `gaia-prime/cmake_wrapper.sh` _build_hook_ [bash]

## Blueprint Confidence

| Section | Confidence |
|---------|------------|
| Runtime | ðŸŸ¢ ConfidenceLevel.HIGH |
| Contract | ðŸŸ¢ ConfidenceLevel.HIGH |
| Dependencies | ðŸŸ¢ ConfidenceLevel.HIGH |
| Failure Modes | ðŸŸ¡ ConfidenceLevel.MEDIUM |
| Intent | ðŸŸ¡ ConfidenceLevel.MEDIUM |

---
_Generated automatically from `gaia-prime.yaml`. Do not edit this file directly._