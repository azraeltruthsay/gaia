# GAIA Service Blueprint: `gaia-core` (The Brain (Cognition))

> **Status:** ğŸŸ¢ LIVE Â· âš ï¸ genesis â€” awaiting validation  
> **Service version:** 0.5  
> **Blueprint version:** 0.2  
> **Generated by:** `manual_seed`  
> **Last reflected:** never  

## Purpose

The cognitive engine of GAIA. Runs the full reasoning loop: intent detection, tool routing, multi-step reflection, and response generation. Deliberately CPU-only to allow gaia-prime and gaia-study to share the GPU without interrupting cognition. All inference is delegated to gaia-prime, with graceful fallback chains preserving uptime across backend failures.


**Cognitive role:** The Brain

### Design Decisions

- CPU-only runtime enables GPU handoffs between prime and study without blocking the cognition loop
- Falls back through groq â†’ gguf rather than hard-failing â€” uptime over raw capability
- gaia-web owns output routing so core remains interface-agnostic
- Four-tier memory: session (short-term), semantic codex (mid-term), vector store (long-term), prime.md checkpoint (sleep continuity)
- Guided decoding via json-architect LoRA adapter for reliable structured output from smaller models
- Sleep/wake cognitive continuity â€” LLM-generated checkpoint captures introspective state, consumed-sentinel prevents stale injection
- Parallel wake strategy â€” GPU reclaim and checkpoint load happen concurrently for faster wake
- Human-in-the-loop approval flow for destructive tool calls via gaia-mcp /request_approval

### âš ï¸ Open Questions

- Should reflection loop depth be dynamic based on query complexity or fixed per persona?
- Semantic codex hot-reload interval is hardcoded â€” should it be configurable via gaia_constants.json?

## Runtime

| Property | Value |
|----------|-------|
| Port | `6415` |
| Base image | `python:3.11-slim` |
| GPU | No |
| Health check | `curl -f http://localhost:6415/health` |
| Startup | `uvicorn gaia_core.main:app --host 0.0.0.0 --port 6415` |
| User | `${UID}:${GID}` |
| Dockerfile | `gaia-core/Dockerfile` |
| Compose service | `gaia-core` |

## Interfaces

### Inbound

- **`process_packet`** âœ… `http_rest` `/process_packet`  
  Primary cognition entry point. Receives CognitionPackets from gaia-web, runs the full reasoning loop, returns completed packet.
- **`health`** âœ… `http_rest` `/health`  
  Container health check endpoint.
- **`root`** âœ… `http_rest` `/`  
  Root endpoint â€” returns list of available endpoints for service discovery.
- **`status`** âœ… `http_rest` `/status`  
  Cognitive status â€” current state, uptime, active sessions.
- **`gpu_status`** âœ… `http_rest` `/gpu/status`  
  GPU allocation state â€” owned, released, or unavailable.
- **`gpu_release`** âœ… `http_rest` `/gpu/release`  
  Release GPU to orchestrator pool. Called by gaia-orchestrator.
- **`gpu_reclaim`** âœ… `http_rest` `/gpu/reclaim`  
  Reclaim GPU from orchestrator pool. Called by gaia-orchestrator.
- **`sleep_wake`** âœ… `http_rest` `/sleep/wake`  
  Wake signal from gaia-web â€” triggers transition from ASLEEP to WAKING.
- **`sleep_status`** âœ… `http_rest` `/sleep/status`  
  Query current sleep state (ACTIVE, DROWSY, ASLEEP, WAKING).
- **`sleep_study_handoff`** âœ… `http_rest` `/sleep/study-handoff`  
  Study handoff endpoint â€” orchestrator signals study cycle complete, GPU available.
- **`sleep_distracted_check`** âœ… `http_rest` `/sleep/distracted-check`  
  Check if GAIA is asleep and should return a canned/distracted response.
- **`sleep_shutdown`** âœ… `http_rest` `/sleep/shutdown`  
  Graceful shutdown â€” completes current cycle, writes checkpoint, enters sleep.

### Outbound

- **`prime_inference`** âœ… `http_rest` `/v1/chat/completions`  
  LLM inference requests to gaia-prime via OpenAI-compatible API.
- **`prime_health`** âœ… `http_rest` `/health`  
  Health probe to gaia-prime on boot â€” sets prime_available flag.
- **`mcp_dispatch`** âœ… `mcp` `â€”`  
  Tool execution requests dispatched to gaia-mcp via JSON-RPC.
- **`mcp_approval`** âœ… `http_rest` `/request_approval`  
  Tool approval requests sent to gaia-mcp for human-in-the-loop confirmation.
- **`orchestrator_gpu_sleep`** âœ… `http_rest` `/gpu/sleep`  
  Notify orchestrator that gaia-core is entering sleep â€” GPU available for study.
- **`orchestrator_gpu_wake`** âœ… `http_rest` `/gpu/wake`  
  Notify orchestrator that gaia-core is waking â€” request GPU reclamation.
- **`web_presence`** âœ… `http_rest` `/presence`  
  Presence updates to gaia-web â€” online/typing/sleeping status for Discord.

## Service Dependencies

- **`gaia-prime`** â€” inference Â· optional (fallback: `groq-api`)
- **`gaia-mcp`** â€” tool_execution Â· optional (fallback: `â€”`)
- **`gaia-web`** â€” output_routing Â· required
- **`gaia-orchestrator`** â€” gpu_lifecycle Â· optional (fallback: `â€”`)

## External API Dependencies

- **groq** â€” inference_fallback_when_prime_unavailable Â· optional
- **openai** â€” oracle_tier_inference Â· optional
- **gemini** â€” oracle_tier_inference Â· optional

## Volume Dependencies

- **gaia-shared** `rw` â†’ `/shared` â€” Session state, cognitive checkpoints, prime.md sleep notes
- **knowledge** `ro` â†’ `/knowledge` â€” Blueprints, semantic codex, recitable docs
- **vector_store** `rw` â†’ `/vector_store` â€” FAISS vector indices for long-term memory
- **models** `ro` â†’ `/models` â€” LoRA adapters (json-architect, persona weights)
- **logs** `rw` â†’ `/logs` â€” Structured cognition logs, heartbeat telemetry

## Failure Modes

- ğŸŸ¡ **gaia-prime unavailable**
  â†’ Falls back to Groq API; if Groq unavailable, falls back to local GGUF model
- ğŸŸ¡ **gaia-mcp unavailable**
  â†’ Tool calls skipped; responds with capability_unavailable in packet
- ğŸŸ  **All inference backends unavailable**
  â†’ Returns error packet to gaia-web; session preserved for retry
- ğŸŸ¡ **Session state corruption**
  â†’ Clears session, starts fresh, logs incident to heartbeat
- ğŸŸ¡ **gaia-orchestrator unreachable**
  â†’ Sleep/wake GPU handoff skipped; gaia-core retains GPU, study cycle deferred
- ğŸŸ¡ **Checkpoint write failure**
  â†’ Sleep proceeds without checkpoint; next wake has no prime.md restoration context

## Source Files

- `candidates/gaia-core/gaia_core/main.py` _entrypoint_ [python]
- `candidates/gaia-core/gaia_core/cognition/cognitive_dispatcher.py` _core_logic_ [python]
- `candidates/gaia-core/gaia_core/cognition/tool_selector.py` _tool_routing_ [python]
- `candidates/gaia-core/gaia_core/cognition/goal_detector.py` _intent_detection_ [python]
- `candidates/gaia-core/gaia_core/cognition/sleep_cycle_loop.py` _sleep_lifecycle_ [python]
- `candidates/gaia-core/gaia_core/cognition/sleep_wake_manager.py` _sleep_state_machine_ [python]
- `candidates/gaia-core/gaia_core/cognition/prime_checkpoint.py` _cognitive_checkpoint_ [python]
- `candidates/gaia-core/gaia_core/api/gpu_endpoints.py` _gpu_api_ [python]
- `candidates/gaia-core/gaia_core/api/sleep_endpoints.py` _sleep_api_ [python]
- `candidates/gaia-core/gaia_core/memory/session_manager.py` _session_management_ [python]
- `candidates/gaia-core/gaia_core/memory/semantic_codex.py` _mid_term_memory_ [python]
- `candidates/gaia-core/gaia_core/models/vllm_remote_model.py` _inference_client_ [python]
- `candidates/gaia-core/gaia_core/utils/mcp_client.py` _tool_dispatch_ [python]
- `candidates/gaia-core/gaia_core/utils/prompt_builder.py` _prompt_assembly_ [python]
- `gaia-common/gaia_common/constants/gaia_constants.json` _config_ [json]

## Blueprint Confidence

| Section | Confidence |
|---------|------------|
| Runtime | ğŸŸ¢ ConfidenceLevel.HIGH |
| Contract | ğŸŸ¢ ConfidenceLevel.HIGH |
| Dependencies | ğŸŸ¢ ConfidenceLevel.HIGH |
| Failure Modes | ğŸŸ¡ ConfidenceLevel.MEDIUM |
| Intent | ğŸŸ¡ ConfidenceLevel.MEDIUM |

---
_Generated automatically from `gaia-core.yaml`. Do not edit this file directly._