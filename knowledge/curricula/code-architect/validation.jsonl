{"pair_id": "2cc70391-ca9a-4a1d-be3f-19ff3947ffac", "service_id": "gaia-orchestrator", "pair_type": "retroactive", "instruction": "You are GAIA's code-architect. Generate Python source code that faithfully\nimplements the following blueprint. Your output must satisfy all contract,\ndependency, failure mode, and intent specifications exactly.\n\nBLUEPRINT:\narchitecture:\n  components:\n  - consumes_interfaces: []\n    description: \"FastAPI application with HTTP and WebSocket endpoints. Exposes GPU\\\n      \\ management (acquire/release/wait/sleep/wake), container lifecycle (start/stop\\\n      \\ live and candidate stacks), service swapping, prime\\u2194study GPU handoff\\\n      \\ orchestration, and real-time notification streaming via WebSocket. All state-mutating\\\n      \\ operations delegate to specialized manager components.\\n\"\n    exposes_interfaces:\n    - health\n    - root\n    - status\n    - gpu_status\n    - gpu_acquire\n    - gpu_release\n    - gpu_wait\n    - gpu_sleep\n    - gpu_wake\n    - stop_live\n    - start_live\n    - stop_candidate\n    - start_candidate\n    - swap_service\n    - handoff_prime_to_study\n    - handoff_study_to_prime\n    - handoff_status\n    - oracle_fallback\n    - notifications_ws\n    id: api_layer\n    key_classes:\n    - GPUAcquireRequest\n    - GPUAcquireResponse\n    - HandoffRequest\n    - Notification\n    key_functions:\n    - get_status()\n    - acquire_gpu()\n    - release_gpu()\n    - gpu_sleep()\n    - gpu_wake()\n    - stop_live_stack()\n    - start_live_stack()\n    - swap_service()\n    - handoff_prime_to_study()\n    - handoff_study_to_prime()\n    - websocket_notifications()\n    label: API Layer\n    source_files:\n    - candidates/gaia-orchestrator/gaia_orchestrator/main.py\n    - candidates/gaia-orchestrator/gaia_orchestrator/models/schemas.py\n  - consumes_interfaces: []\n    description: 'GPU lifecycle controller. Tracks GPU ownership (prime/study/none),\n      manages NVML queries for memory state, and coordinates container start/stop\n      for GPU handoffs. Communicates with gaia-core to request GPU release/reclaim\n      and with gaia-study to signal GPU availability.\n\n      '\n    exposes_interfaces: []\n    id: gpu_manager\n    key_classes:\n    - GPUManager\n    key_functions:\n    - GPUManager.get_memory_info()\n    - GPUManager.is_gpu_free()\n    - GPUManager.wait_for_gpu_cleanup()\n    - GPUManager.stop_prime_container()\n    - GPUManager.start_prime_container()\n    - GPUManager.request_release_from_core()\n    - GPUManager.request_reclaim_by_core()\n    - GPUManager.signal_study_gpu_ready()\n    label: GPU Manager\n    source_files:\n    - candidates/gaia-orchestrator/gaia_orchestrator/gpu_manager.py\n  - consumes_interfaces: []\n    description: 'Container orchestration via Docker SDK and docker-compose CLI. Monitors\n      container health, starts/stops live and candidate stacks, swaps individual services\n      between live and candidate versions. Provides unified status view of all containers.\n\n      '\n    exposes_interfaces: []\n    id: docker_manager\n    key_classes:\n    - DockerManager\n    key_functions:\n    - DockerManager.get_status()\n    - DockerManager.stop_live()\n    - DockerManager.start_live()\n    - DockerManager.stop_candidate()\n    - DockerManager.start_candidate()\n    - DockerManager.swap_service()\n    label: Docker Manager\n    source_files:\n    - candidates/gaia-orchestrator/gaia_orchestrator/docker_manager.py\n  - consumes_interfaces: []\n    description: \"Orchestrates GPU handoffs between gaia-prime and gaia-study. A prime-to-study\\\n      \\ handoff stops prime, releases GPU, signals study to claim it for training.\\\n      \\ Study-to-prime reverses the flow. Tracks handoff phases (requested \\u2192\\\n      \\ gpu_releasing \\u2192 gpu_released \\u2192 gpu_acquiring \\u2192 complete) with\\\n      \\ failure handling at each stage.\\n\"\n    exposes_interfaces: []\n    id: handoff_manager\n    key_classes:\n    - HandoffManager\n    key_functions:\n    - HandoffManager.start_prime_to_study()\n    - HandoffManager.start_study_to_prime()\n    - HandoffManager.cancel_handoff()\n    label: Handoff Manager\n    source_files:\n    - candidates/gaia-orchestrator/gaia_orchestrator/handoff_manager.py\n  - consumes_interfaces: []\n    description: 'Persistent state store (GPU ownership, container status, active\n      handoffs) with async context manager for atomic updates. Writes state to disk\n      for crash recovery. NotificationManager broadcasts events (GPU changes, handoff\n      progress, service errors) to connected WebSocket clients and maintains notification\n      history.\n\n      '\n    exposes_interfaces: []\n    id: state_and_notifications\n    key_classes:\n    - StateManager\n    - NotificationManager\n    - OrchestratorState\n    key_functions:\n    - StateManager.get_gpu_status()\n    - StateManager.set_gpu_owner()\n    - StateManager.release_gpu()\n    - StateManager.start_handoff()\n    - NotificationManager.broadcast()\n    - NotificationManager.notify_gpu_released()\n    - NotificationManager.notify_handoff_started()\n    label: State & Notification Manager\n    source_files:\n    - candidates/gaia-orchestrator/gaia_orchestrator/state.py\n    - candidates/gaia-orchestrator/gaia_orchestrator/notification_manager.py\n  edges:\n  - data_flow: \"GPUAcquireRequest \\u2192 GPUAcquireResponse\"\n    from_component: api_layer\n    label: delegates GPU acquire/release/sleep/wake\n    to_component: gpu_manager\n    transport: function_call\n  - data_flow: \"ContainerStartRequest \\u2192 status\"\n    from_component: api_layer\n    label: delegates container start/stop/swap\n    to_component: docker_manager\n    transport: function_call\n  - data_flow: \"HandoffRequest \\u2192 HandoffStatus\"\n    from_component: api_layer\n    label: initiates GPU handoffs\n    to_component: handoff_manager\n    transport: function_call\n  - data_flow: OrchestratorState, Notification\n    from_component: api_layer\n    label: reads state, streams notifications\n    to_component: state_and_notifications\n    transport: function_call\n  - data_flow: container name\n    from_component: gpu_manager\n    label: starts/stops prime container for GPU ops\n    to_component: docker_manager\n    transport: function_call\n  - data_flow: GPUOwner enum\n    from_component: gpu_manager\n    label: updates GPU ownership state\n    to_component: state_and_notifications\n    transport: function_call\n  - data_flow: handoff phase transitions\n    from_component: handoff_manager\n    label: coordinates GPU release/acquire sequence\n    to_component: gpu_manager\n    transport: function_call\n  - data_flow: HandoffStatus updates\n    from_component: handoff_manager\n    label: tracks handoff phases, broadcasts progress\n    to_component: state_and_notifications\n    transport: function_call\ndependencies:\n  external_apis: []\n  services:\n  - fallback: null\n    id: gaia-core\n    required: true\n    role: gpu_negotiation\n  - fallback: null\n    id: gaia-prime\n    required: false\n    role: inference_container\n  - fallback: null\n    id: gaia-study\n    required: false\n    role: training_container\n  volumes:\n  - access: ro\n    mount_path: /var/run/docker.sock\n    name: docker-socket\n    purpose: Docker daemon access for container lifecycle management\n  - access: rw\n    mount_path: /shared\n    name: gaia-shared\n    purpose: State persistence (/shared/orchestrator/state.json)\n  - access: ro\n    mount_path: /gaia/GAIA_Project\n    name: project-root\n    purpose: Compose file access for live + candidate stacks\nfailure_modes:\n- auto_recovers: false\n  condition: Docker daemon unreachable\n  response: Container start/stop endpoints return 500; GPU tracking still functional\n  severity: fatal\n- auto_recovers: false\n  condition: gaia-core GPU release/reclaim fails\n  response: Logs error; GPU lease remains held; handoff may stall\n  severity: partial\n- auto_recovers: false\n  condition: gaia-prime health check timeout (120s)\n  response: GPU wake fails; study cycle exits with timeout\n  severity: partial\n- auto_recovers: false\n  condition: CUDA cleanup timeout (30s)\n  response: Handoff fails with timeout; GPU remains locked to previous owner\n  severity: partial\n- auto_recovers: true\n  condition: State file corruption on disk\n  response: Fresh state initialized on boot; handoff history lost but GPU ownership\n    recovers\n  severity: degraded\n- auto_recovers: true\n  condition: gaia-study unreachable during handoff\n  response: Signal calls log warnings but don't fail handoff; study may miss GPU-ready\n    notification\n  severity: degraded\n- auto_recovers: true\n  condition: Concurrent handoffs triggered\n  response: Handoff manager rejects 2nd handoff with 'already in progress' error\n  severity: degraded\nid: gaia-orchestrator\nintent:\n  cognitive_role: The Coordinator\n  design_decisions:\n  - \"CPU-only by design \\u2014 async handoff orchestration without blocking cognition\"\n  - Single GPU multiplexing via container stop/start (not vLLM sleep) because --enforce-eager\n    prevents full VRAM release\n  - VRAM cleanup uses pynvml if available; falls back to container status check on\n    non-GPU hosts\n  - Atomic state persistence to /shared/orchestrator/state.json with rename to prevent\n    corruption\n  - \"Lazy manager initialization \\u2014 GPU, Docker, Handoff managers init only on\\\n    \\ first use\"\n  - \"Graceful service degradation \\u2014 HTTP calls to study/prime that timeout log\\\n    \\ warnings but don't fail handoff\"\n  - WebSocket history buffer keeps last 100 notifications for new clients to catch\n    up\n  - Docker compose subprocess (not Python SDK) for full compose support (profiles,\n    env overrides)\n  - 'Handoff phases with progress tracking (6 phases: INITIATED -> RELEASING -> CLEANUP\n    -> TRANSFER -> SIGNALING -> COMPLETED)'\n  open_questions:\n  - Should GPU cleanup timeout be dynamic based on model size, or remain fixed at\n    30s?\n  - Should concurrent handoff requests queue, or immediately fail with 409 Conflict?\n  - Is prime health polling sufficient, or should orchestrator have explicit ownership\n    tracking?\n  purpose: 'Central resource broker for GPU allocation and container lifecycle in\n    a single-GPU GAIA system. Enables gaia-prime (inference) and gaia-study (training)\n    to share the GPU via coordinated handoffs. Provides real-time monitoring, container\n    swapping, and graceful state recovery.\n\n    '\ninterfaces:\n- description: Container health check endpoint.\n  direction: inbound\n  id: health\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\n- description: \"Root endpoint \\u2014 full API overview for service discovery.\"\n  direction: inbound\n  id: root\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /\n    type: http_rest\n- description: 'Complete orchestrator state: GPU status, container statuses, active\n    handoff.'\n  direction: inbound\n  id: status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /status\n    type: http_rest\n- description: Current GPU ownership (owner, lease_id, acquired_at, queue).\n  direction: inbound\n  id: gpu_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /gpu/status\n    type: http_rest\n- description: Request GPU ownership; queues if unavailable.\n  direction: inbound\n  id: gpu_acquire\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/acquire\n    type: http_rest\n- description: Release GPU ownership; advances queue to next requester.\n  direction: inbound\n  id: gpu_release\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/release\n    type: http_rest\n- description: Block and wait for GPU availability, then acquire.\n  direction: inbound\n  id: gpu_wait\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/wait\n    type: http_rest\n- description: \"Release GPU for sleep \\u2014 stops prime container, demotes gpu_prime.\\\n    \\ Called by gaia-core SleepCycleLoop.\"\n  direction: inbound\n  id: gpu_sleep\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/sleep\n    type: http_rest\n- description: \"Reclaim GPU after wake \\u2014 starts prime container, restores gpu_prime.\\\n    \\ Called by gaia-core SleepCycleLoop.\"\n  direction: inbound\n  id: gpu_wake\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/wake\n    type: http_rest\n- description: Get all live + candidate service container states.\n  direction: inbound\n  id: containers_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /containers/status\n    type: http_rest\n- description: Start live stack (gaia-core, gaia-web, gaia-mcp, gaia-study).\n  direction: inbound\n  id: containers_live_start\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /containers/live/start\n    type: http_rest\n- description: Stop live stack and release GPU.\n  direction: inbound\n  id: containers_live_stop\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /containers/live/stop\n    type: http_rest\n- description: Start candidate stack with --profile full.\n  direction: inbound\n  id: containers_candidate_start\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /containers/candidate/start\n    type: http_rest\n- description: Stop candidate stack and release GPU if owned.\n  direction: inbound\n  id: containers_candidate_stop\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /containers/candidate/stop\n    type: http_rest\n- description: Swap service between live/candidate; injects candidate into live traffic.\n  direction: inbound\n  id: containers_swap\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /containers/swap\n    type: http_rest\n- description: 'Initiate GPU handoff from prime to study (5-phase: release -> cleanup\n    -> transfer -> signal -> complete).'\n  direction: inbound\n  id: handoff_prime_to_study\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /handoff/prime-to-study\n    type: http_rest\n- description: Initiate GPU handoff from study back to prime.\n  direction: inbound\n  id: handoff_study_to_prime\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /handoff/study-to-prime\n    type: http_rest\n- description: Query handoff status by ID.\n  direction: inbound\n  id: handoff_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /handoff/{handoff_id}/status\n    type: http_rest\n- description: Receive Oracle inference fallback notifications.\n  direction: inbound\n  id: notify_oracle_fallback\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /notify/oracle-fallback\n    type: http_rest\n- description: Real-time notification stream; broadcasts to all connected WebSocket\n    clients.\n  direction: inbound\n  id: ws_notifications\n  status: active\n  transport:\n    path: /ws/notifications\n    protocol: null\n    type: websocket\n- description: Notify gaia-core to demote gpu_prime from model pool.\n  direction: outbound\n  id: core_gpu_release\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/release\n    type: http_rest\n- description: Notify gaia-core to restore gpu_prime to model pool.\n  direction: outbound\n  id: core_gpu_reclaim\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/reclaim\n    type: http_rest\n- description: Notify gaia-core sleep state machine of handoff completion.\n  direction: outbound\n  id: core_study_handoff\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /sleep/study-handoff\n    type: http_rest\n- description: Signal gaia-study that GPU is available for training.\n  direction: outbound\n  id: study_gpu_ready\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /study/gpu-ready\n    type: http_rest\n- description: Request gaia-study release GPU during study-to-prime handoff.\n  direction: outbound\n  id: study_gpu_release\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /study/gpu-release\n    type: http_rest\n- description: Health probe to gaia-prime during container start (waits for model\n    loaded).\n  direction: outbound\n  id: prime_health_probe\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\nmeta:\n  blueprint_version: '0.2'\n  confidence:\n    contract: high\n    dependencies: high\n    failure_modes: medium\n    intent: medium\n    runtime: high\n  created_at: '2026-02-21T15:55:35.786241Z'\n  divergence_score: null\n  generated_by: manual_seed\n  genesis: true\n  last_reflected: null\n  promoted_at: null\n  reflection_notes: null\n  schema_version: '1.0'\n  status: candidate\nrole: The Coordinator (Infrastructure)\nruntime:\n  base_image: python:3.11-slim\n  compose_service: gaia-orchestrator\n  dockerfile: gaia-orchestrator/Dockerfile\n  gpu: false\n  gpu_count: null\n  health_check: curl -f http://localhost:6410/health\n  port: 6410\n  security: null\n  startup_cmd: uvicorn gaia_orchestrator.main:app --host 0.0.0.0 --port 6410\n  user: ${UID}:${GID}\nservice_status: live\nsource_files:\n- file_type: python\n  path: candidates/gaia-orchestrator/gaia_orchestrator/main.py\n  role: entrypoint\n- file_type: python\n  path: candidates/gaia-orchestrator/gaia_orchestrator/gpu_manager.py\n  role: gpu_coordination\n- file_type: python\n  path: candidates/gaia-orchestrator/gaia_orchestrator/docker_manager.py\n  role: container_lifecycle\n- file_type: python\n  path: candidates/gaia-orchestrator/gaia_orchestrator/handoff_manager.py\n  role: handoff_protocol\n- file_type: python\n  path: candidates/gaia-orchestrator/gaia_orchestrator/state.py\n  role: state_persistence\n- file_type: python\n  path: candidates/gaia-orchestrator/gaia_orchestrator/notification_manager.py\n  role: websocket_broadcast\n- file_type: python\n  path: candidates/gaia-orchestrator/gaia_orchestrator/config.py\n  role: configuration\n- file_type: python\n  path: candidates/gaia-orchestrator/gaia_orchestrator/models/schemas.py\n  role: data_models\n- file_type: dockerfile\n  path: candidates/gaia-orchestrator/Dockerfile\n  role: build_config\nversion: '0.5'\n\n\nAVAILABLE GAIA IDIOMS (from reference implementations):\n\n### gaia-mcp\n  approval.py: classes=['ApprovalStore'], functions=[]\n  main.py: classes=[], functions=['startup_event', 'shutdown_event']\n  server.py: classes=[], functions=['get_study_service', 'health_check', 'create_app', 'dispatch_tool', '_send_discord_message_impl', '_ai_write_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_write_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl', 'jsonrpc_endpoint', 'read_root', 'request_approval', 'pending_approvals', 'approve_action']\n  tools.py: classes=[], functions=['list_tools', 'describe_tool', 'execute_tool', '_ai_write_impl', '_write_file_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_get_study_client', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl']\n  web_tools.py: classes=['SourceTrustConfig', '_RateLimiter'], functions=['_get_config', 'web_search', '_ddg_search', 'web_fetch', '_extract_content', '_extract_title_bs4', '_domain_from_url']\n\n### gaia-web\n  discord_interface.py: classes=['DiscordInterface'], functions=['_run_on_bot_loop', 'send_to_channel', 'send_to_user', 'start_discord_bot', 'stop_discord_bot', 'is_bot_ready', 'get_discord_status', 'change_presence_from_external']\n  main.py: classes=[], functions=['_load_constants', 'health_check', 'queue_status', 'root', 'dashboard_redirect', 'system_status_proxy', 'system_services', 'system_sleep_proxy', 'process_user_input', 'process_audio_input', 'update_presence', 'output_router', 'startup_event', 'shutdown_event']\n  queue/message_queue.py: classes=['QueuedMessage', 'MessageQueue'], functions=[]\n  routes/blueprints.py: classes=[], functions=['get_graph', 'list_blueprints', 'get_blueprint_detail', 'get_blueprint_markdown', 'get_component_topology']\n  routes/files.py: classes=['WriteRequest'], functions=['_parse_roots', '_safe_path', 'list_roots', 'browse', 'read_file', 'write_file']\n  routes/hooks.py: classes=['CodexSearchRequest'], functions=['sleep_status', 'sleep_wake', 'sleep_shutdown', 'gpu_status', 'gpu_release', 'gpu_reclaim', 'codex_search']\n  routes/terminal.py: classes=[], functions=['_get_client', 'list_containers', 'terminal_ws']\n  routes/voice.py: classes=['WhitelistAdd'], functions=['_get_vm', 'list_users', 'list_whitelisted', 'add_to_whitelist', 'remove_from_whitelist', 'voice_status', 'force_disconnect']\n  utils/retry.py: classes=[], functions=['_is_maintenance_mode', 'post_with_retry']\n  voice_manager.py: classes=['VoiceWhitelist', 'SimpleVAD', 'GaiaVoiceSink', 'VoiceManager'], functions=['pcm_48k_stereo_to_16k_mono', 'pcm_48k_stereo_to_16k_mono_fast', 'pcm_to_wav_base64']\n\n### gaia-core\n  api/gpu_endpoints.py: classes=['GPUReleaseRequest', 'GPUReclaimRequest'], functions=['_prime_endpoint', '_get_model_pool', 'gpu_status', 'gpu_release', 'gpu_reclaim']\n  api/sleep_endpoints.py: classes=[], functions=['receive_wake_signal', 'voice_state', 'get_sleep_status', 'study_handoff', 'distracted_check', 'shutdown']\n  behavior/persona_adapter.py: classes=['PersonaAdapter'], functions=[]\n  behavior/persona_manager.py: classes=['PersonaManager'], functions=[]\n  behavior/persona_switcher.py: classes=[], functions=['_normalize_text', '_load_persona_config', 'get_knowledge_base_for_persona', 'get_persona_for_knowledge_base', 'get_persona_for_request']\n  behavior/persona_writer.py: classes=['PersonaWriter'], functions=[]\n  cognition/adapter_trigger_system.py: classes=['TriggerRule', 'TriggerMatch', 'AdapterTriggerSystem'], functions=['get_trigger_system']\n  cognition/agent_core.py: classes=['AgentCore'], functions=['_format_retrieved_session_context', 'find_recitable_document']\n  cognition/cognition_packet_v0.2_backup.py: classes=['CognitionPacket'], functions=['create_packet']\n  cognition/cognitive_audit.py: classes=[], functions=['_build_audit_context', '_parse_audit_output', 'run_cognitive_self_audit']\n  cognition/cognitive_dispatcher.py: classes=[], functions=['process_execution_results']\n  cognition/conversation_curator.py: classes=['ConversationCurator'], functions=[]\n  cognition/external_voice.py: classes=['ExternalVoice'], functions=['suppress_llama_stderr', 'extract_and_format_execute_blocks']\n  cognition/goal_detector.py: classes=['GoalDetector'], functions=[]\n  cognition/heartbeat.py: classes=['ThoughtSeedHeartbeat'], functions=[]\n  cognition/history_review.py: classes=[], functions=['_count_violations', '_is_user_correction', '_redact_message', '_build_correction_summary', 'review_history']\n  cognition/initiative_engine.py: classes=['InitiativeEngine'], functions=[]\n  cognition/knowledge_enhancer.py: classes=[], functions=['enhance_packet']\n  cognition/knowledge_ingestion.py: classes=[], functions=['detect_save_command', 'detect_knowledge_dump', 'classify_content', 'check_dedup', '_sanitize_filename', 'format_document', 'write_and_embed', 'run_explicit_save', 'run_auto_detect', 'detect_knowledge_update', 'retrieve_entity_document', 'run_update_detect']\n  cognition/lite_journal.py: classes=['LiteJournal'], functions=[]\n  cognition/loop_detector.py: classes=['LoopDetectorConfig', 'DetectionResult', 'AggregatedResult', 'ToolCallRecord', 'ErrorRecord', 'ToolCallRepetitionDetector', 'OutputSimilarityDetector', 'StateOscillationDetector', 'ErrorCycleDetector', 'TokenPatternDetector', 'LoopDetectionAggregator', 'LoopDetector'], functions=[]\n  cognition/loop_patterns.py: classes=['DescriptionTemplate', 'ClassifiedPattern', 'PatternClassifier', 'PatternRenderer'], functions=[]\n  cognition/loop_recovery.py: classes=['LoopMetadata', 'CapturedState', 'LoopRecoveryManager', 'LoopInterrupt', 'LoopDetectorObserver'], functions=['get_recovery_manager', 'inject_recovery_context_if_needed', 'build_loop_detection_config_from_constants']\n  cognition/nlu/embed_intent_classifier.py: classes=['EmbedIntentClassifier'], functions=[]\n  cognition/nlu/intent_detection.py: classes=['Plan'], functions=['fast_intent_check', '_detect_direct_list_tools', '_detect_tree_request', '_detect_list_files_request', '_detect_read_file_request', '_mentions_file_like_action', '_detect_fragmentation_request', '_detect_tool_routing_request', '_keyword_intent_classify', '_fast_track_intent_detection', 'model_intent_detection', 'detect_intent']\n  cognition/packet_upgrade.py: classes=[], functions=['_ensure', '_ensure_slots', 'upgrade_packet']\n  cognition/packet_utils.py: classes=[], functions=['is_execution_safe', 'upgrade_v2_to_v3_packet']\n  cognition/prime_checkpoint.py: classes=['PrimeCheckpointManager'], functions=[]\n  cognition/self_reflection.py: classes=[], functions=['reflect_and_refine', 'run_self_reflection']\n  cognition/self_review_worker.py: classes=[], functions=['_get_model_pool', '_load_dev_matrix', '_save_dev_matrix', 'run_review_once', 'run_review_with_prompt']\n  cognition/semantic_probe.py: classes=['ProbeHit', 'SemanticProbeResult', 'SessionProbeCache', 'ProbeSessionStats'], functions=['_load_probe_config', '_get_session_cache', '_get_session_stats', 'get_session_probe_stats', 'extract_candidate_phrases', '_probe_single_collection', '_determine_primary_and_supplemental', 'probe_collections', 'should_skip_probe', 'run_semantic_probe']\n  cognition/sleep_cycle_loop.py: classes=['SleepCycleLoop'], functions=[]\n  cognition/sleep_task_scheduler.py: classes=['SleepTask', 'SleepTaskScheduler'], functions=[]\n  cognition/sleep_wake_manager.py: classes=['SleepWakeManager'], functions=[]\n  cognition/telemetric_senses.py: classes=[], functions=['tick', 'update_token_usage', 'scan_files', 'get_gpu_usage', 'get_hardware_profile', 'get_system_resources', 'system_health', 'get_telemetry_summary', 'full_sense_sweep']\n  cognition/temporal_interviewer.py: classes=['TemporalInterviewer'], functions=[]\n  cognition/temporal_state_manager.py: classes=['TemporalStateManager'], functions=[]\n  cognition/tests/test_goal_detector.py: classes=['TestFastPath', 'TestSessionCarry', 'TestLLMDetect', 'TestGoalShift', 'TestEdgeCases'], functions=['_make_packet', '_make_session_manager']\n  cognition/tests/test_heartbeat.py: classes=['TestSeedDirectoryOps', 'TestTriageSeed', 'TestActOnSeed', 'FakeConfig', 'TestHeartbeatLifecycle', 'TestTemporalIntegration'], functions=['_patch_seeds_dirs', '_write_seed', '_mock_llm']\n  cognition/tests/test_initiative_engine.py: classes=['FakeConfig', 'TestNoTopics', 'TestNoAgentCore', 'TestSuccessfulTurn', 'TestSelfPrompt', 'TestErrorHandling'], functions=['config', 'mock_agent_core']\n  cognition/tests/test_lite_journal.py: classes=['TestJournalLifecycle', 'TestRotation', 'TestLoadEntries'], functions=['mock_config', '_mock_llm', '_mock_pool', '_mock_swm']\n  cognition/tests/test_sleep_gpu_integration.py: classes=['TestGPUReleaseOnSleep', 'TestGPUReclaimOnWake', 'TestPresenceDuringSleep', 'TestDreamingPresence', 'TestSOAPresence'], functions=['mock_config', 'mock_discord', 'loop']\n  cognition/tests/test_sleep_task_scheduler.py: classes=['FakeConfig', 'TestRegistration', 'TestScheduling', 'TestExecution', 'TestStatus', 'TestBlueprintValidation'], functions=['config', 'scheduler', 'bare_scheduler']\n  cognition/tests/test_sleep_wake_manager.py: classes=['TestInitialState', 'TestDrowsyThreshold', 'TestInitiateDrowsy', 'TestReceiveWakeSignal', 'TestCompleteWake', 'TestStatus', 'TestFormatCheckpoint', 'TestTransitionToWaking', 'TestCannedResponses', 'TestDreamingTransition', 'TestDistractedTransition', 'TestOffline', 'TestLLMCheckpoint', 'TestCheckpointConsumed', 'TestDrowsyCancelBehavior', 'TestWakeSignalIdleMonitor'], functions=['mock_config', 'manager']\n  cognition/tests/test_stream_observer.py: classes=['DummyLLM'], functions=['_make_packet', 'mock_config', 'observer', 'packet', 'test_verify_thought_seed_file_exists', 'test_verify_thought_seed_file_missing', 'test_verify_thought_seed_file_empty', 'test_verify_sidecar_action_success', 'test_verify_sidecar_action_error', 'test_verify_goal_shift_ok', 'test_verify_no_side_effects', 'test_verify_disabled_by_config', 'test_verify_fallback_thought_seed', 'test_verify_appends_reflection_log', 'test_verify_multiple_issues', 'test_verify_no_packet_returns_ok']\n  cognition/tests/test_temporal_context.py: classes=['TestSemanticTime', 'TestFormatDuration', 'TestSessionSummary', 'TestStateSummary', 'TestCodeEvolutionSummary', 'TestBuildTemporalContext'], functions=[]\n  cognition/tests/test_temporal_interviewer.py: classes=['FakeLlamaState', 'TestInterviewTargetSelection', 'TestInterviewFlow', 'TestLockBehavior', 'TestNarrativeCoherence', 'TestTranscriptStorage', 'FakeConfig', 'TestHeartbeatIntegration'], functions=['mock_config', 'mock_llm', 'mock_model_pool', '_create_baked_state', 'tsm_with_states', 'mock_journal', 'mock_timeline', 'interviewer']\n  cognition/tests/test_temporal_state_manager.py: classes=['FakeLlamaState', 'TestStateDirectory', 'TestBakeState', 'TestLoadState', 'TestRotation', 'TestContextReconstruction'], functions=['mock_config', 'mock_llm', 'mock_model_pool', 'mock_timeline', 'mock_session_manager', 'mock_journal']\n  cognition/thought_seed.py: classes=[], functions=['save_thought_seed', 'list_unreviewed_seeds', 'get_seed_by_id', 'update_seed', 'review_and_process_seeds', 'refine_seed', 'link_seeds', 'maybe_review_seeds', 'archive_seed', 'defer_seed', 'list_pending_seeds_due']\n  cognition/tool_selector.py: classes=[], functions=['_structured_json_kwargs', '_registry_to_catalog', 'needs_tool_routing', 'select_tool', 'review_selection', '_build_tool_catalog', '_extract_content', '_extract_json_from_response', 'initialize_tool_routing', 'inject_tool_result_into_packet']\n  cognition/topic_manager.py: classes=[], functions=['_load_topic_cache', '_save_topic_cache', 'add_topic', 'resolve_topic', 'update_topic', 'prune_resolved_topics', 'list_topics', 'prioritize_topics']\n  config.py: classes=['Config'], functions=['get_config']\n  ethics/consent_protocol.py: classes=['ConsentProtocol'], functions=[]\n  ethics/core_identity_guardian.py: classes=['CoreIdentityGuardian'], functions=[]\n  ethics/ethical_sentinel.py: classes=['EthicalSentinel'], functions=[]\n  integrations/discord_connector.py: classes=['DiscordConnector'], functions=[]\n  main.py: classes=['AIManagerShim', 'MinimalPersona'], functions=['initialize_cognitive_system', 'lifespan', '_write_shutdown_checkpoints', 'health_check', 'root', 'get_status', 'cognition_checkpoint', 'process_packet']\n  memory/codex_writer.py: classes=['CodexWriter'], functions=[]\n  memory/conversation/archiver.py: classes=['ConversationArchiver'], functions=[]\n  memory/conversation/keywords.py: classes=['ConversationKeywordExtractor'], functions=[]\n  memory/conversation/manager.py: classes=['ConversationManager'], functions=[]\n  memory/conversation/summarizer.py: classes=['ConversationSummarizer'], functions=['_get_model_pool']\n  memory/dev_matrix.py: classes=['GAIADevMatrix'], functions=[]\n  memory/knowledge_integrity.py: classes=[], functions=['hash_file', 'check_or_generate_hash_manifest']\n  memory/memory_manager.py: classes=['MemoryManager'], functions=[]\n  memory/priority_manager.py: classes=['GAIAPriorityManager'], functions=[]\n  memory/semantic_codex.py: classes=['CodexEntry', 'SemanticCodex'], functions=[]\n  memory/session_history_indexer.py: classes=['SessionHistoryIndexer'], functions=['_cosine_similarity', '_get_embed_model']\n  memory/session_manager.py: classes=['Session', 'SessionManager'], functions=[]\n  memory/status_tracker.py: classes=['GAIAStatus'], functions=[]\n  memory/tests/test_semantic_codex.py: classes=[], functions=['temp_knowledge_dir', 'mock_config', 'semantic_codex', 'test_write_entry_creates_markdown_file', 'test_load_one_markdown_with_front_matter', 'test_load_one_markdown_missing_symbol', 'test_load_one_markdown_invalid_yaml', 'test_load_one_json_still_works', 'test_iter_files_includes_self_generated_docs', 'test_hot_reload_updates_markdown_entry']\n  memory/tests/test_session_history_indexer.py: classes=['FakeEmbedModel', 'TestInstantiation', 'TestTurnIndexing', 'TestRetrieval', 'TestTopicSummaries', 'TestPersistence', 'TestArchiveAndReset', 'TestGracefulDegradation', 'TestCosineSimilarity'], functions=['clear_singletons', 'persist_dir', 'fake_model', 'patched_model', 'no_model', 'indexer', 'indexer_no_model']\n  memory/tests/test_session_rag_integration.py: classes=['FakeEmbedModel', 'TestSessionManagerIndexingHook', 'TestFormatRetrievedContext', 'TestPromptBuilderTier15', 'TestSlidingWindow', 'TestArchiveFlowIntegration'], functions=['clear_singletons', 'persist_dir', 'fake_model']\n  models/_model_pool_impl.py: classes=['ModelPool'], functions=['_get_sentence_transformer', '_read_manifest', '_ensure_download', 'resolve_model_paths', '_get_gpu_free_total_bytes', '_choose_initial_n_gpu']\n  models/dev_model.py: classes=['DevModel'], functions=[]\n  models/document.py: classes=['DocumentProcessor'], functions=[]\n  models/fine_tune_gaia.py: classes=[], functions=['run_fine_tuning', 'main']\n  models/gemini_model.py: classes=['GeminiAPIModel'], functions=[]\n  models/groq_model.py: classes=['GroqAPIModel'], functions=['_ensure_groq_imported']\n  models/hf_model.py: classes=['HFModel'], functions=[]\n  models/mcp_proxy_model.py: classes=['MCPProxyModel'], functions=[]\n  models/model_manager.py: classes=['ModelManager'], functions=['_model_manager_child_loader', 'get_manager']\n  models/model_pool.py: classes=[], functions=['get_model_pool']\n  models/oracle_model.py: classes=['GPTAPIModel'], functions=[]\n  models/tts.py: classes=['SpeechManager'], functions=[]\n  models/vector_store.py: classes=['VectorStoreManager'], functions=[]\n  models/vllm_model.py: classes=['LoRAAdapterInfo', 'VLLMChatModel'], functions=[]\n  models/vllm_remote_model.py: classes=['VLLMRemoteModel'], functions=[]\n  utils/dev_matrix_analyzer.py: classes=['DevMatrixAnalyzer'], functions=['analyze_dev_matrix']\n  utils/dev_matrix_utils.py: classes=[], functions=['load_dev_matrix', 'save_dev_matrix', 'diff_dev_matrix', 'mark_task_complete']\n  utils/gaia_rescue_helper.py: classes=['GAIARescueHelper'], functions=['_safe_read_text', '_find_first_with_ext', '_helper', 'sketch', 'sketchpad_write', 'show_sketchpad', 'sketchpad_read', 'clear_sketchpad', 'sketchpad_clear', 'load_blueprint', 'load_cheatsheet', 'queue_thought_seed', 'run_shell_safe', 'buffer_and_execute_shell', 'code_read', 'code_span', 'code_symbol', 'code_summarize', 'remember_fact', 'recall_fact', 'get_recent_facts']\n  utils/mcp_client.py: classes=[], functions=['_normalize_endpoint', 'call_jsonrpc', 'dispatch_sidecar_actions', 'ai_read', 'ai_write', 'ai_execute', 'embedding_query', 'request_approval_via_mcp', 'approve_action_via_mcp', 'get_pending_action', 'discover']\n  utils/output_router.py: classes=[], functions=['_strip_think_tags_robust', '_strip_stray_cjk', '_get_destination_registry', 'route_output', '_legacy_destination_to_enum', '_strip_gcp_metadata', '_parse_llm_output_into_packet']\n  utils/packet_builder.py: classes=[], functions=['build_packet_snapshot']\n  utils/packet_templates.py: classes=[], functions=['_trim', '_clean_dict', 'packet_to_template_dict', 'render_gaia_packet_template']\n  utils/prompt_builder.py: classes=[], functions=['build_from_packet', '_build_prompt_core', 'build_prompt']\n  utils/resource_monitor.py: classes=['ResourceMonitor'], functions=['shutdown_monitor']\n  utils/stream_observer.py: classes=['Interrupt', 'StreamObserver'], functions=[]\n  utils/temporal_context.py: classes=[], functions=['build_temporal_context', '_semantic_time', '_format_duration', '_wake_cycle_summary', '_session_summary', '_activity_summary', '_state_summary', '_code_evolution_summary']\n  utils/world_state.py: classes=[], functions=['_uptime_seconds', '_mem_summary', '_load_avg', '_model_paths', '_mcp_tools_sample', '_mcp_tools_full', 'world_state_snapshot', 'world_state_detail', '_capability_affordances', 'format_world_state_snapshot']\n\nGenerate the implementation for: gaia-orchestrator", "output": "## FILE: gaia-orchestrator/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA Orchestrator - GPU and Container Lifecycle Coordination Service.\n\nProvides centralized control for:\n- GPU ownership and handoff between services\n- Container lifecycle management (live/candidate s\"\"\"\n\n## FILE: gaia-orchestrator/config.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Configuration management for GAIA Orchestrator.\n\nLoads settings from environment variables and optional YAML config file.\"\"\"\nclass OrchestratorConfig(BaseSettings):\n    \"\"\"Configuration for the GAIA Orchestrator service.\"\"\"\n\ndef load_yaml_config(config_path: Optional[Path] = None) -> dict: ...\ndef get_config() -> OrchestratorConfig: ...\ndef reset_config() -> None: ...\n\n## FILE: gaia-orchestrator/docker_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Docker management for GAIA Orchestrator.\n\nWraps the Docker SDK to provide container lifecycle operations\nfor live and candidate stacks.\"\"\"\nclass DockerManager():\n    \"\"\"Manages Docker container lifecycle for GAIA services.\"\"\"\n    def __init__(self, state_manager: StateManager): ...\n    def client(self) -> docker.DockerClient: ...\n    def _get_container_state_sync(self, container_name: str) -> ContainerState: ...\n    def _get_container_state(self, container_name: str) -> ContainerState: ...\n    def _check_service_health(self, container_name: str, port: int) -> bool: ...\n    def get_status(self) -> ContainerStatus: ...\n    def _run_compose(self, compose_file: Path, command: List[str], env: Optional[Dict[str, str]] = None) -> Dict: ...\n    def stop_live(self) -> Dict: ...\n    def start_live(self, gpu_enabled: bool = True) -> Dict: ...\n    def stop_candidate(self) -> Dict: ...\n    def start_candidate(self, gpu_enabled: bool = True) -> Dict: ...\n    def swap_service(self, service: str, target: str) -> Dict: ...\n    def stop_container(self, container_name: str) -> bool: ...\n    def start_container(self, container_name: str) -> bool: ...\n\n\n## FILE: gaia-orchestrator/gpu_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GPU management for GAIA Orchestrator.\n\nMonitors GPU state via pynvml and coordinates GPU ownership\nbetween services. Uses Docker container stop/start for VRAM\nrelease/reclaim since vLLM sleep mode can\"\"\"\nclass GPUManager():\n    \"\"\"Manages GPU resources and monitors VRAM usage.\"\"\"\n    def __init__(self, state_manager: StateManager): ...\n    def _ensure_nvml(self) -> bool: ...\n    def get_memory_info(self) -> Optional[GPUMemoryInfo]: ...\n    def is_gpu_free(self) -> bool: ...\n    def wait_for_gpu_cleanup(self, timeout: Optional[float] = None) -> bool: ...\n    def _get_docker_client(self) -> 'docker.DockerClient': ...\n    def stop_prime_container(self) -> bool: ...\n    def start_prime_container(self) -> bool: ...\n    def request_release_from_core(self) -> bool: ...\n    def request_reclaim_by_core(self) -> bool: ...\n    def signal_study_gpu_ready(self) -> bool: ...\n    def signal_study_gpu_release(self) -> bool: ...\n    def shutdown(self): ...\n\n\n## FILE: gaia-orchestrator/handoff_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Handoff manager for GAIA Orchestrator.\n\nCoordinates GPU handoff between services:\n- Prime (Core) <-> Study for training sessions\n- Live <-> Candidate for testing\"\"\"\nclass HandoffManager():\n    \"\"\"Manages GPU handoff operations between services.\"\"\"\n    def __init__(self, state_manager: StateManager, gpu_manager: Optional[GPUManager] = None): ...\n    def _update_handoff_phase(self, handoff: HandoffStatus, phase: HandoffPhase, progress: int, error: Optional[str] = None) -> HandoffStatus: ...\n    def start_prime_to_study(self, request: HandoffRequest) -> HandoffStatus: ...\n    def start_study_to_prime(self, request: HandoffRequest) -> HandoffStatus: ...\n    def _notify_study_handoff(self, direction: str, handoff_id: str) -> None: ...\n    def cancel_handoff(self, handoff_id: str) -> HandoffStatus: ...\n\n\n## FILE: gaia-orchestrator/health_watchdog.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Health Watchdog for GAIA Orchestrator â€” HA-aware edition.\n\nBackground asyncio task that monitors both live and candidate service health.\nTracks consecutive failures, derives an HA status, and broadcas\"\"\"\nclass HealthWatchdog():\n    \"\"\"Monitors live + candidate service health, derives HA status.\"\"\"\n    def __init__(self, notification_manager = None) -> None: ...\n    def start(self) -> None: ...\n    def stop(self) -> None: ...\n    def _poll_loop(self) -> None: ...\n    def _poll_service(self, name: str, url: str, registry: Dict[str, bool]) -> None: ...\n    def _check_health(self, name: str, url: str) -> bool: ...\n    def _evaluate_ha_status(self) -> None: ...\n    def _run_session_sync(self) -> None: ...\n    def _broadcast_state_change(self, service_name: str, old_state: str, new_state: str) -> None: ...\n    def _broadcast_ha_change(self, old_status: HAStatus, new_status: HAStatus) -> None: ...\n    def _is_maintenance_mode() -> bool: ...\n    def get_status(self) -> Dict: ...\n\n\n## FILE: gaia-orchestrator/main.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA Orchestrator - FastAPI Application.\n\nCentral coordination service for GPU resources and container lifecycle.\"\"\"\ndef lifespan(app: FastAPI): ...\ndef health_check(): ...\ndef root(): ...\ndef get_status(): ...\ndef get_gpu_status() -> GPUStatus: ...\ndef _acquire_gpu_inner(request: GPUAcquireRequest) -> GPUAcquireResponse: ...\ndef acquire_gpu(request: GPUAcquireRequest) -> GPUAcquireResponse: ...\ndef release_gpu(lease_id: Optional[str] = None): ...\ndef wait_for_gpu(request: GPUAcquireRequest) -> GPUAcquireResponse: ...\ndef get_container_status() -> ContainerStatus: ...\ndef stop_live_stack(): ...\ndef start_live_stack(request: ContainerStartRequest): ...\ndef stop_candidate_stack(): ...\ndef start_candidate_stack(request: ContainerStartRequest): ...\ndef swap_service(request: ContainerSwapRequest): ...\ndef handoff_prime_to_study(request: HandoffRequest = None): ...\ndef handoff_study_to_prime(request: HandoffRequest = None): ...\ndef get_handoff_status(handoff_id: str) -> HandoffStatus: ...\ndef gpu_sleep(): ...\ndef gpu_wake(): ...\ndef notify_oracle_fallback(notification: OracleNotification): ...\ndef websocket_notifications(websocket: WebSocket): ...\ndef main(): ...\n\n## FILE: gaia-orchestrator/models/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Pydantic models for request/response validation.\"\"\"\n\n## FILE: gaia-orchestrator/models/schemas.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Pydantic models for the GAIA Orchestrator API.\n\nDefines request/response schemas for GPU management, container lifecycle,\nhandoff protocol, and notifications.\"\"\"\nclass GPUAcquireRequest(BaseModel):\n    \"\"\"Request to acquire GPU ownership.\"\"\"\n\nclass GPUAcquireResponse(BaseModel):\n    \"\"\"Response to GPU acquire request.\"\"\"\n\nclass GPUMemoryInfo(BaseModel):\n    \"\"\"GPU memory status.\"\"\"\n\nclass GPUStatus(BaseModel):\n    \"\"\"Current GPU ownership and state.\"\"\"\n\nclass ServiceHealth(BaseModel):\n    \"\"\"Health status of a single service.\"\"\"\n\nclass ContainerStatus(BaseModel):\n    \"\"\"Status of all containers in a stack.\"\"\"\n\nclass ContainerStartRequest(BaseModel):\n    \"\"\"Request to start containers.\"\"\"\n\nclass ContainerSwapRequest(BaseModel):\n    \"\"\"Request to swap a service from live to candidate.\"\"\"\n\nclass HandoffRequest(BaseModel):\n    \"\"\"Request to initiate a GPU handoff.\"\"\"\n\nclass HandoffStatus(BaseModel):\n    \"\"\"Status of a handoff operation.\"\"\"\n\nclass OracleNotification(BaseModel):\n    \"\"\"Notification when Oracle fallback is used.\"\"\"\n\nclass Notification(BaseModel):\n    \"\"\"Generic notification message.\"\"\"\n\nclass OrchestratorState(BaseModel):\n    \"\"\"Complete orchestrator state for persistence.\"\"\"\n\n\n## FILE: gaia-orchestrator/notification_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Notification manager for GAIA Orchestrator.\n\nManages WebSocket connections and broadcasts notifications\nto connected clients (gaia-web, Discord bot, etc.).\"\"\"\nclass NotificationManager():\n    \"\"\"Manages WebSocket connections and notification broadcasting.\"\"\"\n    def __init__(self): ...\n    def connect(self, websocket: WebSocket) -> None: ...\n    def disconnect(self, websocket: WebSocket) -> None: ...\n    def broadcast(self, notification: Notification) -> int: ...\n    def send_to(self, websocket: WebSocket, notification: Notification) -> bool: ...\n    def get_history(self, limit: int = 50) -> List[Notification]: ...\n    def connection_count(self) -> int: ...\n    def notify_oracle_fallback(self, fallback_model: str, original_role: str, reason: str = '') -> int: ...\n    def notify_gpu_released(self, previous_owner: str, reason: str = '') -> int: ...\n    def notify_gpu_acquired(self, new_owner: str, reason: str = '') -> int: ...\n    def notify_handoff_started(self, handoff_id: str, handoff_type: str, source: str, destination: str) -> int: ...\n    def notify_handoff_completed(self, handoff_id: str, handoff_type: str, source: str, destination: str) -> int: ...\n    def notify_handoff_failed(self, handoff_id: str, error: str) -> int: ...\n    def notify_service_error(self, service: str, error: str) -> int: ...\n\n\n## FILE: gaia-orchestrator/state.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"State persistence for GAIA Orchestrator.\n\nManages saving/loading orchestrator state to disk for crash recovery\nand state inspection.\"\"\"\nclass StateManager():\n    \"\"\"Manages orchestrator state with disk persistence.\"\"\"\n    def __init__(self, state_dir: Optional[Path] = None): ...\n    def initialize(self) -> None: ...\n    def _reconcile_stale_handoff(self) -> None: ...\n    def _load_state(self) -> None: ...\n    def _save_state(self) -> None: ...\n    def save(self) -> None: ...\n    def modify(self): ...\n    def state(self) -> OrchestratorState: ...\n    def get_gpu_status(self) -> GPUStatus: ...\n    def set_gpu_owner(self, owner: GPUOwner, lease_id: str, reason: str) -> None: ...\n    def release_gpu(self) -> None: ...\n    def add_to_gpu_queue(self, requester: str) -> int: ...\n    def remove_from_gpu_queue(self, requester: str) -> None: ...\n    def get_container_status(self) -> ContainerStatus: ...\n    def update_container_status(self, status: ContainerStatus) -> None: ...\n    def get_active_handoff(self) -> Optional[HandoffStatus]: ...\n    def start_handoff(self, handoff: HandoffStatus) -> None: ...\n    def update_handoff(self, handoff: HandoffStatus) -> None: ...\n    def complete_handoff(self, handoff: HandoffStatus) -> None: ...\n    def get_handoff_by_id(self, handoff_id: str) -> Optional[HandoffStatus]: ...\n\ndef get_state_manager() -> StateManager: ...\ndef reset_state_manager() -> None: ...\n", "fidelity": 0.7, "weight": 1.0}
{"pair_id": "a8a1b6f2-0bd0-421b-83d8-227eb8357912", "service_id": "gaia-core", "pair_type": "forward", "instruction": "You are GAIA's code-architect. Generate Python source code that faithfully\nimplements the following blueprint. Your output must satisfy all contract,\ndependency, failure mode, and intent specifications exactly.\n\nBLUEPRINT:\narchitecture:\n  components:\n  - consumes_interfaces: []\n    description: 'FastAPI entrypoint and endpoint routers. Receives HTTP requests,\n      deserializes CognitionPackets, and routes to the cognition engine. Includes\n      GPU lifecycle endpoints (release/reclaim) and sleep/wake control endpoints used\n      by gaia-orchestrator and gaia-web respectively.\n\n      '\n    exposes_interfaces:\n    - process_packet\n    - health\n    - root\n    - status\n    - gpu_status\n    - gpu_release\n    - gpu_reclaim\n    - sleep_wake\n    - sleep_status\n    - sleep_study_handoff\n    - sleep_distracted_check\n    - sleep_shutdown\n    id: api_layer\n    key_classes:\n    - AIManagerShim\n    - GPUReleaseRequest\n    - GPUReclaimRequest\n    key_functions:\n    - initialize_cognitive_system()\n    - process_packet(packet_data)\n    - gpu_status()\n    - gpu_release(request)\n    - gpu_reclaim(request)\n    - receive_wake_signal(request)\n    - get_sleep_status()\n    - study_handoff(request)\n    label: API Layer\n    source_files:\n    - candidates/gaia-core/gaia_core/main.py\n    - candidates/gaia-core/gaia_core/api/gpu_endpoints.py\n    - candidates/gaia-core/gaia_core/api/sleep_endpoints.py\n  - consumes_interfaces: []\n    description: 'Core reasoning loop. AgentCore runs multi-turn reasoning with self-improvement.\n      CognitiveDispatcher processes execution results. GoalDetector identifies user\n      intent via fast-path heuristics or LLM classification. ToolSelector decides\n      if/which MCP tools are needed. ExternalVoice generates LLM responses. SelfReflection\n      reviews output quality before delivery.\n\n      '\n    exposes_interfaces: []\n    id: cognition_engine\n    key_classes:\n    - AgentCore\n    - GoalDetector\n    - ExternalVoice\n    key_functions:\n    - AgentCore.run_turn()\n    - AgentCore.run_self_improvement()\n    - process_execution_results()\n    - GoalDetector.detect()\n    - needs_tool_routing()\n    - select_tool()\n    - ExternalVoice.stream_response()\n    - reflect_and_refine()\n    - enhance_packet()\n    label: Cognition Engine\n    source_files:\n    - candidates/gaia-core/gaia_core/cognition/agent_core.py\n    - candidates/gaia-core/gaia_core/cognition/cognitive_dispatcher.py\n    - candidates/gaia-core/gaia_core/cognition/goal_detector.py\n    - candidates/gaia-core/gaia_core/cognition/tool_selector.py\n    - candidates/gaia-core/gaia_core/cognition/external_voice.py\n    - candidates/gaia-core/gaia_core/cognition/self_reflection.py\n    - candidates/gaia-core/gaia_core/cognition/knowledge_enhancer.py\n    - candidates/gaia-core/gaia_core/cognition/knowledge_ingestion.py\n  - consumes_interfaces: []\n    description: \"Manages ACTIVE \\u2192 DROWSY \\u2192 ASLEEP \\u2192 WAKING lifecycle.\\\n      \\ SleepCycleLoop is the main event loop driving state transitions. SleepWakeManager\\\n      \\ is the state machine (GaiaState enum). PrimeCheckpointManager writes LLM-generated\\\n      \\ introspective checkpoints (prime.md) on sleep and marks them consumed on wake.\\\n      \\ SleepTaskScheduler runs background tasks (conversation curation, thought seed\\\n      \\ review, blueprint validation) during sleep windows.\\n\"\n    exposes_interfaces: []\n    id: sleep_system\n    key_classes:\n    - SleepCycleLoop\n    - GaiaState\n    - SleepWakeManager\n    - PrimeCheckpointManager\n    - SleepTaskScheduler\n    - SleepTask\n    key_functions:\n    - SleepCycleLoop.start()\n    - SleepWakeManager.initiate_drowsy()\n    - SleepWakeManager.receive_wake_signal()\n    - SleepWakeManager.complete_wake()\n    - PrimeCheckpointManager.create_checkpoint()\n    - PrimeCheckpointManager.mark_consumed()\n    - SleepTaskScheduler.execute_task()\n    label: Sleep/Wake System\n    source_files:\n    - candidates/gaia-core/gaia_core/cognition/sleep_cycle_loop.py\n    - candidates/gaia-core/gaia_core/cognition/sleep_wake_manager.py\n    - candidates/gaia-core/gaia_core/cognition/prime_checkpoint.py\n    - candidates/gaia-core/gaia_core/cognition/sleep_task_scheduler.py\n  - consumes_interfaces: []\n    description: 'Four-tier memory architecture. SessionManager handles per-conversation\n      history with summarize-and-archive. SemanticCodex provides mid-term cross-session\n      recall via symbol-indexed entries. SessionHistoryIndexer builds per-session\n      vector indices for semantic retrieval within a conversation. MemoryManager is\n      the top-level singleton coordinating short/long-term access. CodexWriter generates\n      structured codex entries from conversations using LLM refinement.\n\n      '\n    exposes_interfaces: []\n    id: memory_system\n    key_classes:\n    - SessionManager\n    - Session\n    - SemanticCodex\n    - CodexEntry\n    - MemoryManager\n    - SessionHistoryIndexer\n    - CodexWriter\n    key_functions:\n    - SessionManager.get_or_create_session()\n    - SessionManager.add_message()\n    - SessionManager.summarize_and_archive()\n    - SemanticCodex.write_entry()\n    - SemanticCodex.search()\n    - MemoryManager.query_long()\n    label: Memory System\n    source_files:\n    - candidates/gaia-core/gaia_core/memory/session_manager.py\n    - candidates/gaia-core/gaia_core/memory/semantic_codex.py\n    - candidates/gaia-core/gaia_core/memory/memory_manager.py\n    - candidates/gaia-core/gaia_core/memory/session_history_indexer.py\n    - candidates/gaia-core/gaia_core/memory/codex_writer.py\n  - consumes_interfaces:\n    - prime_inference\n    - prime_health\n    description: 'Unified inference abstraction over multiple LLM backends. ModelPool\n      provides role-based model selection (primary, fallback, lite, oracle). VLLMRemoteModel\n      calls gaia-prime''s OpenAI-compatible API. GroqAPIModel provides free-tier cloud\n      fallback. GPTAPIModel and GeminiAPIModel are oracle backends for high-quality\n      reasoning. All backends implement create_chat_completion() for uniform access.\n      Supports LoRA adapter hot-loading and health-checked failover.\n\n      '\n    exposes_interfaces: []\n    id: model_pool\n    key_classes:\n    - ModelPool\n    - ModelManager\n    - VLLMRemoteModel\n    - GroqAPIModel\n    - GPTAPIModel\n    - GeminiAPIModel\n    key_functions:\n    - ModelPool.get_model_for_role()\n    - ModelPool.forward_to_model()\n    - ModelPool.load_prime_only()\n    - VLLMRemoteModel.create_chat_completion()\n    - VLLMRemoteModel.health_check()\n    label: Model Pool\n    source_files:\n    - candidates/gaia-core/gaia_core/models/_model_pool_impl.py\n    - candidates/gaia-core/gaia_core/models/model_manager.py\n    - candidates/gaia-core/gaia_core/models/vllm_remote_model.py\n    - candidates/gaia-core/gaia_core/models/groq_model.py\n    - candidates/gaia-core/gaia_core/models/oracle_model.py\n    - candidates/gaia-core/gaia_core/models/gemini_model.py\n  - consumes_interfaces:\n    - mcp_dispatch\n    - mcp_approval\n    description: 'JSON-RPC client for communicating with gaia-mcp. Provides typed\n      wrappers for all MCP tool methods: file I/O, shell execution, vector queries,\n      knowledge management, and fragment assembly. Includes approval workflow integration\n      for sensitive operations.\n\n      '\n    exposes_interfaces: []\n    id: tool_dispatch\n    key_classes: []\n    key_functions:\n    - call_jsonrpc()\n    - dispatch_sidecar_actions()\n    - ai_read()\n    - ai_write()\n    - ai_execute()\n    - embedding_query()\n    - request_approval_via_mcp()\n    - discover()\n    label: Tool Dispatch (MCP Client)\n    source_files:\n    - candidates/gaia-core/gaia_core/utils/mcp_client.py\n  - consumes_interfaces: []\n    description: \"Builds the final LLM prompt from conversation history, knowledge\\\n      \\ context, persona config, sleep checkpoint injection, and system instructions.\\\n      \\ OutputRouter post-processes LLM output \\u2014 strips think tags, CJK artifacts,\\\n      \\ and GCP metadata, then parses structured fields back into the CognitionPacket.\\n\"\n    exposes_interfaces: []\n    id: prompt_assembly\n    key_classes: []\n    key_functions:\n    - build_from_packet()\n    - build_prompt()\n    - route_output()\n    - _parse_llm_output_into_packet()\n    label: Prompt Assembly & Output Routing\n    source_files:\n    - candidates/gaia-core/gaia_core/utils/prompt_builder.py\n    - candidates/gaia-core/gaia_core/utils/output_router.py\n  - consumes_interfaces: []\n    description: \"Detects and recovers from cognitive loops \\u2014 repeated tool calls,\\\n      \\ similar outputs, state oscillation, error cycles, and token pattern repetition.\\\n      \\ LoopDetector aggregates 5 specialized detectors. LoopRecoveryManager captures\\\n      \\ state and injects recovery context. StreamObserver monitors output quality\\\n      \\ in real-time. ResourceMonitor tracks system resource pressure for distracted-state\\\n      \\ detection.\\n\"\n    exposes_interfaces: []\n    id: resilience\n    key_classes:\n    - LoopDetector\n    - LoopDetectionAggregator\n    - LoopRecoveryManager\n    - StreamObserver\n    - ResourceMonitor\n    key_functions:\n    - LoopDetector.check()\n    - LoopDetector.trigger_reset()\n    - LoopRecoveryManager.check_and_handle()\n    - StreamObserver.observe()\n    - ResourceMonitor.is_distracted()\n    label: Resilience & Loop Detection\n    source_files:\n    - candidates/gaia-core/gaia_core/cognition/loop_detector.py\n    - candidates/gaia-core/gaia_core/cognition/loop_recovery.py\n    - candidates/gaia-core/gaia_core/cognition/loop_patterns.py\n    - candidates/gaia-core/gaia_core/utils/stream_observer.py\n    - candidates/gaia-core/gaia_core/utils/resource_monitor.py\n  edges:\n  - data_flow: CognitionPacket\n    from_component: api_layer\n    label: dispatches CognitionPackets to\n    to_component: cognition_engine\n    transport: function_call\n  - data_flow: wake signal, sleep commands\n    from_component: api_layer\n    label: routes sleep/wake/GPU endpoints to\n    to_component: sleep_system\n    transport: function_call\n  - data_flow: \"messages[] \\u2192 completion text\"\n    from_component: cognition_engine\n    label: requests LLM completions via\n    to_component: model_pool\n    transport: function_call\n  - data_flow: JSON-RPC method + params\n    from_component: cognition_engine\n    label: sends tool execution requests to\n    to_component: tool_dispatch\n    transport: function_call\n  - data_flow: session_id, messages, codex entries\n    from_component: cognition_engine\n    label: reads/writes session state and knowledge\n    to_component: memory_system\n    transport: function_call\n  - data_flow: \"context \\u2192 prompt, output \\u2192 parsed packet\"\n    from_component: cognition_engine\n    label: builds prompts and routes output via\n    to_component: prompt_assembly\n    transport: function_call\n  - data_flow: tool calls, outputs, state transitions\n    from_component: cognition_engine\n    label: monitors for loops and quality issues\n    to_component: resilience\n    transport: function_call\n  - data_flow: \"metacognitive prompt \\u2192 checkpoint text\"\n    from_component: sleep_system\n    label: generates LLM checkpoint via lite model\n    to_component: model_pool\n    transport: function_call\n  - data_flow: session summary text\n    from_component: sleep_system\n    label: reads evolving summary for checkpoint context\n    to_component: memory_system\n    transport: function_call\n  - data_flow: \"SleepTask \\u2192 task results\"\n    from_component: sleep_system\n    label: runs sleep tasks (curation, thought seeds)\n    to_component: cognition_engine\n    transport: function_call\n  - data_flow: session history, codex results\n    from_component: prompt_assembly\n    label: retrieves history and knowledge context\n    to_component: memory_system\n    transport: function_call\ndependencies:\n  external_apis:\n  - name: groq\n    purpose: inference_fallback_when_prime_unavailable\n    required: false\n  - name: openai\n    purpose: oracle_tier_inference\n    required: false\n  - name: gemini\n    purpose: oracle_tier_inference\n    required: false\n  services:\n  - fallback: groq-api\n    id: gaia-prime\n    required: false\n    role: inference\n  - fallback: null\n    id: gaia-mcp\n    required: false\n    role: tool_execution\n  - fallback: null\n    id: gaia-web\n    required: true\n    role: output_routing\n  - fallback: null\n    id: gaia-orchestrator\n    required: false\n    role: gpu_lifecycle\n  volumes:\n  - access: rw\n    mount_path: /shared\n    name: gaia-shared\n    purpose: Session state, cognitive checkpoints, prime.md sleep notes\n  - access: ro\n    mount_path: /knowledge\n    name: knowledge\n    purpose: Blueprints, semantic codex, recitable docs\n  - access: rw\n    mount_path: /vector_store\n    name: vector_store\n    purpose: FAISS vector indices for long-term memory\n  - access: ro\n    mount_path: /models\n    name: models\n    purpose: LoRA adapters (json-architect, persona weights)\n  - access: rw\n    mount_path: /logs\n    name: logs\n    purpose: Structured cognition logs, heartbeat telemetry\nfailure_modes:\n- auto_recovers: true\n  condition: gaia-prime unavailable\n  response: Falls back to Groq API; if Groq unavailable, falls back to local GGUF\n    model\n  severity: degraded\n- auto_recovers: true\n  condition: gaia-mcp unavailable\n  response: Tool calls skipped; responds with capability_unavailable in packet\n  severity: degraded\n- auto_recovers: false\n  condition: All inference backends unavailable\n  response: Returns error packet to gaia-web; session preserved for retry\n  severity: partial\n- auto_recovers: true\n  condition: Session state corruption\n  response: Clears session, starts fresh, logs incident to heartbeat\n  severity: degraded\n- auto_recovers: true\n  condition: gaia-orchestrator unreachable\n  response: Sleep/wake GPU handoff skipped; gaia-core retains GPU, study cycle deferred\n  severity: degraded\n- auto_recovers: true\n  condition: Checkpoint write failure\n  response: Sleep proceeds without checkpoint; next wake has no prime.md restoration\n    context\n  severity: degraded\nid: gaia-core\nintent:\n  cognitive_role: The Brain\n  design_decisions:\n  - CPU-only runtime enables GPU handoffs between prime and study without blocking\n    the cognition loop\n  - \"Falls back through groq \\u2192 gguf rather than hard-failing \\u2014 uptime over\\\n    \\ raw capability\"\n  - gaia-web owns output routing so core remains interface-agnostic\n  - 'Four-tier memory: session (short-term), semantic codex (mid-term), vector store\n    (long-term), prime.md checkpoint (sleep continuity)'\n  - Guided decoding via json-architect LoRA adapter for reliable structured output\n    from smaller models\n  - \"Sleep/wake cognitive continuity \\u2014 LLM-generated checkpoint captures introspective\\\n    \\ state, consumed-sentinel prevents stale injection\"\n  - \"Parallel wake strategy \\u2014 GPU reclaim and checkpoint load happen concurrently\\\n    \\ for faster wake\"\n  - Human-in-the-loop approval flow for destructive tool calls via gaia-mcp /request_approval\n  open_questions:\n  - Should reflection loop depth be dynamic based on query complexity or fixed per\n    persona?\n  - \"Semantic codex hot-reload interval is hardcoded \\u2014 should it be configurable\\\n    \\ via gaia_constants.json?\"\n  purpose: 'The cognitive engine of GAIA. Runs the full reasoning loop: intent detection,\n    tool routing, multi-step reflection, and response generation. Deliberately CPU-only\n    to allow gaia-prime and gaia-study to share the GPU without interrupting cognition.\n    All inference is delegated to gaia-prime, with graceful fallback chains preserving\n    uptime across backend failures.\n\n    '\ninterfaces:\n- description: Primary cognition entry point. Receives CognitionPackets from gaia-web,\n    runs the full reasoning loop, returns completed packet.\n  direction: inbound\n  id: process_packet\n  status: active\n  transport:\n    input_schema: CognitionPacket\n    method: POST\n    output_schema: CognitionPacket\n    path: /process_packet\n    type: http_rest\n- description: Container health check endpoint.\n  direction: inbound\n  id: health\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\n- description: \"Root endpoint \\u2014 returns list of available endpoints for service\\\n    \\ discovery.\"\n  direction: inbound\n  id: root\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /\n    type: http_rest\n- description: \"Cognitive status \\u2014 current state, uptime, active sessions.\"\n  direction: inbound\n  id: status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /status\n    type: http_rest\n- description: \"GPU allocation state \\u2014 owned, released, or unavailable.\"\n  direction: inbound\n  id: gpu_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /gpu/status\n    type: http_rest\n- description: Release GPU to orchestrator pool. Called by gaia-orchestrator.\n  direction: inbound\n  id: gpu_release\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/release\n    type: http_rest\n- description: Reclaim GPU from orchestrator pool. Called by gaia-orchestrator.\n  direction: inbound\n  id: gpu_reclaim\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/reclaim\n    type: http_rest\n- description: \"Wake signal from gaia-web \\u2014 triggers transition from ASLEEP to\\\n    \\ WAKING.\"\n  direction: inbound\n  id: sleep_wake\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /sleep/wake\n    type: http_rest\n- description: Query current sleep state (ACTIVE, DROWSY, ASLEEP, WAKING).\n  direction: inbound\n  id: sleep_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /sleep/status\n    type: http_rest\n- description: \"Study handoff endpoint \\u2014 orchestrator signals study cycle complete,\\\n    \\ GPU available.\"\n  direction: inbound\n  id: sleep_study_handoff\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /sleep/study-handoff\n    type: http_rest\n- description: Check if GAIA is asleep and should return a canned/distracted response.\n  direction: inbound\n  id: sleep_distracted_check\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /sleep/distracted-check\n    type: http_rest\n- description: \"Graceful shutdown \\u2014 completes current cycle, writes checkpoint,\\\n    \\ enters sleep.\"\n  direction: inbound\n  id: sleep_shutdown\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /sleep/shutdown\n    type: http_rest\n- description: LLM inference requests to gaia-prime via OpenAI-compatible API.\n  direction: outbound\n  id: prime_inference\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /v1/chat/completions\n    type: http_rest\n- description: \"Health probe to gaia-prime on boot \\u2014 sets prime_available flag.\"\n  direction: outbound\n  id: prime_health\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\n- description: Tool execution requests dispatched to gaia-mcp via JSON-RPC.\n  direction: outbound\n  id: mcp_dispatch\n  status: active\n  transport:\n    methods:\n    - run_shell\n    - write_file\n    - read_file\n    - vector_query\n    - memory_rebuild_index\n    - request_approval\n    target_service: gaia-mcp\n    type: mcp\n- description: Tool approval requests sent to gaia-mcp for human-in-the-loop confirmation.\n  direction: outbound\n  id: mcp_approval\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /request_approval\n    type: http_rest\n- description: \"Notify orchestrator that gaia-core is entering sleep \\u2014 GPU available\\\n    \\ for study.\"\n  direction: outbound\n  id: orchestrator_gpu_sleep\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/sleep\n    type: http_rest\n- description: \"Notify orchestrator that gaia-core is waking \\u2014 request GPU reclamation.\"\n  direction: outbound\n  id: orchestrator_gpu_wake\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/wake\n    type: http_rest\n- description: \"Presence updates to gaia-web \\u2014 online/typing/sleeping status\\\n    \\ for Discord.\"\n  direction: outbound\n  id: web_presence\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /presence\n    type: http_rest\nmeta:\n  blueprint_version: '0.2'\n  confidence:\n    contract: high\n    dependencies: high\n    failure_modes: medium\n    intent: medium\n    runtime: high\n  created_at: '2026-02-21T15:55:35.767496Z'\n  divergence_score: null\n  generated_by: manual_seed\n  genesis: true\n  last_reflected: null\n  promoted_at: null\n  reflection_notes: null\n  schema_version: '1.0'\n  status: candidate\nrole: The Brain (Cognition)\nruntime:\n  base_image: python:3.11-slim\n  compose_service: gaia-core\n  dockerfile: gaia-core/Dockerfile\n  gpu: false\n  gpu_count: null\n  health_check: curl -f http://localhost:6415/health\n  port: 6415\n  security: null\n  startup_cmd: uvicorn gaia_core.main:app --host 0.0.0.0 --port 6415\n  user: ${UID}:${GID}\nservice_status: live\nsource_files:\n- file_type: python\n  path: candidates/gaia-core/gaia_core/main.py\n  role: entrypoint\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/cognitive_dispatcher.py\n  role: core_logic\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/tool_selector.py\n  role: tool_routing\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/goal_detector.py\n  role: intent_detection\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/sleep_cycle_loop.py\n  role: sleep_lifecycle\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/sleep_wake_manager.py\n  role: sleep_state_machine\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/prime_checkpoint.py\n  role: cognitive_checkpoint\n- file_type: python\n  path: candidates/gaia-core/gaia_core/api/gpu_endpoints.py\n  role: gpu_api\n- file_type: python\n  path: candidates/gaia-core/gaia_core/api/sleep_endpoints.py\n  role: sleep_api\n- file_type: python\n  path: candidates/gaia-core/gaia_core/memory/session_manager.py\n  role: session_management\n- file_type: python\n  path: candidates/gaia-core/gaia_core/memory/semantic_codex.py\n  role: mid_term_memory\n- file_type: python\n  path: candidates/gaia-core/gaia_core/models/vllm_remote_model.py\n  role: inference_client\n- file_type: python\n  path: candidates/gaia-core/gaia_core/utils/mcp_client.py\n  role: tool_dispatch\n- file_type: python\n  path: candidates/gaia-core/gaia_core/utils/prompt_builder.py\n  role: prompt_assembly\n- file_type: json\n  path: gaia-common/gaia_common/constants/gaia_constants.json\n  role: config\nversion: '0.5'\n\n\nAVAILABLE GAIA IDIOMS (from reference implementations):\n\n### gaia-orchestrator\n  config.py: classes=['OrchestratorConfig'], functions=['load_yaml_config', 'get_config', 'reset_config']\n  docker_manager.py: classes=['DockerManager'], functions=[]\n  gpu_manager.py: classes=['GPUManager'], functions=[]\n  handoff_manager.py: classes=['HandoffManager'], functions=[]\n  health_watchdog.py: classes=['HealthWatchdog'], functions=[]\n  main.py: classes=[], functions=['lifespan', 'health_check', 'root', 'get_status', 'get_gpu_status', '_acquire_gpu_inner', 'acquire_gpu', 'release_gpu', 'wait_for_gpu', 'get_container_status', 'stop_live_stack', 'start_live_stack', 'stop_candidate_stack', 'start_candidate_stack', 'swap_service', 'handoff_prime_to_study', 'handoff_study_to_prime', 'get_handoff_status', 'gpu_sleep', 'gpu_wake', 'notify_oracle_fallback', 'websocket_notifications', 'main']\n  models/schemas.py: classes=['GPUAcquireRequest', 'GPUAcquireResponse', 'GPUMemoryInfo', 'GPUStatus', 'ServiceHealth', 'ContainerStatus', 'ContainerStartRequest', 'ContainerSwapRequest', 'HandoffRequest', 'HandoffStatus', 'OracleNotification', 'Notification', 'OrchestratorState'], functions=[]\n  notification_manager.py: classes=['NotificationManager'], functions=[]\n  state.py: classes=['StateManager'], functions=['get_state_manager', 'reset_state_manager']\n\n### gaia-mcp\n  approval.py: classes=['ApprovalStore'], functions=[]\n  main.py: classes=[], functions=['startup_event', 'shutdown_event']\n  server.py: classes=[], functions=['get_study_service', 'health_check', 'create_app', 'dispatch_tool', '_send_discord_message_impl', '_ai_write_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_write_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl', 'jsonrpc_endpoint', 'read_root', 'request_approval', 'pending_approvals', 'approve_action']\n  tools.py: classes=[], functions=['list_tools', 'describe_tool', 'execute_tool', '_ai_write_impl', '_write_file_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_get_study_client', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl']\n  web_tools.py: classes=['SourceTrustConfig', '_RateLimiter'], functions=['_get_config', 'web_search', '_ddg_search', 'web_fetch', '_extract_content', '_extract_title_bs4', '_domain_from_url']\n\nGenerate the implementation for: gaia-core", "output": "## FILE: gaia-core/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia-core: The Brain - Cognitive loop and reasoning engine.\n\nThis service is the heart of GAIA, responsible for:\n- Agent cognitive loop (reason-act-reflect)\n- Model pool orchestration (Prime/Lite/Embe\"\"\"\n\n## FILE: gaia-core/api/gpu_endpoints.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GPU management endpoints for gaia-core.\n\nThese endpoints allow the orchestrator to manage gaia-core's model pool\nwhen GPU ownership changes. The orchestrator handles the actual container\nstop/start fo\"\"\"\nclass GPUReleaseRequest(BaseModel):\n\nclass GPUReclaimRequest(BaseModel):\n\ndef _prime_endpoint() -> str: ...\ndef _get_model_pool(): ...\ndef gpu_status(): ...\ndef gpu_release(request: GPUReleaseRequest = GPUReleaseRequest()): ...\ndef gpu_reclaim(request: GPUReclaimRequest = GPUReclaimRequest()): ...\n\n## FILE: gaia-core/api/sleep_endpoints.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Sleep cycle HTTP endpoints for gaia-core.\n\nFollows the existing pattern from gpu_endpoints.py:\n  - Separate router file with APIRouter(prefix=\"/sleep\")\n  - Registered in main.py via app.include_router\"\"\"\ndef receive_wake_signal(request: Request): ...\ndef voice_state(request: Request): ...\ndef get_sleep_status(request: Request): ...\ndef study_handoff(request: Request): ...\ndef distracted_check(request: Request): ...\ndef shutdown(request: Request): ...\n\n## FILE: gaia-core/behavior/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.behavior - Persona and behavioral adaptation modules.\n\nThis package provides:\n- persona_manager: Load and manage persona definitions\n- persona_adapter: Adapt responses based on active person\"\"\"\n\n## FILE: gaia-core/behavior/persona_adapter.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Persona Adapter (pillar-compliant, robust)\n- Adapts/merges persona config with current pipeline context.\n- Ensures context has correct template, instructions, and allows future persona behaviors.\"\"\"\nclass PersonaAdapter():\n    \"\"\"Adapts/wraps raw persona data into a consistent object for use throughout GAIA.\nEnsures persona attr\"\"\"\n    def __init__(self, persona_data: dict, config = None): ...\n    def get_full_instructions(self) -> str: ...\n    def __repr__(self): ...\n    def __str__(self): ...\n\n\n## FILE: gaia-core/behavior/persona_manager.py\n# AST Summary (substitute with actual source at training time)\nclass PersonaManager():\n    \"\"\"Manages loading and listing of GAIA's personas from disk.\nThis class is a stateless service for retr\"\"\"\n    def __init__(self, personas_dir: str): ...\n    def load_persona_data(self, name: str) -> Optional[Dict]: ...\n    def list_personas(self) -> List[str]: ...\n    def get_persona(self, name: str): ...\n\n\n## FILE: gaia-core/behavior/persona_switcher.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"This module contains the logic for dynamically switching GAIA's persona based on user intent.\"\"\"\ndef _normalize_text(text: str) -> str: ...\ndef _load_persona_config(persona_name: str) -> Optional[dict]: ...\ndef get_knowledge_base_for_persona(persona_name: str) -> Optional[str]: ...\ndef get_persona_for_knowledge_base(kb_name: str) -> Optional[str]: ...\ndef get_persona_for_request(user_input: str) -> Tuple[str, Optional[str]]: ...\n\n## FILE: gaia-core/behavior/persona_writer.py\n# AST Summary (substitute with actual source at training time)\nclass PersonaWriter():\n    \"\"\"Handles creation of persona folders and writing JSON + instruction overlays to disk.\nUsed during int\"\"\"\n    def __init__(self, vectordb_client, personas_dir = '/personas'): ...\n    def create_persona_from_template(self, template: Dict, instructions: Optional[Dict[str, str]] = None) -> bool: ...\n    def _summarize_persona(self, template: Dict) -> str: ...\n    def _embed_to_vectordb(self, summary_text: str, tag: str) -> None: ...\n\n\n## FILE: gaia-core/cognition/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.cognition - Cognitive processing and reasoning modules.\n\nThis package provides:\n- agent_core: Main cognitive loop (AgentCore class)\n- cognitive_dispatcher: Route and dispatch cognitive tasks\"\"\"\n\n## FILE: gaia-core/cognition/adapter_trigger_system.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Adapter Trigger System - Automatic LoRA adapter activation based on content\n\nMonitors user input for keywords/patterns and automatically loads relevant\nadapters to enhance GAIA's knowledge for specifi\"\"\"\nclass TriggerRule():\n    \"\"\"A rule for triggering adapter activation.\"\"\"\n\nclass TriggerMatch():\n    \"\"\"Result of a trigger match.\"\"\"\n\nclass AdapterTriggerSystem():\n    \"\"\"Monitors input and determines which adapters should be activated.\n\nFeatures:\n- Keyword matching (cas\"\"\"\n    def __init__(self, adapter_base_dir: str = '/models/lora_adapters', max_concurrent_adapters: int = 3): ...\n    def set_load_callback(self, callback: Callable[[str, str, int], bool]): ...\n    def set_unload_callback(self, callback: Callable[[str], bool]): ...\n    def load_rules_from_adapters(self) -> int: ...\n    def add_rule(self, rule: TriggerRule): ...\n    def remove_rule(self, adapter_name: str) -> bool: ...\n    def check_triggers(self, text: str) -> List[TriggerMatch]: ...\n    def process_input(self, text: str, auto_load: bool = True) -> Tuple[List[TriggerMatch], List[str], List[str]]: ...\n    def tick_cooldowns(self): ...\n    def deactivate_adapter(self, adapter_name: str) -> bool: ...\n    def get_active_adapters(self) -> List[str]: ...\n    def get_suggested_adapters(self, text: str, top_k: int = 3) -> List[Dict[str, Any]]: ...\n\ndef get_trigger_system(adapter_base_dir: str = '/models/lora_adapters', force_new: bool = False) -> AdapterTriggerSystem: ...\n\n## FILE: gaia-core/cognition/agent_core.py\n# AST Summary (substitute with actual source at training time)\nclass AgentCore():\n    \"\"\"Encapsulates the core \"Reason-Act-Reflect\" loop for GAIA.\nThis class is UI-agnostic and yields struc\"\"\"\n    def __init__(self, ai_manager, ethical_sentinel = None): ...\n    def _emit_timeline_message(self, session_id: str, role: str, source: str = '') -> None: ...\n    def _build_output_routing(self, source: str, destination: str, metadata: dict) -> OutputRouting: ...\n    def _create_initial_packet(self, user_input: str, session_id: str, history: List[Dict[str, Any]], selected_model_name: str, source: str = 'cli', destination: str = 'cli_chat', metadata: dict = None) -> CognitionPacket: ...\n    def _run_pre_generation_safety_check(self, packet: CognitionPacket, assembled_prompt: str) -> (bool, str): ...\n    def run_turn(self, user_input: str, session_id: str, destination: str = 'cli_chat', source: str = 'cli', metadata: dict = None) -> Generator[Dict[str, Any], None, None]: ...\n    def _knowledge_acquisition_workflow(self, packet: CognitionPacket) -> CognitionPacket: ...\n    def _suppress_repetition(self, text: str, max_repeat: int = 2) -> str: ...\n    def _dedup_block(text: str, min_block: int = 120, similarity_threshold: float = 0.85) -> str: ...\n    def _build_response_header(self, model_name: str, packet, observer_instance, active_stream_observer, post_run_observer) -> str: ...\n    def _should_escalate_to_thinker(self, text: str) -> bool: ...\n    def _should_use_slim_prompt(self, plan: Plan, user_input: str) -> bool: ...\n    def _run_slim_prompt(self, selected_model_name: str, user_input: str, history: List[Dict[str, Any]], intent: str = '', session_id: str = '', source: str = 'cli', metadata: dict = None, packet: CognitionPacket = None) -> str: ...\n    def _build_recitation_search_query(user_input: str) -> str: ...\n    def _validate_recitation_content(self, content: str, user_input: str) -> bool: ...\n    def _web_retrieve_for_recitation(self, user_input: str, session_id: str) -> Optional[Dict[str, str]]: ...\n    def _run_with_document_recitation(self, user_input: str, document: Dict[str, str], selected_model_name: str, history: List[Dict[str, Any]], session_id: str = '', output_as_file: bool = False) -> str: ...\n    def _run_with_fragmentation(self, user_input: str, selected_model_name: str, history: List[Dict[str, Any]], session_id: str = '', max_fragments: int = 5, output_as_file: bool = False, output_filename: Optional[str] = None) -> str: ...\n    def _read_fragments_from_sketchpad(self, fragment_keys: List[str], memory_fallback: Optional[Dict[str, str]] = None) -> str: ...\n    def _run_assembly_turn(self, original_request: str, fragment_keys: List[str], selected_model_name: str, session_id: str = '', memory_fallback: Optional[Dict[str, str]] = None) -> str: ...\n    def _write_assembled_to_file(self, content: str, original_request: str, request_id: str, filename: Optional[str] = None) -> str: ...\n    def _assemble_fragments(self, fragments: List[str]) -> str: ...\n    def _run_mcp_list_tree(self) -> str: ...\n    def _run_mcp_list_files(self) -> str: ...\n    def detect_truncation(self, response: str, max_tokens: int = 1000) -> Dict[str, Any]: ...\n    def build_continuation_prompt(self, original_request: str, previous_content: str, continuation_hint: str = '') -> str: ...\n    def assess_task_confidence(self, intent: str, user_input: str, model_name: str = 'lite', session_id: str = '') -> Dict[str, Any]: ...\n    def reflect_on_truncation(self, original_request: str, truncated_output: str, model_name: str = 'lite') -> Dict[str, Any]: ...\n    def _check_topic_alignment(self, original_request: str, continuation_prompt: str) -> tuple: ...\n    def _build_grounded_continuation(self, original_request: str, truncated_output: str) -> str: ...\n    def _find_relevant_files(self, topic: str, max_files: int = 10) -> List[Dict[str, Any]]: ...\n    def _analyze_code_for_topic(self, topic: str, file_paths: List[str], task_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]: ...\n    def _propose_code_fix(self, file_path: str, issue_description: str, suggestion: Optional[str] = None) -> Dict[str, Any]: ...\n    def _apply_code_fix(self, file_path: str, new_content: str, reason: str, run_syntax_check: bool = True) -> Dict[str, Any]: ...\n    def _update_dev_matrix_task(self, task_context: Dict[str, Any], topic: str, fixes_applied: int, files_modified: List[str], analysis_summary: str) -> Dict[str, Any]: ...\n    def run_self_improvement(self, topic: str, auto_apply: bool = False, max_files: int = 5) -> Generator[Dict[str, Any], None, None]: ...\n    def _run_tool_routing_loop(self, packet: CognitionPacket, user_input: str, session_id: str = '', source: str = 'cli', metadata: dict = None) -> CognitionPacket: ...\n    def _execute_mcp_tool(self, tool: SelectedTool) -> ToolExecutionResult: ...\n    def _should_use_tool_routing(self, plan: Plan, user_input: str) -> bool: ...\n\ndef _format_retrieved_session_context(results: dict) -> str: ...\ndef find_recitable_document(user_input: str) -> Optional[Dict[str, str]]: ...\n\n## FILE: gaia-core/cognition/cognition_packet_v0.2_backup.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"CognitionPacket â€“ dynamic state for GAIAâ€™s self-reflection loop.\n\nSchema:\n  prompt:         str            # original user prompt\n  persona:        str            # active persona ID\n  identity:      \"\"\"\nclass CognitionPacket():\n    def __init__(self, session_id: str, packet_id: str, time_date: str, packet_type: str, intent: str, intent_confidence: float, identity: str, persona: str, contextual_instructions: str, prompt: str, history: List[Dict[str, Any]], reflection: str, reflection_confidence: float, execution: str, execution_confidence: float, response: str, response_confidence: float, data_fields: Dict[str, Any], sub_packet_id: str = None, config: Config = None): ...\n    def to_json(self) -> str: ...\n    def from_json(data: str | Dict) -> CognitionPacket: ...\n\ndef create_packet(config: Config, prompt: str, session_id: str, history: List[Dict[str, Any]], persona_instructions: List[str]) -> CognitionPacket: ...\n\n## FILE: gaia-core/cognition/cognitive_audit.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Cognitive Self-Audit â€” Phase 1 of Reflective Self-Talk\n\nInserts a structured self-assessment between planning and reflection.\nThe model reads its own plan + packet state and writes evaluations,\nsketch\"\"\"\ndef _build_audit_context(packet: CognitionPacket) -> str: ...\ndef _parse_audit_output(text: str, packet: CognitionPacket) -> None: ...\ndef run_cognitive_self_audit(packet: CognitionPacket, plan_text: str, config, llm) -> None: ...\n\n## FILE: gaia-core/cognition/cognitive_dispatcher.py\n# AST Summary (substitute with actual source at training time)\ndef process_execution_results(execution_results, session_manager, session_id, packet: CognitionPacket): ...\n\n## FILE: gaia-core/cognition/conversation_curator.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Auto-append notable Discord conversations to the knowledge examples file.\n\nHooks into SessionManager.summarize_and_archive() to evaluate each archived\nconversation for \"notability\" using simple heuris\"\"\"\nclass ConversationCurator():\n    \"\"\"Evaluates archived conversations and appends notable ones to the examples file.\"\"\"\n    def __init__(self, output_dir: Optional[str] = None): ...\n    def curate(self, session_id: str, messages: List[Dict]) -> bool: ...\n    def is_notable(self, messages: List[Dict]) -> bool: ...\n    def _detect_channel_type(self, session_id: str) -> str: ...\n    def _format_conversation(self, session_id: str, messages: List[Dict]) -> str: ...\n    def _append_to_file(self, formatted: str) -> None: ...\n    def _trim_oldest(self, needed_bytes: int) -> None: ...\n\n\n## FILE: gaia-core/cognition/external_voice.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"external_voice.py â€” handles all inbound/outbound chat traffic for GAIA\n(streaming, observer hooks, basic logging).  This module is the *sole*\nentry and exit for chat-based interactions.\"\"\"\nclass ExternalVoice():\n    def __init__(self, model, model_pool, config: Config, thought: Optional[str] = None, messages: Optional[List[Dict]] = None, context: Optional[Dict] = None, session_id: str = 'shell', source: str = 'web', observer: Optional[StreamObserver] = None) -> None: ...\n    def stream_response(self, user_input: Optional[str] = None) -> Generator[str | Dict[str, Any], None, None]: ...\n    def generate_full_response(self, user_input: Optional[str] = None) -> str: ...\n    def from_thought(cls, model, thought: str, **kw): ...\n    def from_messages(cls, model, messages: List[Dict], **kw): ...\n    def one_shot(cls, model, prompt: str, **kw) -> str: ...\n    def _apply_stream_spacing(self, token: str, prev_char: str) -> str: ...\n    def _get_first_visible_char(text: str) -> str: ...\n    def _get_last_visible_char(text: str) -> str: ...\n\ndef suppress_llama_stderr() -> Generator[None, None, None]: ...\ndef extract_and_format_execute_blocks(response_text: str) -> str: ...\n\n## FILE: gaia-core/cognition/goal_detector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Goal Detection Module â€” detects and carries user goals across conversation turns.\n\nThree detection paths:\n  1. Fast-path â€” self-evident intents map directly to a goal\n  2. Session-carry â€” active goal \"\"\"\nclass GoalDetector():\n    \"\"\"Detects and manages user goals across conversation turns.\"\"\"\n    def __init__(self, config = None): ...\n    def detect(self, packet: CognitionPacket, session_manager, session_id: str, model_pool = None) -> GoalState: ...\n    def _fast_path_detect(self, intent: str, user_input: str) -> Optional[DetectedGoal]: ...\n    def _session_carry(self, session_manager, session_id: str) -> Optional[GoalState]: ...\n    def _llm_detect(self, packet: CognitionPacket, model_pool) -> Optional[DetectedGoal]: ...\n    def _parse_llm_response(text: str) -> Optional[DetectedGoal]: ...\n    def handle_goal_shift(new_goal_desc: str, packet: CognitionPacket, session_manager, session_id: str): ...\n    def _persist_goal(self, session_manager, session_id: str, state: GoalState): ...\n    def _persist_goal_static(session_manager, session_id: str, state: GoalState): ...\n\n\n## FILE: gaia-core/cognition/heartbeat.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Thought Seed Heartbeat â€” regular-interval daemon that triages dormant seeds.\n\nRuns independently of the sleep cycle on a configurable timer (default 20 min).\nFor each unreviewed seed, Lite performs a \"\"\"\nclass ThoughtSeedHeartbeat():\n    \"\"\"Daemon thread that triages thought seeds on a regular interval.\"\"\"\n    def __init__(self, config, model_pool = None, agent_core = None, sleep_wake_manager = None, timeline_store = None, session_manager = None) -> None: ...\n    def start(self) -> None: ...\n    def stop(self) -> None: ...\n    def _run(self) -> None: ...\n    def _tick(self) -> None: ...\n    def _run_temporal_tasks(self) -> tuple[bool, bool, bool]: ...\n    def _triage_seed(self, llm, seed_data: Dict[str, Any]) -> tuple[str, str]: ...\n    def _do_archive(self, filename: str) -> None: ...\n    def _do_defer(self, filename: str) -> None: ...\n    def _act_on_seed(self, llm, seed_filename: str, seed_data: Dict[str, Any]) -> None: ...\n    def _expand_seed(self, llm, seed_data: Dict[str, Any]) -> str: ...\n    def _ensure_active(self, seed_filename: str) -> bool: ...\n    def _emit_heartbeat_tick(self, seeds_found: int, archived: int, deferred: int, acted: int, journal_written: bool = False, state_baked: bool = False, interview_conducted: bool = False) -> None: ...\n\n\n## FILE: gaia-core/cognition/history_review.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"History Review â€” Pre-injection audit of conversation history.\n\nBefore history is injected into the LLM prompt, each assistant message\nis checked for epistemic violations:\n  - Fabricated file paths (ci\"\"\"\ndef _count_violations(text: str) -> Tuple[int, List[str]]: ...\ndef _is_user_correction(text: str) -> bool: ...\ndef _redact_message(original: str, reasons: List[str]) -> str: ...\ndef _build_correction_summary(user_msg: str, assistant_msg: str, reasons: List[str]) -> Optional[str]: ...\ndef review_history(history: List[Dict[str, str]], config: Optional[dict] = None, session_id: str = '') -> List[Dict[str, str]]: ...\n\n## FILE: gaia-core/cognition/initiative_engine.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Initiative Engine â€” ported from archive/gaia-assistant-monolith/run_gil.py.\n\nExecutes a single autonomous thought cycle: picks the highest-priority topic\nfrom the topic cache and feeds a self-generate\"\"\"\nclass InitiativeEngine():\n    \"\"\"Autonomous thought engine driven by the topic manager.\"\"\"\n    def __init__(self, config, agent_core = None) -> None: ...\n    def execute_turn(self) -> Optional[Dict[str, Any]]: ...\n    def _build_self_prompt(topic: Dict[str, Any]) -> str: ...\n\n\n## FILE: gaia-core/cognition/knowledge_enhancer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"This module is responsible for enhancing the CognitionPacket with relevant knowledge from knowledge bases.\"\"\"\ndef enhance_packet(packet: CognitionPacket): ...\n\n## FILE: gaia-core/cognition/knowledge_ingestion.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Knowledge Ingestion Pipeline for D&D Campaign Content\n\nDetects incoming knowledge dumps (explicit save commands or heuristic auto-detect),\nclassifies content, checks for duplicates, formats as structu\"\"\"\ndef detect_save_command(user_input: str) -> Optional[Dict[str, str]]: ...\ndef detect_knowledge_dump(user_input: str, kb_name: str) -> bool: ...\ndef classify_content(text: str) -> Dict[str, str]: ...\ndef check_dedup(content: str, kb_name: str) -> Optional[Dict]: ...\ndef _sanitize_filename(text: str) -> str: ...\ndef format_document(content: str, classification: Dict[str, str], subject: str = '') -> Tuple[str, str]: ...\ndef write_and_embed(filename: str, doc_content: str, kb_name: str) -> Dict[str, object]: ...\ndef run_explicit_save(user_input: str, kb_name: str) -> Optional[Dict]: ...\ndef run_auto_detect(user_input: str, kb_name: str) -> Optional[Dict]: ...\ndef detect_knowledge_update(user_input: str, kb_name: str) -> Optional[Dict[str, str]]: ...\ndef retrieve_entity_document(entity: str, kb_name: str) -> Optional[Dict]: ...\ndef run_update_detect(user_input: str, kb_name: str) -> Optional[Dict]: ...\n\n## FILE: gaia-core/cognition/lite_journal.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Lite Cognitive Journal â€” running introspective log written by the Lite model.\n\nMirrors the PrimeCheckpointManager pattern: regular writes, timestamped entries,\nrotation to history directory when the j\"\"\"\nclass LiteJournal():\n    \"\"\"Manages Lite's introspective journal (Lite.md).\"\"\"\n    def __init__(self, config, model_pool = None, timeline_store = None, sleep_wake_manager = None) -> None: ...\n    def write_entry(self) -> Optional[str]: ...\n    def load_latest(self) -> str: ...\n    def load_recent_entries(self, n: int = 5) -> List[str]: ...\n    def rotate(self) -> None: ...\n    def get_entry_count(self) -> int: ...\n    def _generate_entry(self, llm) -> str: ...\n    def _build_journal_prompt(self) -> tuple[str, str]: ...\n    def _append_entry(self, entry_text: str) -> None: ...\n    def _format_duration(seconds: float) -> str: ...\n    def _summarize_event(data: Dict[str, Any]) -> str: ...\n\n\n## FILE: gaia-core/cognition/loop_detector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Loop Detection System for GAIA Cognitive Pipeline.\n\nDetects when the model enters generation loops and provides signals for\ngraceful recovery. Uses multiple parallel detectors that vote on loop presen\"\"\"\nclass LoopDetectorConfig():\n    \"\"\"Configuration for loop detection thresholds.\"\"\"\n\nclass DetectionResult():\n    \"\"\"Result from a single detector.\"\"\"\n\nclass AggregatedResult():\n    \"\"\"Combined result from all detectors.\"\"\"\n\nclass ToolCallRecord():\n    \"\"\"Record of a single tool call for tracking.\"\"\"\n\nclass ErrorRecord():\n    \"\"\"Record of an error for tracking.\"\"\"\n\nclass ToolCallRepetitionDetector():\n    \"\"\"Detects repeated tool calls with same/similar arguments.\n\nCatches:\n- Exact repetition: Same tool + a\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def record(self, tool: str, args: Dict[str, Any], result: str = '') -> None: ...\n    def detect(self) -> DetectionResult: ...\n    def _detect_exact_repetition(self) -> DetectionResult: ...\n    def _detect_ping_pong(self) -> DetectionResult: ...\n    def _detect_same_results(self) -> DetectionResult: ...\n    def _hash_args(self, args: Dict[str, Any]) -> str: ...\n    def _summarize_args(self, args: Dict[str, Any]) -> str: ...\n    def reset(self) -> None: ...\n\nclass OutputSimilarityDetector():\n    \"\"\"Detects nearly identical outputs across turns.\n\nUses multi-strategy similarity:\n- Jaccard on word se\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def record(self, output: str) -> None: ...\n    def detect(self) -> DetectionResult: ...\n    def _similarity(self, a: str, b: str) -> float: ...\n    def _ngram_similarity(self, a: str, b: str, n: int = 3) -> float: ...\n    def _structural_similarity(self, a: str, b: str) -> float: ...\n    def _normalize(self, text: str) -> str: ...\n    def reset(self) -> None: ...\n\nclass StateOscillationDetector():\n    \"\"\"Detects oscillating states without progress.\n\nTracks:\n- Goal changes\n- File modification patterns\n- \"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def record(self, goal: str = '', modified_files: Optional[Set[str]] = None, state_snapshot: Optional[Dict[str, Any]] = None) -> None: ...\n    def detect(self) -> DetectionResult: ...\n    def _detect_goal_oscillation(self) -> DetectionResult: ...\n    def reset(self) -> None: ...\n\nclass ErrorCycleDetector():\n    \"\"\"Detects recurring error patterns.\n\nCatches:\n- Same error repeated\n- Same fix attempted multiple time\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def record(self, error_type: str, error_message: str, attempted_fix: str = '', was_success: bool = False) -> None: ...\n    def detect(self) -> DetectionResult: ...\n    def _detect_fix_repetition(self) -> DetectionResult: ...\n    def _detect_whack_a_mole(self) -> DetectionResult: ...\n    def reset(self) -> None: ...\n\nclass TokenPatternDetector():\n    \"\"\"Detects repetitive patterns during token streaming.\n\nCatches:\n- Exact phrase repetition (\"I'll help.\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def add_tokens(self, tokens: str) -> Optional[DetectionResult]: ...\n    def detect(self) -> DetectionResult: ...\n    def _detect_phrase_repetition(self) -> DetectionResult: ...\n    def _detect_word_repetition(self) -> DetectionResult: ...\n    def _detect_structural_repetition(self) -> DetectionResult: ...\n    def reset(self) -> None: ...\n\nclass LoopDetectionAggregator():\n    \"\"\"Combines signals from all detectors to determine if a loop is occurring.\n\nTriggering rules:\n1. Any s\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def evaluate(self) -> AggregatedResult: ...\n    def _should_warn(self) -> bool: ...\n    def mark_warned(self) -> None: ...\n    def mark_reset(self) -> None: ...\n    def record_tool_call(self, tool: str, args: Dict[str, Any], result: str = '') -> None: ...\n    def record_output(self, output: str) -> None: ...\n    def record_state(self, goal: str = '', modified_files: Optional[Set[str]] = None, state_snapshot: Optional[Dict[str, Any]] = None) -> None: ...\n    def record_error(self, error_type: str, error_message: str, attempted_fix: str = '', was_success: bool = False) -> None: ...\n    def add_tokens(self, tokens: str) -> Optional[DetectionResult]: ...\n    def reset(self) -> None: ...\n\nclass LoopDetector():\n    \"\"\"Main interface for loop detection in GAIA cognitive pipeline.\n\nUsage:\n    detector = LoopDetector.ge\"\"\"\n    def __init__(self, config: Optional[LoopDetectorConfig] = None): ...\n    def get_instance(cls, config: Optional[LoopDetectorConfig] = None) -> LoopDetector: ...\n    def reset_instance(cls) -> None: ...\n    def enabled(self) -> bool: ...\n    def enabled(self, value: bool) -> None: ...\n    def reset_count(self) -> int: ...\n    def record_tool_call(self, tool: str, args: Dict[str, Any], result: str = '') -> None: ...\n    def record_output(self, output: str) -> None: ...\n    def record_state(self, goal: str = '', modified_files: Optional[Set[str]] = None, state_snapshot: Optional[Dict[str, Any]] = None) -> None: ...\n    def record_error(self, error_type: str, error_message: str, attempted_fix: str = '', was_success: bool = False) -> None: ...\n    def add_tokens(self, tokens: str) -> Optional[DetectionResult]: ...\n    def check(self) -> AggregatedResult: ...\n    def mark_warned(self) -> None: ...\n    def trigger_reset(self) -> None: ...\n    def reset_detectors(self) -> None: ...\n    def full_reset(self) -> None: ...\n\n\n## FILE: gaia-core/cognition/loop_patterns.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Loop Pattern Classification and Description System.\n\nGenerates human-readable descriptions of detected loops for:\n1. Brief - Status line / notifications (~50 chars)\n2. Summary - User-facing display (2\"\"\"\nclass DescriptionTemplate():\n    \"\"\"Template for loop description at different verbosity levels.\"\"\"\n\nclass ClassifiedPattern():\n    \"\"\"A classified loop pattern with description.\"\"\"\n\nclass PatternClassifier():\n    \"\"\"Classifies detected loops into specific pattern types and generates\nhuman-readable descriptions.\"\"\"\n    def classify(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_tool_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_output_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_state_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_error_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_token_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_generic(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _build_full_template(self, title: str, pattern: str, details: List[str], what_went_wrong: str, suggestions: List[str]) -> str: ...\n\nclass PatternRenderer():\n    \"\"\"Renders classified patterns at different verbosity levels.\"\"\"\n    def __init__(self): ...\n    def render(self, result: AggregatedResult, format: str = 'summary', reset_count: int = 0) -> str: ...\n    def _render_model_context(self, classified: ClassifiedPattern, reset_count: int) -> str: ...\n    def _get_urgency(self, reset_count: int) -> str: ...\n    def get_notification(self, result: AggregatedResult, reset_count: int = 0) -> Dict[str, Any]: ...\n    def _get_override_warning(self, reset_count: int) -> Optional[str]: ...\n\n\n## FILE: gaia-core/cognition/loop_recovery.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Loop Recovery System for GAIA Cognitive Pipeline.\n\nOrchestrates the reset flow when a loop is detected:\n1. Capture current packet state\n2. Inject recovery context into next prompt\n3. Manage escalation\"\"\"\nclass LoopMetadata():\n    \"\"\"Metadata about a detected loop, attached to packets for context.\"\"\"\n    def to_dict(self) -> Dict[str, Any]: ...\n    def from_dict(cls, data: Dict[str, Any]) -> LoopMetadata: ...\n\nclass CapturedState():\n    \"\"\"Captured state before reset for context preservation.\"\"\"\n    def to_dict(self) -> Dict[str, Any]: ...\n\nclass LoopRecoveryManager():\n    \"\"\"Manages the loop detection and recovery lifecycle.\n\nResponsibilities:\n- Coordinate detection checks\n\"\"\"\n    def __init__(self, config: Optional[LoopDetectorConfig] = None, on_warn: Optional[Callable[[AggregatedResult], None]] = None, on_block: Optional[Callable[[AggregatedResult], None]] = None, on_escalate: Optional[Callable[[int], None]] = None): ...\n    def enabled(self) -> bool: ...\n    def enabled(self, value: bool) -> None: ...\n    def reset_count(self) -> int: ...\n    def check_and_handle(self, session_id: str = '', packet_id: str = '', goal: str = '', last_output: str = '') -> Optional[AggregatedResult]: ...\n    def _capture_state(self, session_id: str, packet_id: str, goal: str, last_output: str, result: AggregatedResult) -> None: ...\n    def _prepare_recovery_context(self, result: AggregatedResult) -> None: ...\n    def get_recovery_context(self) -> Optional[str]: ...\n    def clear_recovery_context(self) -> None: ...\n    def _mark_recovery_success(self) -> None: ...\n    def override_detection(self, duration_seconds: float = 300) -> None: ...\n    def cancel_override(self) -> None: ...\n    def get_notification(self, result: AggregatedResult) -> Dict[str, Any]: ...\n    def should_require_user_intervention(self) -> bool: ...\n    def get_escalation_message(self) -> str: ...\n    def record_tool_call(self, tool: str, args: Dict[str, Any], result: str = '') -> None: ...\n    def record_output(self, output: str) -> None: ...\n    def record_state(self, goal: str = '', modified_files: Optional[set] = None, state_snapshot: Optional[Dict[str, Any]] = None) -> None: ...\n    def record_error(self, error_type: str, error_message: str, attempted_fix: str = '', was_success: bool = False) -> None: ...\n    def add_tokens(self, tokens: str) -> Optional[Any]: ...\n\nclass LoopInterrupt():\n    \"\"\"Interrupt signal for streaming loop detection.\nCompatible with existing Interrupt class from stream_\"\"\"\n    def from_detection(cls, result: AggregatedResult, is_warn: bool = True) -> LoopInterrupt: ...\n\nclass LoopDetectorObserver():\n    \"\"\"Observer adapter for integration with ExternalVoice streaming.\n\nMonitors token stream for loop patte\"\"\"\n    def __init__(self, manager: Optional[LoopRecoveryManager] = None, think_tag_char_threshold: int = 500, think_tag_ratio_threshold: float = 0.9): ...\n    def _check_think_tag_ratio(self) -> Optional[LoopInterrupt]: ...\n    def on_token(self, token: str) -> Optional[LoopInterrupt]: ...\n    def reset(self) -> None: ...\n\ndef get_recovery_manager() -> LoopRecoveryManager: ...\ndef inject_recovery_context_if_needed(prompt: str) -> str: ...\ndef build_loop_detection_config_from_constants(constants: Dict[str, Any]) -> LoopDetectorConfig: ...\n\n## FILE: gaia-core/cognition/nlu/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.cognition.nlu - Natural Language Understanding modules.\n\nThis package provides:\n- intent_detection: Fast reflex, LLM-powered, and embedding-based intent detection\n- embed_intent_classifier: \"\"\"\n\n## FILE: gaia-core/cognition/nlu/embed_intent_classifier.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Embedding-based intent classifier for GAIA.\n\nReplaces the keyword-heuristic fallback in intent_detection.py with\ncosine-similarity classification against a bank of labeled exemplar\nphrases.  Uses the \"\"\"\nclass EmbedIntentClassifier():\n    \"\"\"Singleton embedding-based intent classifier.\"\"\"\n    def __init__(self): ...\n    def instance(cls) -> 'EmbedIntentClassifier': ...\n    def initialise(self, embed_model, config: Optional[dict] = None) -> bool: ...\n    def classify(self, text: str, confidence_threshold: float = 0.45, top_k: int = 3) -> Tuple[str, float]: ...\n    def ready(self) -> bool: ...\n\n\n## FILE: gaia-core/cognition/nlu/intent_detection.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Intent Detection (pillar-compliant, robust)\n- Fast reflex/regex path for autonomic commands (help, exit, shell, etc).\n- LLM-powered detection for all ambiguous/natural input.\n- Returns simple intent l\"\"\"\nclass Plan():\n\ndef fast_intent_check(text): ...\ndef _detect_direct_list_tools(text: str) -> bool: ...\ndef _detect_tree_request(text: str) -> bool: ...\ndef _detect_list_files_request(text: str) -> bool: ...\ndef _detect_read_file_request(text: str) -> bool: ...\ndef _mentions_file_like_action(text: str) -> bool: ...\ndef _detect_fragmentation_request(text: str) -> bool: ...\ndef _detect_tool_routing_request(text: str) -> bool: ...\ndef _keyword_intent_classify(text: str, probe_context: str = '') -> str: ...\ndef _fast_track_intent_detection(text: str) -> Optional[str]: ...\ndef model_intent_detection(text, config, lite_llm = None, full_llm = None, fallback_llm = None, probe_context = '', embed_model = None): ...\ndef detect_intent(text, config, lite_llm = None, full_llm = None, fallback_llm = None, probe_context = '', embed_model = None) -> Plan: ...\n\n## FILE: gaia-core/cognition/nlu/intent_service.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Public faÃ§ade for intent detection.\n\nOther code should import ONLY from this file:\n    from gaia_core.cognition.nlu.intent_service import detect_intent\n\nBehind the curtain we forward to the real detec\"\"\"\n\n## FILE: gaia-core/cognition/packet_upgrade.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Non-destructive CognitionPacket upgrader â€” DEPRECATED.\n\nThis module was part of the v0.2 â†’ v0.3 migration path. The attributes it\nsets (cot, scratch, cheats, proposed_actions, etc.) do not exist on th\"\"\"\ndef _ensure(obj: object, name: str, default): ...\ndef _ensure_slots(dct, keys): ...\ndef upgrade_packet(packet, config) -> object: ...\n\n## FILE: gaia-core/cognition/packet_utils.py\n# AST Summary (substitute with actual source at training time)\ndef is_execution_safe(packet: CognitionPacket) -> bool: ...\ndef upgrade_v2_to_v3_packet(old_packet_data: Dict) -> CognitionPacket: ...\n\n## FILE: gaia-core/cognition/prime_checkpoint.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Prime model cognitive state checkpointing.\n\nManages the prime.md checkpoint file that preserves GAIA's working memory\nacross GPU sleep/wake cycles.  This is the natural-language replacement for\nKV cac\"\"\"\nclass PrimeCheckpointManager():\n    \"\"\"Manages Prime model's cognitive state checkpointing.\"\"\"\n    def __init__(self, config, timeline_store = None): ...\n    def create_checkpoint(self, packet = None, model_pool = None) -> Path: ...\n    def rotate_checkpoints(self) -> None: ...\n    def load_latest(self) -> str: ...\n    def is_consumed(self) -> bool: ...\n    def mark_consumed(self) -> None: ...\n    def get_checkpoint_history(self, limit: int = 10) -> list: ...\n    def _generate_checkpoint(self, packet, model_pool) -> str: ...\n    def _generate_with_llm(self, llm, ctx: dict) -> str: ...\n    def _build_template(self, ctx: dict) -> str: ...\n    def _emit_checkpoint(self, packet, method: str) -> None: ...\n    def _extract_context(self, packet) -> dict: ...\n    def _load_evolving_summary(self, session_id: str) -> str: ...\n    def _truncate(text: str, max_len: int) -> str: ...\n\n\n## FILE: gaia-core/cognition/self_reflection.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Self Reflection Processor (model-powered, robust pipeline)\n- Calls LLM for post-generation analysis and hallucination/error detection.\n- Integrates config-driven safety and can fallback to rule-based \"\"\"\ndef reflect_and_refine(packet: CognitionPacket, output: str, config, llm, ethical_sentinel) -> str: ...\ndef run_self_reflection(packet: CognitionPacket, output: str, config = None, llm = None): ...\n\n## FILE: gaia-core/cognition/self_review_worker.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Self-review worker: reviews thought seeds and proposes dev_matrix updates.\n\nThis worker runs in proposal-only mode: it will create a pending MCP action to\nupdate `knowledge/system_reference/dev_matrix\"\"\"\ndef _get_model_pool(): ...\ndef _load_dev_matrix(): ...\ndef _save_dev_matrix(data): ...\ndef run_review_once(config: Config = None): ...\ndef run_review_with_prompt(prompt: str, task_key: str = 'thought_seed_system', session_id: str = 'rescue-shell', persona_id: str = 'RescueOperator'): ...\n\n## FILE: gaia-core/cognition/semantic_probe.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Semantic Probe â€” Pre-cognition vector lookup for context discovery.\n\nRuns BEFORE intent detection and persona selection. Extracts interesting\nphrases from user input, probes all indexed vector collect\"\"\"\nclass ProbeHit():\n    \"\"\"A single vector match from the probe.\"\"\"\n    def to_dict(self) -> dict: ...\n\nclass SemanticProbeResult():\n    \"\"\"Aggregated result from probing all collections.\"\"\"\n    def to_dict(self) -> dict: ...\n    def has_hits(self) -> bool: ...\n    def to_metrics_dict(self) -> Dict: ...\n\nclass SessionProbeCache():\n    \"\"\"Per-session cache of phrase â†’ probe hits with turn-based eviction.\"\"\"\n    def get(self, phrase: str) -> Optional[List[ProbeHit]]: ...\n    def put(self, phrase: str, hits: List[ProbeHit]): ...\n    def advance_turn(self): ...\n\nclass ProbeSessionStats():\n    \"\"\"Cumulative probe effectiveness stats for a session.\"\"\"\n    def record(self, result: 'SemanticProbeResult', was_skipped: bool = False): ...\n    def hit_rate(self) -> float: ...\n    def avg_probe_time_ms(self) -> float: ...\n    def to_dict(self) -> Dict: ...\n\ndef _load_probe_config() -> Dict: ...\ndef _get_session_cache(session_id: str) -> SessionProbeCache: ...\ndef _get_session_stats(session_id: str) -> ProbeSessionStats: ...\ndef get_session_probe_stats(session_id: str) -> Optional[Dict]: ...\ndef extract_candidate_phrases(text: str) -> List[str]: ...\ndef _probe_single_collection(phrases: List[str], collection_name: str, top_k: int = 3) -> List[ProbeHit]: ...\ndef _determine_primary_and_supplemental(hits: List[ProbeHit]) -> Tuple[Optional[str], List[str]]: ...\ndef probe_collections(phrases: List[str], knowledge_bases: Dict[str, dict], session_id: str = '', top_k_per_phrase: int = 3) -> SemanticProbeResult: ...\ndef should_skip_probe(user_input: str) -> bool: ...\ndef run_semantic_probe(user_input: str, knowledge_bases: Dict[str, dict], session_id: str = '', top_k_per_phrase: int = ...) -> SemanticProbeResult: ...\n\n## FILE: gaia-core/cognition/sleep_cycle_loop.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Sleep cycle loop â€” runs as a daemon thread in gaia-core.\n\nUses gaia-common primitives (IdleMonitor) for idle detection but owns\nall sleep/wake orchestration logic.  Replaces the legacy\nBackgroundProce\"\"\"\nclass SleepCycleLoop():\n    \"\"\"Background thread that monitors idle state and drives sleep/wake.\"\"\"\n    def __init__(self, config, discord_connector = None, model_pool = None, agent_core = None, session_manager = None) -> None: ...\n    def start(self) -> None: ...\n    def stop(self) -> None: ...\n    def initiate_shutdown(self) -> None: ...\n    def _run(self) -> None: ...\n    def _handle_active(self, idle_minutes: float) -> None: ...\n    def _handle_asleep(self) -> None: ...\n    def _handle_dreaming(self) -> None: ...\n    def _handle_distracted(self) -> None: ...\n    def _release_gpu_for_sleep(self) -> None: ...\n    def _reclaim_gpu_for_wake(self) -> None: ...\n    def _update_presence(self, status_text: Optional[str], sleeping: bool = False, offline: bool = False, status_override: Optional[str] = None) -> None: ...\n\n\n## FILE: gaia-core/cognition/sleep_task_scheduler.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Sleep Task Scheduler â€” orchestrates autonomous maintenance during SLEEPING state.\n\nRegistered tasks are executed one-at-a-time in priority order (lowest number = highest\npriority), with least-recently\"\"\"\nclass SleepTask():\n    \"\"\"A single registerable sleep-time task.\"\"\"\n\nclass SleepTaskScheduler():\n    \"\"\"Priority-based scheduler for sleep-time maintenance tasks.\"\"\"\n    def __init__(self, config, model_pool = None, agent_core = None, timeline_store = None) -> None: ...\n    def register_task(self, task: SleepTask) -> None: ...\n    def _register_default_tasks(self) -> None: ...\n    def get_next_task(self) -> Optional[SleepTask]: ...\n    def execute_task(self, task: SleepTask) -> bool: ...\n    def get_status(self) -> List[Dict[str, Any]]: ...\n    def _run_conversation_curation(self) -> None: ...\n    def _run_blueprint_validation(self) -> None: ...\n    def _validate_yaml_blueprints(self) -> int: ...\n    def _validate_legacy_blueprints(self) -> int: ...\n    def _extract_facts(source_files: List[str], source_roots: List[Path]) -> Dict[str, List[str]]: ...\n    def _check_facts(facts: Dict[str, List[str]], bp_text: str) -> List[str]: ...\n    def _append_update_notes(bp_path: Path, bp_text: str, missing: List[str]) -> None: ...\n    def _rebuild_blueprint_embeddings(self) -> None: ...\n    def _run_code_evolution(self) -> None: ...\n    def _emit_task_exec(self, task_id: str, task_type: str, elapsed: float, success: bool, error: str = '') -> None: ...\n\n\n## FILE: gaia-core/cognition/sleep_wake_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA Sleep/Wake State Machine.\n\nManages six public states + two internal transient phases:\n\nPublic states:\n    OFFLINE â†’ ACTIVE â†’ DROWSY â†’ ASLEEP â†’ DREAMING / DISTRACTED\n\nInternal phases (not in the p\"\"\"\nclass SleepWakeManager():\n    \"\"\"Manages GAIA's sleep/wake state transitions with cognitive continuity.\"\"\"\n    def __init__(self, config, model_pool = None, idle_monitor = None, timeline_store = None) -> None: ...\n    def get_state(self) -> GaiaState: ...\n    def should_transition_to_drowsy(self, idle_minutes: float) -> bool: ...\n    def _emit_state_change(self, from_state: str, to_state: str, reason: str = '') -> None: ...\n    def _notify_audio_state(self, to_state: str) -> None: ...\n    def initiate_drowsy(self, current_packet = None) -> bool: ...\n    def receive_wake_signal(self) -> None: ...\n    def set_voice_active(self, active: bool) -> None: ...\n    def transition_to_waking(self) -> None: ...\n    def complete_wake(self) -> Dict[str, Any]: ...\n    def enter_dreaming(self, handoff_id: str) -> bool: ...\n    def exit_dreaming(self) -> bool: ...\n    def enter_distracted(self) -> bool: ...\n    def exit_distracted(self) -> bool: ...\n    def initiate_offline(self) -> None: ...\n    def get_status(self) -> Dict[str, Any]: ...\n    def get_canned_response(self) -> Optional[str]: ...\n    def _format_checkpoint_as_review(checkpoint: str) -> str: ...\n\n\n## FILE: gaia-core/cognition/telemetric_senses.py\n# AST Summary (substitute with actual source at training time)\ndef tick(): ...\ndef update_token_usage(count: int): ...\ndef scan_files(): ...\ndef get_gpu_usage() -> dict[str, any]: ...\ndef get_hardware_profile() -> dict[str, any]: ...\ndef get_system_resources() -> dict[str, any]: ...\ndef system_health(): ...\ndef get_telemetry_summary() -> str: ...\ndef full_sense_sweep(): ...\n\n## FILE: gaia-core/cognition/temporal_interviewer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Temporal Interviewer â€” Prime interviews past-Lite via KV cache state swapping.\n\nPhase 2 of the Temporal Awareness Framework.  Prime formulates structured\nquestions about a past moment, Lite answers fr\"\"\"\nclass TemporalInterviewer():\n    \"\"\"Orchestrates Prime-interviews-past-Lite sessions.\"\"\"\n    def __init__(self, config, model_pool = None, temporal_state_manager = None, lite_journal = None, timeline_store = None) -> None: ...\n    def conduct_interview(self, state_id: Optional[str] = None) -> Optional[Dict[str, Any]]: ...\n    def _select_interview_target(self) -> Optional[Dict[str, Any]]: ...\n    def _has_transcript(self, state_id: str) -> bool: ...\n    def _run_interview_rounds(self, llm, state_metadata: Dict[str, Any]) -> List[Dict[str, str]]: ...\n    def _prime_ask(self, previous_rounds: List[Dict[str, str]], state_ts: str, round_idx: int) -> str: ...\n    def _lite_answer(self, llm, previous_rounds: List[Dict[str, str]], question: str) -> str: ...\n    def _analyze_coherence(self, transcript_rounds: List[Dict[str, str]], state_metadata: Dict[str, Any]) -> Dict[str, Any]: ...\n    def _get_journal_for_state(self, state_metadata: Dict[str, Any]) -> str: ...\n    def _parse_coherence(self, text: str) -> Dict[str, Any]: ...\n    def _default_coherence() -> Dict[str, Any]: ...\n    def _build_transcript(self, state_id: str, state_metadata: Dict[str, Any], rounds: List[Dict[str, str]], coherence: Dict[str, Any], duration_ms: int) -> Dict[str, Any]: ...\n    def _save_transcript(self, transcript: Dict[str, Any]) -> Optional[Path]: ...\n    def _emit_interview_event(self, transcript: Dict[str, Any]) -> None: ...\n\n\n## FILE: gaia-core/cognition/temporal_state_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Temporal State Manager â€” KV cache state baking, storage, and restoration for Lite.\n\nManages segmented KV cache snapshots that capture Lite's cognitive state at specific\npoints in time.  These snapshot\"\"\"\nclass TemporalStateManager():\n    \"\"\"Manages Lite KV cache state snapshots for temporal self-awareness.\"\"\"\n    def __init__(self, config, model_pool = None, timeline_store = None, session_manager = None, lite_journal = None) -> None: ...\n    def bake_state(self) -> Optional[Path]: ...\n    def load_state(self, state_id: str) -> bool: ...\n    def restore_current(self) -> bool: ...\n    def list_states(self) -> List[Dict[str, Any]]: ...\n    def get_state_metadata(self, state_id: str) -> Optional[Dict[str, Any]]: ...\n    def cleanup_old_states(self) -> int: ...\n    def _build_bake_context(self) -> List[Dict[str, str]]: ...\n    def _reconstruct_wake_cycle(self) -> str: ...\n    def _reconstruct_timeline_context(self) -> str: ...\n    def _reconstruct_conversation_context(self) -> str: ...\n    def _reconstruct_world_state(self) -> str: ...\n    def _reconstruct_journal_content(self) -> str: ...\n    def _save_lite_state(self, llm, metadata: Dict[str, Any]) -> Path: ...\n    def save_current_state_memory(self, llm) -> Any: ...\n    def restore_state_memory(self, llm, state_data) -> None: ...\n    def _load_lite_state(self, llm, state_path: Path) -> bool: ...\n    def _build_metadata(self, bake_duration_ms: int) -> Dict[str, Any]: ...\n    def _delete_state_files(self, bin_path: Path) -> None: ...\n    def _event_summary(data: Dict[str, Any]) -> str: ...\n\n\n## FILE: gaia-core/cognition/tests/test_goal_detector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for GoalDetector.\"\"\"\nclass TestFastPath():\n    def test_greeting_maps_to_casual_conversation(self): ...\n    def test_question_maps_to_information_seeking(self): ...\n    def test_tool_use_maps_to_task_execution(self): ...\n    def test_help_request_maps_to_task_assistance(self): ...\n\nclass TestSessionCarry():\n    def test_carries_active_goal(self): ...\n    def test_carry_decays_after_max_turns(self): ...\n\nclass TestLLMDetect():\n    def test_llm_detection_parses_response(self): ...\n    def test_llm_detection_graceful_on_failure(self): ...\n\nclass TestGoalShift():\n    def test_goal_shift_archives_previous(self): ...\n    def test_goal_shift_without_previous_goal(self): ...\n\nclass TestEdgeCases():\n    def test_no_goal_on_empty_input(self): ...\n    def test_fast_path_takes_priority_over_carry(self): ...\n    def test_parse_llm_response_handles_garbage(self): ...\n    def test_persistence_round_trip(self): ...\n\ndef _make_packet(user_intent: str = 'greeting', user_input: str = 'Hello!') -> CognitionPacket: ...\ndef _make_session_manager(meta: dict | None = None): ...\n\n## FILE: gaia-core/cognition/tests/test_heartbeat.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for ThoughtSeedHeartbeat â€” GAIA's thought seed triage daemon.\"\"\"\nclass TestSeedDirectoryOps():\n    def test_archive_seed_moves_file(self, _patch_seeds_dirs): ...\n    def test_archive_nonexistent_returns_false(self, _patch_seeds_dirs): ...\n    def test_defer_seed_moves_file(self, _patch_seeds_dirs): ...\n    def test_defer_seed_with_revisit_after(self, _patch_seeds_dirs): ...\n    def test_pending_due_promotes_back(self, _patch_seeds_dirs): ...\n    def test_pending_not_due_stays(self, _patch_seeds_dirs): ...\n    def test_pending_with_future_revisit_stays(self, _patch_seeds_dirs): ...\n    def test_pending_with_past_revisit_promotes(self, _patch_seeds_dirs): ...\n\nclass TestTriageSeed():\n    def test_archive_decision(self): ...\n    def test_pending_decision(self): ...\n    def test_act_decision(self): ...\n    def test_unparseable_defaults_to_pending(self): ...\n    def test_llm_failure_defaults_to_pending(self): ...\n\nclass TestActOnSeed():\n    def test_act_when_active(self, _patch_seeds_dirs): ...\n    def test_act_defers_when_dreaming(self, _patch_seeds_dirs): ...\n    def test_seed_archived_after_act(self, _patch_seeds_dirs): ...\n\nclass FakeConfig():\n\nclass TestHeartbeatLifecycle():\n    def test_start_creates_thread(self): ...\n    def test_stop_terminates_thread(self): ...\n    def test_tick_emits_timeline_event(self, _patch_seeds_dirs): ...\n    def test_tick_triages_seeds(self, _patch_seeds_dirs): ...\n\nclass TestTemporalIntegration():\n    def test_tick_writes_journal_entry(self, _patch_seeds_dirs): ...\n    def test_tick_bakes_state_on_interval(self, _patch_seeds_dirs): ...\n    def test_tick_skips_bake_off_interval(self, _patch_seeds_dirs): ...\n\ndef _patch_seeds_dirs(tmp_path, monkeypatch): ...\ndef _write_seed(seeds_dir: Path, filename: str, **overrides) -> Path: ...\ndef _mock_llm(response_text: str) -> MagicMock: ...\n\n## FILE: gaia-core/cognition/tests/test_initiative_engine.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for InitiativeEngine.\"\"\"\nclass FakeConfig():\n\nclass TestNoTopics():\n    def test_returns_none_when_no_topics(self, mock_pt, config, mock_agent_core): ...\n\nclass TestNoAgentCore():\n    def test_returns_none_without_agent_core(self, config): ...\n\nclass TestSuccessfulTurn():\n    def test_executes_turn_with_topic(self, mock_pt, config, mock_agent_core): ...\n    def test_uses_gil_session_id(self, mock_pt, config, mock_agent_core): ...\n\nclass TestSelfPrompt():\n    def test_prompt_contains_topic_description(self): ...\n    def test_prompt_contains_metadata(self): ...\n    def test_prompt_contains_reflection_header(self): ...\n\nclass TestErrorHandling():\n    def test_agent_core_error_returns_error_status(self, mock_pt, config): ...\n\ndef config(): ...\ndef mock_agent_core(): ...\n\n## FILE: gaia-core/cognition/tests/test_lite_journal.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for LiteJournal â€” Lite's introspective journal system.\"\"\"\nclass TestJournalLifecycle():\n    def test_write_creates_journal_file(self, mock_config): ...\n    def test_write_appends_to_existing(self, mock_config): ...\n    def test_returns_none_without_llm(self, mock_config): ...\n    def test_returns_none_without_model_pool(self, mock_config): ...\n    def test_entry_format_has_timestamp(self, mock_config): ...\n    def test_entry_includes_state_metadata(self, mock_config): ...\n\nclass TestRotation():\n    def test_rotate_when_max_entries_exceeded(self, mock_config): ...\n    def test_history_dir_created_on_rotate(self, mock_config): ...\n    def test_rotated_file_is_timestamped(self, mock_config): ...\n\nclass TestLoadEntries():\n    def test_load_latest_returns_full_content(self, mock_config): ...\n    def test_load_recent_entries_returns_n(self, mock_config): ...\n    def test_load_recent_entries_empty_journal(self, mock_config): ...\n    def test_get_entry_count(self, mock_config): ...\n\ndef mock_config(tmp_path): ...\ndef _mock_llm(response: str = ...) -> MagicMock: ...\ndef _mock_pool(response: str = ...) -> MagicMock: ...\ndef _mock_swm(state: str = 'active', seconds: float = 3600.0) -> MagicMock: ...\n\n## FILE: gaia-core/cognition/tests/test_sleep_gpu_integration.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for sleep cycle GPU release/reclaim and Discord presence wiring.\n\nValidates that SleepCycleLoop correctly:\n- Calls orchestrator /gpu/sleep when entering sleep\n- Calls orchestrator /gpu/wake when\"\"\"\nclass TestGPUReleaseOnSleep():\n    def test_gpu_release_called_on_sleep(self, loop): ...\n    def test_gpu_release_failure_nonfatal(self, loop): ...\n    def test_no_gpu_release_when_drowsy_cancelled(self, loop): ...\n\nclass TestGPUReclaimOnWake():\n    def test_gpu_reclaim_called_on_wake(self, loop): ...\n    def test_gpu_reclaim_failure_nonfatal(self, loop): ...\n\nclass TestPresenceDuringSleep():\n    def test_presence_sleeping_during_sleep(self, loop, mock_discord): ...\n    def test_presence_sleeping_during_task(self, loop, mock_discord): ...\n    def test_presence_resets_on_wake(self, loop, mock_discord): ...\n\nclass TestDreamingPresence():\n    def test_dreaming_shows_dnd_studying(self, loop, mock_discord): ...\n\nclass TestSOAPresence():\n    def test_soa_presence_calls_web_endpoint(self, mock_config): ...\n    def test_soa_presence_online_when_not_sleeping(self, mock_config): ...\n    def test_soa_presence_failure_nonfatal(self, mock_config): ...\n    def test_soa_presence_invisible_on_offline(self, mock_config): ...\n    def test_soa_presence_dnd_override(self, mock_config): ...\n\ndef mock_config(): ...\ndef mock_discord(): ...\ndef loop(mock_config, mock_discord): ...\n\n## FILE: gaia-core/cognition/tests/test_sleep_task_scheduler.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for SleepTaskScheduler.\"\"\"\nclass FakeConfig():\n\nclass TestRegistration():\n    def test_default_tasks_registered(self, scheduler): ...\n    def test_default_task_ids(self, scheduler): ...\n    def test_register_custom_task(self, bare_scheduler): ...\n\nclass TestScheduling():\n    def test_priority_ordering(self, bare_scheduler): ...\n    def test_lru_within_same_priority(self, bare_scheduler): ...\n    def test_never_run_beats_recently_run(self, bare_scheduler): ...\n    def test_empty_scheduler_returns_none(self, bare_scheduler): ...\n\nclass TestExecution():\n    def test_successful_execution(self, bare_scheduler): ...\n    def test_failed_execution(self, bare_scheduler): ...\n    def test_run_count_increments(self, bare_scheduler): ...\n    def test_failed_task_does_not_crash_scheduler(self, bare_scheduler): ...\n\nclass TestStatus():\n    def test_status_shape(self, scheduler): ...\n    def test_status_reflects_execution(self, bare_scheduler): ...\n\nclass TestBlueprintValidation():\n    def test_task_registered(self, scheduler): ...\n    def test_task_priority(self, scheduler): ...\n    def test_extract_enums(self, tmp_path): ...\n    def test_extract_endpoints(self, tmp_path): ...\n    def test_extract_constants(self, tmp_path): ...\n    def test_detects_stale_enum(self, tmp_path): ...\n    def test_append_update_notes(self, tmp_path): ...\n    def test_no_false_positives_when_current(self, tmp_path): ...\n\ndef config(): ...\ndef scheduler(config): ...\ndef bare_scheduler(config): ...\n\n## FILE: gaia-core/cognition/tests/test_sleep_wake_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for the GAIA sleep/wake state machine.\n\nTests the 6-state + 2-phase lifecycle:\n    ACTIVE â†’ DROWSY â†’ ASLEEP â†’ DREAMING / DISTRACTED / OFFLINE\n    Internal phases: _FINISHING_TASK, _WAKING\n\n\"\"\"\nclass TestInitialState():\n    def test_starts_active(self, manager): ...\n    def test_no_pending_wake(self, manager): ...\n    def test_prime_not_available(self, manager): ...\n    def test_phase_none(self, manager): ...\n\nclass TestDrowsyThreshold():\n    def test_below_threshold(self, manager): ...\n    def test_at_threshold(self, manager): ...\n    def test_above_threshold(self, manager): ...\n    def test_not_when_asleep(self, manager): ...\n    def test_not_when_dreaming(self, manager): ...\n    def test_not_when_distracted(self, manager): ...\n\nclass TestInitiateDrowsy():\n    def test_happy_path(self, manager): ...\n    def test_checkpoint_written(self, manager, mock_config, tmp_path): ...\n    def test_rejects_from_asleep(self, manager): ...\n    def test_rejects_from_dreaming(self, manager): ...\n    def test_rejects_from_distracted(self, manager): ...\n    def test_cancels_on_wake_during_checkpoint(self, manager): ...\n    def test_rotation_before_create(self, manager): ...\n    def test_previous_checkpoint_preserved_in_backup(self, manager, mock_config): ...\n    def test_consumed_sentinel_cleared_on_new_checkpoint(self, manager, mock_config): ...\n\nclass TestReceiveWakeSignal():\n    def test_wake_while_active(self, manager): ...\n    def test_wake_while_asleep_transitions_to_waking(self, manager): ...\n    def test_wake_while_asleep_non_interruptible(self, manager): ...\n    def test_wake_while_drowsy_sets_flag(self, manager): ...\n    def test_wake_while_dreaming_defers(self, manager): ...\n    def test_wake_while_distracted_notes(self, manager): ...\n\nclass TestCompleteWake():\n    def test_restores_from_checkpoint(self, manager): ...\n    def test_complete_wake_no_checkpoint(self, manager, mock_config, tmp_path): ...\n    def test_complete_wake_wrong_state(self, manager): ...\n    def test_complete_wake_marks_consumed(self, manager, mock_config): ...\n    def test_complete_wake_no_consumed_when_no_checkpoint(self, manager, mock_config): ...\n\nclass TestStatus():\n    def test_status_fields(self, manager): ...\n    def test_status_reflects_state_change(self, manager): ...\n    def test_status_includes_dreaming_handoff_id(self, manager): ...\n\nclass TestFormatCheckpoint():\n    def test_empty_checkpoint(self): ...\n    def test_review_framing(self): ...\n\nclass TestTransitionToWaking():\n    def test_from_asleep(self, manager): ...\n    def test_rejects_from_active(self, manager): ...\n\nclass TestCannedResponses():\n    def test_no_canned_when_active(self, manager): ...\n    def test_no_canned_when_asleep(self, manager): ...\n    def test_canned_when_dreaming(self, manager): ...\n    def test_canned_when_distracted(self, manager): ...\n\nclass TestDreamingTransition():\n    def test_enter_dreaming_from_asleep(self, manager): ...\n    def test_enter_dreaming_rejects_from_active(self, manager): ...\n    def test_exit_dreaming_to_asleep(self, manager): ...\n    def test_exit_dreaming_triggers_pending_wake(self, manager): ...\n    def test_exit_dreaming_rejects_from_active(self, manager): ...\n\nclass TestDistractedTransition():\n    def test_enter_distracted_from_asleep(self, manager): ...\n    def test_enter_distracted_rejects_from_active(self, manager): ...\n    def test_exit_distracted_to_asleep(self, manager): ...\n    def test_exit_distracted_triggers_pending_wake(self, manager): ...\n    def test_exit_distracted_rejects_from_active(self, manager): ...\n\nclass TestOffline():\n    def test_offline_from_active(self, manager): ...\n    def test_offline_from_asleep(self, manager): ...\n    def test_offline_from_dreaming(self, manager): ...\n    def test_offline_from_distracted(self, manager): ...\n\nclass TestLLMCheckpoint():\n    def test_llm_checkpoint_with_model_pool(self, mock_config): ...\n    def test_llm_fallback_on_no_lite_model(self, mock_config): ...\n    def test_llm_fallback_on_exception(self, mock_config): ...\n    def test_template_without_model_pool(self, manager, mock_config): ...\n\nclass TestCheckpointConsumed():\n    def test_is_consumed_false_initially(self, manager): ...\n    def test_mark_consumed_creates_sentinel(self, manager, mock_config): ...\n    def test_create_checkpoint_clears_consumed(self, manager, mock_config): ...\n\nclass TestDrowsyCancelBehavior():\n    def test_early_cancel_skips_checkpoint_entirely(self, mock_config): ...\n    def test_late_cancel_marks_checkpoint_consumed(self, mock_config): ...\n\nclass TestWakeSignalIdleMonitor():\n    def test_receive_wake_signal_resets_idle_monitor(self, mock_config): ...\n    def test_receive_wake_signal_without_idle_monitor(self, mock_config): ...\n    def test_wake_signal_while_active_does_not_reset_idle(self, mock_config): ...\n    def test_wake_signal_during_drowsy_resets_idle(self, mock_config): ...\n\ndef mock_config(tmp_path): ...\ndef manager(mock_config): ...\n\n## FILE: gaia-core/cognition/tests/test_stream_observer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for StreamObserver.verify_side_effects.\n\nTests the post-execution verification layer that checks whether side\neffects reported by route_output() actually produced the expected\nartifacts (fi\"\"\"\nclass DummyLLM():\n    \"\"\"Minimal LLM stub that satisfies StreamObserver.__init__.\"\"\"\n    def create_chat_completion(self, **kwargs): ...\n\ndef _make_packet() -> CognitionPacket: ...\ndef mock_config(): ...\ndef observer(mock_config): ...\ndef packet(): ...\ndef test_verify_thought_seed_file_exists(observer, packet, tmp_path): ...\ndef test_verify_thought_seed_file_missing(observer, packet, tmp_path): ...\ndef test_verify_thought_seed_file_empty(observer, packet, tmp_path): ...\ndef test_verify_sidecar_action_success(observer, packet): ...\ndef test_verify_sidecar_action_error(observer, packet): ...\ndef test_verify_goal_shift_ok(observer, packet): ...\ndef test_verify_no_side_effects(observer, packet): ...\ndef test_verify_disabled_by_config(mock_config, packet): ...\ndef test_verify_fallback_thought_seed(observer, packet, tmp_path, monkeypatch): ...\ndef test_verify_appends_reflection_log(observer, packet): ...\ndef test_verify_multiple_issues(observer, packet, tmp_path): ...\ndef test_verify_no_packet_returns_ok(observer): ...\n\n## FILE: gaia-core/cognition/tests/test_temporal_context.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for the Temporal Context Builder.\"\"\"\nclass TestSemanticTime():\n    def test_includes_day_of_week(self): ...\n    def test_morning(self): ...\n    def test_afternoon(self): ...\n    def test_evening(self): ...\n    def test_night(self): ...\n    def test_early_morning(self): ...\n\nclass TestFormatDuration():\n    def test_sub_minute(self): ...\n    def test_minutes(self): ...\n    def test_hours_and_minutes(self): ...\n    def test_exact_hours(self): ...\n\nclass TestSessionSummary():\n    def test_full_session(self): ...\n    def test_no_data_returns_empty(self): ...\n\nclass TestStateSummary():\n    def test_active_state(self): ...\n    def test_empty_returns_empty(self): ...\n\nclass TestCodeEvolutionSummary():\n    def test_reads_snapshot(self, tmp_path): ...\n    def test_no_changes(self, tmp_path): ...\n    def test_missing_file(self): ...\n\nclass TestBuildTemporalContext():\n    def test_minimal_output(self): ...\n    def test_with_session_data(self): ...\n    def test_graceful_with_broken_timeline(self): ...\n\n\n## FILE: gaia-core/cognition/tests/test_temporal_interviewer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for TemporalInterviewer â€” Prime interviews past-Lite via KV cache swapping.\"\"\"\nclass FakeLlamaState():\n    \"\"\"Picklable stand-in for llama_cpp.LlamaState.\"\"\"\n    def __init__(self, label: str = 'default'): ...\n\nclass TestInterviewTargetSelection():\n    def test_selects_oldest_uninterviewed_state(self, interviewer): ...\n    def test_skips_most_recent_state(self, mock_config, mock_model_pool): ...\n    def test_falls_back_to_already_interviewed(self, interviewer, tsm_with_states): ...\n    def test_returns_none_with_no_states(self, mock_config, mock_model_pool): ...\n\nclass TestInterviewFlow():\n    def test_full_interview_cycle(self, interviewer, mock_llm, mock_model_pool): ...\n    def test_state_restored_on_interview_error(self, interviewer, mock_llm): ...\n    def test_returns_none_without_model_pool(self, mock_config, tsm_with_states): ...\n    def test_returns_none_without_tsm(self, mock_config, mock_model_pool): ...\n\nclass TestLockBehavior():\n    def test_interview_holds_lite_lock(self, interviewer, mock_llm): ...\n    def test_lock_released_after_interview(self, interviewer): ...\n\nclass TestNarrativeCoherence():\n    def test_coherence_analysis_called(self, interviewer, mock_model_pool): ...\n    def test_coherence_parsing(self, interviewer): ...\n    def test_coherence_graceful_on_parse_failure(self, interviewer): ...\n\nclass TestTranscriptStorage():\n    def test_transcript_saved_as_json(self, interviewer): ...\n    def test_transcript_contains_all_rounds(self, interviewer): ...\n    def test_transcript_dir_created(self, mock_config, mock_model_pool, tsm_with_states): ...\n\nclass FakeConfig():\n\nclass TestHeartbeatIntegration():\n    def test_interview_triggered_on_interval(self): ...\n    def test_interview_not_triggered_off_interval(self): ...\n    def test_interview_skipped_when_sleeping(self): ...\n    def test_interview_failure_doesnt_crash_heartbeat(self): ...\n\ndef mock_config(tmp_path): ...\ndef mock_llm(): ...\ndef mock_model_pool(mock_llm): ...\ndef _create_baked_state(state_dir: Path, ts_label: str) -> str: ...\ndef tsm_with_states(mock_config, mock_model_pool, tmp_path): ...\ndef mock_journal(): ...\ndef mock_timeline(): ...\ndef interviewer(mock_config, mock_model_pool, tsm_with_states, mock_journal, mock_timeline): ...\n\n## FILE: gaia-core/cognition/tests/test_temporal_state_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for TemporalStateManager â€” KV cache state baking and restoration.\"\"\"\nclass FakeLlamaState():\n    \"\"\"Picklable stand-in for llama_cpp.LlamaState.\"\"\"\n    def __init__(self): ...\n\nclass TestStateDirectory():\n    def test_creates_state_dir(self, mock_config, mock_model_pool): ...\n    def test_list_states_empty(self, mock_config, mock_model_pool): ...\n\nclass TestBakeState():\n    def test_bake_creates_bin_file(self, mock_config, mock_model_pool, mock_llm): ...\n    def test_bake_creates_json_sidecar(self, mock_config, mock_model_pool): ...\n    def test_bake_returns_none_without_llm(self, mock_config): ...\n    def test_bake_returns_none_without_model_pool(self, mock_config): ...\n    def test_bake_state_is_picklable(self, mock_config, mock_model_pool): ...\n    def test_bake_includes_journal_content(self, mock_config, mock_model_pool, mock_llm, mock_journal): ...\n\nclass TestLoadState():\n    def test_load_existing_state(self, mock_config, mock_model_pool, mock_llm): ...\n    def test_load_nonexistent_returns_false(self, mock_config, mock_model_pool): ...\n    def test_corrupt_state_renamed(self, mock_config, mock_model_pool, mock_llm): ...\n    def test_restore_current_loads_latest(self, mock_config, mock_model_pool, mock_llm): ...\n\nclass TestRotation():\n    def test_cleanup_enforces_max_files(self, mock_config, mock_model_pool): ...\n    def test_cleanup_enforces_max_bytes(self, mock_config, mock_model_pool): ...\n    def test_cleanup_deletes_sidecar_too(self, mock_config, mock_model_pool): ...\n\nclass TestContextReconstruction():\n    def test_reconstruct_timeline_context(self, mock_config, mock_model_pool, mock_timeline): ...\n    def test_reconstruct_conversation_context(self, mock_config, mock_model_pool, mock_session_manager): ...\n    def test_list_states_with_metadata(self, mock_config, mock_model_pool): ...\n\ndef mock_config(tmp_path): ...\ndef mock_llm(): ...\ndef mock_model_pool(mock_llm): ...\ndef mock_timeline(): ...\ndef mock_session_manager(): ...\ndef mock_journal(): ...\n\n## FILE: gaia-core/cognition/thought_seed.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Thought Seed System (GAIA pillar-compliant).\n- Saves, stores, reviews, and processes thought seeds.\n- Seeds are now generated by the main LLM via the THOUGHT_SEED: directive\n- and parsed by the output\"\"\"\ndef save_thought_seed(seed_text: str, packet: CognitionPacket, config: Config) -> Dict[str, Any] | None: ...\ndef list_unreviewed_seeds(): ...\ndef get_seed_by_id(seed_id: str) -> Dict[str, Any] | None: ...\ndef update_seed(seed_id: str, seed_data: Dict[str, Any]) -> bool: ...\ndef review_and_process_seeds(config = None, llm = None, auto_act = False): ...\ndef refine_seed(seed_id, refinement_prompt, config = None, llm = None): ...\ndef link_seeds(source_seed_id, target_seed_id, relationship): ...\ndef maybe_review_seeds(config, llm = None): ...\ndef archive_seed(seed_filename: str) -> bool: ...\ndef defer_seed(seed_filename: str, revisit_after: str | None = None) -> bool: ...\ndef list_pending_seeds_due() -> list[tuple[Path, dict]]: ...\n\n## FILE: gaia-core/cognition/tool_selector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tool Selector Module\n\nResponsible for:\n1. Determining if a request needs MCP tool usage\n2. Selecting the appropriate tool with low-temperature generation\n3. Extracting structured parameters\n4. Providi\"\"\"\ndef _structured_json_kwargs(model: Any, schema: Dict[str, Any]) -> Dict[str, Any]: ...\ndef _registry_to_catalog(registry: Dict[str, Any]) -> Dict[str, Any]: ...\ndef needs_tool_routing(packet: CognitionPacket, user_input: str) -> bool: ...\ndef select_tool(packet: CognitionPacket, user_input: str, model, temperature: float = 0.15) -> Tuple[Optional[SelectedTool], List[SelectedTool]]: ...\ndef review_selection(packet: CognitionPacket, selected_tool: SelectedTool, model, temperature: float = 0.3) -> Tuple[float, str]: ...\ndef _build_tool_catalog() -> str: ...\ndef _extract_content(result) -> str: ...\ndef _extract_json_from_response(content: str) -> str: ...\ndef initialize_tool_routing(packet: CognitionPacket) -> CognitionPacket: ...\ndef inject_tool_result_into_packet(packet: CognitionPacket) -> CognitionPacket: ...\n\n## FILE: gaia-core/cognition/topic_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Manages the creation, update, resolution, prioritization, and pruning of GAIA's topic cache.\n\nSupports the Initiative Loop (GIL) by maintaining a prioritized, well-structured list of emergent discussi\"\"\"\ndef _load_topic_cache(path: str) -> List[Dict[str, Any]]: ...\ndef _save_topic_cache(path: str, cache: List[Dict[str, Any]]): ...\ndef add_topic(path: str, topic: Dict[str, Any]) -> None: ...\ndef resolve_topic(path: str, topic_id: str) -> bool: ...\ndef update_topic(path: str, topic_id: str, updates: Dict[str, Any]) -> bool: ...\ndef prune_resolved_topics(path: str) -> None: ...\ndef list_topics(path: str, include_resolved: bool = False) -> List[Dict[str, Any]]: ...\ndef prioritize_topics(path: str, top_n: Optional[int] = 5) -> List[Dict[str, Any]]: ...\n\n## FILE: gaia-core/config.py\n# AST Summary (substitute with actual source at training time)\nclass Config():\n    \"\"\"A simplified configuration class for gaia-core.\nValues should be injected at runtime or loaded from \"\"\"\n    def __post_init__(self): ...\n    def _load_constants(self): ...\n    def get_api_key(self, provider: str) -> str: ...\n    def get_persona_instructions(self) -> str: ...\n    def get_model_name(self, model_alias: str) -> str: ...\n    def get_instance(cls) -> Config: ...\n    def _load_cheat_sheet(self): ...\n\ndef get_config() -> Config: ...\n\n## FILE: gaia-core/ethics/__init__.py\n# AST Summary (substitute with actual source at training time)\n\n## FILE: gaia-core/ethics/consent_protocol.py\n# AST Summary (substitute with actual source at training time)\nclass ConsentProtocol():\n    \"\"\"Verifies GAIA's explicit consent to operate under current identity, context, and system state.\nMust \"\"\"\n    def request_consent(reason = 'Initial boot') -> bool: ...\n\n\n## FILE: gaia-core/ethics/core_identity_guardian.py\n# AST Summary (substitute with actual source at training time)\nclass CoreIdentityGuardian():\n    \"\"\"Verifies prompt behavior and session instructions against GAIA's immutable Tier I identity.\nOperates\"\"\"\n    def __init__(self, config): ...\n    def load_identity(self) -> Optional[dict]: ...\n    def validate_prompt_stack(self, persona_traits: dict, instructions: List[str], prompt: str) -> bool: ...\n\n\n## FILE: gaia-core/ethics/ethical_sentinel.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"ethics/ethical_sentinel.py\n\nThe Ethical Sentinel monitors system health and cognitive strain for GAIA.\"\"\"\nclass EthicalSentinel():\n    \"\"\"Monitors system health, loop safety, error logs, and optionally Tier I identity violations.\nWorks al\"\"\"\n    def __init__(self, identity_guardian = None): ...\n    def check_system_resources(self) -> bool: ...\n    def check_loop_counter(self) -> bool: ...\n    def check_recent_errors(self) -> bool: ...\n    def register_error(self, exc: Exception): ...\n    def reset_loop(self): ...\n    def run_full_safety_check(self, persona_traits = None, instructions = None, prompt = None) -> bool: ...\n\n\n## FILE: gaia-core/integrations/discord_connector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Discord Connector for GAIA Spinal Column\n\nThis module provides Discord integration as both:\n- Output destination (send responses to Discord channels via webhook or bot)\n- Input source (listen for @GAI\"\"\"\nclass DiscordConnector(DestinationConnector):\n    \"\"\"Discord connector for the GAIA spinal column.\nSupports both webhook output and (future) bot-based bi\"\"\"\n    def __init__(self, config: Optional[DiscordConfig] = None): ...\n    def send(self, content: str, target: DestinationTarget, packet: Optional[CognitionPacket] = None) -> bool: ...\n    def send_stream(self, token_generator: Generator[str, None, None], target: DestinationTarget, packet: Optional[CognitionPacket] = None) -> bool: ...\n    def is_available(self) -> bool: ...\n    def _send_via_webhook(self, content: str, target: DestinationTarget, packet: Optional[CognitionPacket] = None) -> bool: ...\n    def _send_via_bot(self, content: str, target: DestinationTarget, packet: Optional[CognitionPacket] = None) -> bool: ...\n    def _split_message_for_discord(self, content: str) -> List[str]: ...\n    def _is_bot_connected(self) -> bool: ...\n    def set_message_callback(self, callback: Callable[[str, str, Dict[str, Any]], None]) -> None: ...\n    def generate_dm_session_id(user_id: str) -> str: ...\n    def is_dm_session(session_id: str) -> bool: ...\n    def start_bot_listener(self) -> bool: ...\n    def stop_bot_listener(self) -> None: ...\n\n\n## FILE: gaia-core/main.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia-core FastAPI application entry point.\n\nProvides the HTTP API for the cognitive loop service.\nThis is The Brain - Cognitive loop and reasoning.\"\"\"\nclass AIManagerShim():\n    \"\"\"A lightweight shim providing the interface AgentCore expects from ai_manager.\n\nAgentCore requires:\n-\"\"\"\n    def __init__(self, config, model_pool, session_manager): ...\n    def initialize(self, persona_name: str = 'prime'): ...\n\nclass MinimalPersona():\n    \"\"\"Minimal persona object when full persona loading fails or from dict data.\"\"\"\n    def __init__(self, name: str, data: Dict[str, Any] = None): ...\n\ndef initialize_cognitive_system(): ...\ndef lifespan(app: FastAPI): ...\ndef _write_shutdown_checkpoints(app: FastAPI) -> dict: ...\ndef health_check(): ...\ndef root(): ...\ndef get_status(): ...\ndef cognition_checkpoint(): ...\ndef process_packet(packet_data: Dict[str, Any]): ...\n\n## FILE: gaia-core/memory/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.memory - Memory and state management modules.\n\nThis package provides:\n- dev_matrix: Development matrix for tracking agent state\n- conversation: Conversation memory subpackage\"\"\"\n\n## FILE: gaia-core/memory/codex_writer.py\n# AST Summary (substitute with actual source at training time)\nclass CodexWriter():\n    def __init__(self, config: Config, semantic_codex: SemanticCodex): ...\n    def document_information(self, packet: CognitionPacket, info_to_document: str, symbol: str, title: str, tags: Optional[List[str]] = None, llm_model: Optional[Model] = None) -> Optional[Path]: ...\n    def _refine_with_llm(self, raw_info: str, llm_model: Model, packet: CognitionPacket, symbol: str, title: str) -> str: ...\n\n\n## FILE: gaia-core/memory/conversation/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.memory.conversation - Conversation memory management.\n\nThis package provides:\n- summarizer: Conversation summarization for context management\"\"\"\n\n## FILE: gaia-core/memory/conversation/archiver.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"conversation/archiver.py\n\nHandles saving and loading archived conversations in Markdown format.\nManages storage, file formatting, and retrieval operations.\"\"\"\nclass ConversationArchiver():\n    \"\"\"Saves a conversation history to disk, structured by persona + session ID.\"\"\"\n    def __init__(self, config): ...\n    def archive_conversation(self, session_id: str, persona: str, messages: List[dict], summary: str, keywords: List[str]): ...\n\n\n## FILE: gaia-core/memory/conversation/keywords.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"conversation/keywords.py\n\nHandles keyword extraction from conversation history.\nLightweight heuristics for identifying important terms.\"\"\"\nclass ConversationKeywordExtractor():\n    \"\"\"Extracts high-value keywords from a conversation history.\"\"\"\n    def extract_keywords(self, messages: List[dict], max_keywords: int = 10) -> List[str]: ...\n\n\n## FILE: gaia-core/memory/conversation/manager.py\n# AST Summary (substitute with actual source at training time)\nclass ConversationManager():\n    \"\"\"Tracks user-assistant message history and triggers summarization + archiving\nafter N messages. Suppo\"\"\"\n    def __init__(self, config, llm = None, embed_model = None): ...\n    def set_persona(self, persona: str): ...\n    def add_message(self, role: str, content: str): ...\n    def get_recent_messages(self, count: int = 10) -> List[dict]: ...\n    def summarize_and_archive(self): ...\n    def reset(self): ...\n    def build_smart_history(self, current_input: str, max_recent: int = 3, max_salient: int = 2) -> List[Dict]: ...\n\n\n## FILE: gaia-core/memory/conversation/summarizer.py\n# AST Summary (substitute with actual source at training time)\nclass ConversationSummarizer():\n    \"\"\"Uses the LLM to summarize a conversation history.\nFalls back to placeholder text if no LLM is availa\"\"\"\n    def __init__(self, llm = None, embed_model = None): ...\n    def generate_summary(self, messages: List[dict], packet: object = None) -> str: ...\n    def build_smart_history(self, full_history: List[Dict], current_input: str, max_recent: int = 3, max_salient: int = 2) -> List[Dict]: ...\n\ndef _get_model_pool(): ...\n\n## FILE: gaia-core/memory/dev_matrix.py\n# AST Summary (substitute with actual source at training time)\nclass GAIADevMatrix():\n    \"\"\"Persistent manager for GAIA's self-development tasks.\nStores and retrieves structured roadmap tasks.\"\"\"\n    def __init__(self, config): ...\n    def _load(self): ...\n    def _save(self): ...\n    def add_task(self, label: str, purpose: str, urgency: str = 'medium', impact: str = 'medium', source: str = 'manual') -> None: ...\n    def get_open_tasks(self) -> List[Dict]: ...\n    def resolve_task(self, label: str) -> bool: ...\n    def dump(self) -> List[Dict]: ...\n\n\n## FILE: gaia-core/memory/knowledge_integrity.py\n# AST Summary (substitute with actual source at training time)\ndef hash_file(filepath): ...\ndef check_or_generate_hash_manifest(): ...\n\n## FILE: gaia-core/memory/memory_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"MemoryManager facade to unify short-term, session, and long-term memory.\n\nShort-term: in-process dict (fast cache)\nWorking: SessionManager (persistent session history)\nLong-term: VectorIndexer via MCP\"\"\"\nclass MemoryManager():\n    def __init__(self, config: Config = None): ...\n    def instance(cls, config: Config = None) -> 'MemoryManager': ...\n    def set_short(self, key: str, value: Any): ...\n    def get_short(self, key: str, default = None): ...\n    def add_message(self, session_id: str, role: str, content: str): ...\n    def get_history(self, session_id: str): ...\n    def query_long(self, query: str, top_k: int = 5): ...\n\n\n## FILE: gaia-core/memory/priority_manager.py\n# AST Summary (substitute with actual source at training time)\nclass GAIAPriorityManager():\n    \"\"\"Centralized persistent task and priority memory for GAIA.\nTracks open tasks from all sources (manual\"\"\"\n    def __init__(self, config): ...\n    def _load(self): ...\n    def _save(self): ...\n    def add_task(self, label: str, details: str, urgency: str = 'medium', impact: str = 'medium', source: str = 'manual') -> None: ...\n    def get_open_tasks(self) -> List[Dict]: ...\n    def get_top_tasks(self, limit: int = 5) -> List[Dict]: ...\n    def resolve_task(self, label: str) -> bool: ...\n    def clear_resolved(self): ...\n    def dump(self) -> List[Dict]: ...\n\n\n## FILE: gaia-core/memory/semantic_codex.py\n# AST Summary (substitute with actual source at training time)\nclass CodexEntry():\n\nclass SemanticCodex():\n    \"\"\"Side-car memory for semantically compressed concepts.\nLoads JSON/YAML files from Config.KNOWLEDGE_CO\"\"\"\n    def __init__(self, config): ...\n    def instance(cls, config): ...\n    def write_entry(self, entry: CodexEntry) -> Path: ...\n    def _iter_files(self): ...\n    def _checksum(self, path: Path) -> str: ...\n    def _load_one(self, path: Path): ...\n    def _load_all(self): ...\n    def hot_reload(self) -> bool: ...\n    def get(self, symbol: str) -> Optional[CodexEntry]: ...\n    def search(self, query: str, limit: int = 10) -> List[CodexEntry]: ...\n\n\n## FILE: gaia-core/memory/session_history_indexer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Per-session vector index for conversation turns and topic summaries.\n\nProvides semantic retrieval over session history so that older turns\ncan be recalled by relevance rather than recency. Designed to\"\"\"\nclass SessionHistoryIndexer():\n    \"\"\"Per-session vector index for conversation turns and topic summaries.\"\"\"\n    def __init__(self, session_id: str, persist_dir: str = _DEFAULT_PERSIST_DIR): ...\n    def instance(cls, session_id: str) -> 'SessionHistoryIndexer': ...\n    def _get_model(self): ...\n    def _encode(self, text: str) -> Optional[np.ndarray]: ...\n    def index_turn(self, turn_idx: int, user_msg: str, assistant_msg: str): ...\n    def retrieve(self, query: str, top_k_turns: int = 3, top_k_topics: int = 2, exclude_recent_n: int = 6) -> Dict: ...\n    def _maybe_generate_topic_summary(self): ...\n    def archive_and_reset(self): ...\n    def _save(self): ...\n    def _save_to(self, path: str): ...\n    def _load(self): ...\n\ndef _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float: ...\ndef _get_embed_model(): ...\n\n## FILE: gaia-core/memory/session_manager.py\n# AST Summary (substitute with actual source at training time)\nclass Session():\n    \"\"\"A dedicated data class to hold the state for a single conversation session.\nUsing a class instead of\"\"\"\n    def __init__(self, session_id: str, persona: str = 'default'): ...\n    def to_dict(self) -> Dict: ...\n    def from_dict(cls, data: Dict) -> 'Session': ...\n    def last_message_timestamp(self): ...\n\nclass SessionManager():\n    \"\"\"Manages loading, saving, and accessing all persistent conversation sessions.\nThis is the single sour\"\"\"\n    def __init__(self, config, llm = None, embed_model = None): ...\n    def _load_state(self) -> Dict[str, Session]: ...\n    def _sanitize_for_json(obj): ...\n    def _save_state(self): ...\n    def get_or_create_session(self, session_id: str, persona: str = 'default') -> Session: ...\n    def add_message(self, session_id: str, role: str, content: str): ...\n    def summarize_and_archive(self, session_id: str): ...\n    def get_session_meta(self, session_id: str, key: str, default = None): ...\n    def set_session_meta(self, session_id: str, key: str, value): ...\n    def get_history(self, session_id: str) -> List[Dict]: ...\n    def reset_session(self, session_id: str): ...\n    def sanitize_sessions(self, vector_dir: str = 'data/shared/session_vectors', max_age_days: int = 7, max_active_messages: int = 0) -> Dict[str, int]: ...\n    def record_last_activity(self): ...\n\n\n## FILE: gaia-core/memory/status_tracker.py\n# AST Summary (substitute with actual source at training time)\nclass GAIAStatus():\n    \"\"\"Thread-safe global status manager for GAIA boot and runtime state.\nAllows concurrent modules to upda\"\"\"\n    def update(cls, key: str, value): ...\n    def get(cls, key: str, default = None): ...\n    def as_dict(cls): ...\n    def clear(cls): ...\n\n\n## FILE: gaia-core/memory/tests/test_semantic_codex.py\n# AST Summary (substitute with actual source at training time)\ndef temp_knowledge_dir(tmp_path): ...\ndef mock_config(temp_knowledge_dir): ...\ndef semantic_codex(mock_config): ...\ndef test_write_entry_creates_markdown_file(semantic_codex, temp_knowledge_dir): ...\ndef test_load_one_markdown_with_front_matter(semantic_codex, temp_knowledge_dir): ...\ndef test_load_one_markdown_missing_symbol(semantic_codex, temp_knowledge_dir, caplog): ...\ndef test_load_one_markdown_invalid_yaml(semantic_codex, temp_knowledge_dir, caplog): ...\ndef test_load_one_json_still_works(semantic_codex, temp_knowledge_dir): ...\ndef test_iter_files_includes_self_generated_docs(semantic_codex, temp_knowledge_dir): ...\ndef test_hot_reload_updates_markdown_entry(semantic_codex, temp_knowledge_dir): ...\n\n## FILE: gaia-core/memory/tests/test_session_history_indexer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for SessionHistoryIndexer â€” the per-session vector index that powers\nthe RAG component of the rolling history feature.\n\nTests cover:\n1. Instantiation and singleton pattern\n2. Turn indexing (with\"\"\"\nclass FakeEmbedModel():\n    \"\"\"A deterministic fake embedding model for testing.\n\nEncodes text by hashing characters into a fixed-d\"\"\"\n    def encode(self, texts, show_progress_bar = False): ...\n\nclass TestInstantiation():\n    def test_creates_empty_index(self, indexer): ...\n    def test_singleton_returns_same_instance(self, persist_dir, patched_model): ...\n    def test_singleton_different_sessions_are_different(self, persist_dir, patched_model): ...\n\nclass TestTurnIndexing():\n    def test_index_one_turn(self, indexer): ...\n    def test_index_multiple_turns(self, indexer): ...\n    def test_duplicate_turn_idx_is_skipped(self, indexer): ...\n    def test_long_messages_are_truncated(self, indexer): ...\n    def test_embedding_is_numpy_array(self, indexer): ...\n\nclass TestRetrieval():\n    def _populate(self, indexer, n = 10): ...\n    def test_retrieve_returns_dict_with_turns_and_topics(self, indexer): ...\n    def test_retrieve_empty_index_returns_empty(self, indexer): ...\n    def test_retrieve_excludes_recent_turns(self, indexer): ...\n    def test_retrieve_returns_similarity_scores(self, indexer): ...\n    def test_retrieve_respects_top_k(self, indexer): ...\n    def test_retrieve_filters_by_minimum_threshold(self, indexer): ...\n\nclass TestTopicSummaries():\n    def test_no_topic_before_interval(self, indexer): ...\n    def test_topic_generated_at_interval(self, indexer): ...\n    def test_topic_has_correct_turn_range(self, indexer): ...\n    def test_multiple_topics_generated(self, indexer): ...\n    def test_topic_embedding_stored(self, indexer): ...\n    def test_topics_retrievable(self, indexer): ...\n\nclass TestPersistence():\n    def test_save_creates_json_file(self, indexer, persist_dir): ...\n    def test_load_restores_turns(self, persist_dir, patched_model): ...\n    def test_load_restores_topics(self, persist_dir, patched_model): ...\n    def test_load_restores_last_topic_turn_idx(self, persist_dir, patched_model): ...\n    def test_corrupt_file_starts_fresh(self, persist_dir, patched_model): ...\n\nclass TestArchiveAndReset():\n    def test_archive_creates_archive_file(self, indexer, persist_dir): ...\n    def test_archive_clears_index(self, indexer): ...\n    def test_archive_preserves_data_in_archive_file(self, indexer, persist_dir): ...\n    def test_archive_empty_index_is_noop(self, indexer, persist_dir): ...\n    def test_new_turns_after_archive(self, indexer): ...\n\nclass TestGracefulDegradation():\n    def test_index_turn_is_noop_without_model(self, indexer_no_model): ...\n    def test_retrieve_returns_empty_without_model(self, indexer_no_model): ...\n    def test_archive_works_without_model(self, indexer_no_model): ...\n    def test_no_topics_generated_without_model(self, indexer_no_model): ...\n\nclass TestCosineSimilarity():\n    def test_identical_vectors(self): ...\n    def test_orthogonal_vectors(self): ...\n    def test_zero_vector(self): ...\n    def test_opposite_vectors(self): ...\n\ndef clear_singletons(): ...\ndef persist_dir(tmp_path): ...\ndef fake_model(): ...\ndef patched_model(fake_model): ...\ndef no_model(): ...\ndef indexer(persist_dir, patched_model): ...\ndef indexer_no_model(persist_dir, no_model): ...\n\n## FILE: gaia-core/memory/tests/test_session_rag_integration.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Integration tests for the RAG + Rolling History pipeline.\n\nTests the interaction between:\n- SessionManager indexing hook (add_message â†’ index_turn)\n- AgentCore sliding window + RAG retrieval (_create_\"\"\"\nclass FakeEmbedModel():\n    \"\"\"Deterministic fake embedding model.\"\"\"\n    def encode(self, texts, show_progress_bar = False): ...\n\nclass TestSessionManagerIndexingHook():\n    \"\"\"Verify that SessionManager.add_message() triggers turn indexing.\"\"\"\n    def test_assistant_message_triggers_indexing(self, tmp_path, persist_dir, fake_model): ...\n    def test_user_message_alone_does_not_index(self, tmp_path, persist_dir, fake_model): ...\n    def test_indexing_failure_does_not_block_message(self, tmp_path): ...\n\nclass TestFormatRetrievedContext():\n    def test_empty_results(self): ...\n    def test_turns_only(self): ...\n    def test_topics_only(self): ...\n    def test_both_turns_and_topics(self): ...\n\nclass TestPromptBuilderTier15():\n    \"\"\"Test that Tier 1.5 (retrieved_session_context) is correctly injected.\"\"\"\n    def test_rag_content_appears_in_prompt(self): ...\n    def test_rag_content_truncated_when_over_budget(self): ...\n    def test_empty_rag_content_produces_no_prompt(self): ...\n\nclass TestSlidingWindow():\n    \"\"\"Test that the sliding window correctly limits history in the packet.\"\"\"\n    def test_short_history_fully_included(self): ...\n    def test_long_history_windowed(self): ...\n    def test_rag_only_triggers_beyond_window(self): ...\n\nclass TestArchiveFlowIntegration():\n    \"\"\"Test that summarize_and_archive correctly archives the vector index.\"\"\"\n    def test_archive_called_during_summarize(self, tmp_path, fake_model): ...\n\ndef clear_singletons(): ...\ndef persist_dir(tmp_path): ...\ndef fake_model(): ...\n\n## FILE: gaia-core/models/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.models - Model pool and LLM backend implementations.\n\nThis package provides:\n- model_pool: Unified model pool interface (Prime/Lite/Embedding)\n- model_manager: Model lifecycle management\n- v\"\"\"\n\n## FILE: gaia-core/models/_model_pool_impl.py\n# AST Summary (substitute with actual source at training time)\nclass ModelPool():\n    def __init__(self, config: Config = None): ...\n    def register_dev_model(self, name: str): ...\n    def enable_prime_load(self): ...\n    def load_models(self, use_oracle = False): ...\n    def load_prime_only(self, force: bool = False) -> bool: ...\n    def _prime_guard_allows(self, force: bool = False) -> bool: ...\n    def _auto_set_gpu_layers(self): ...\n    def _start_embed_loader(self): ...\n    def _apply_env_model_overrides(self): ...\n    def _ordered_model_keys(self) -> List[str]: ...\n    def _load_model_entry(self, model_name: str, use_oracle: bool = False, force: bool = False) -> bool: ...\n    def _promote_prime_aliases(self): ...\n    def wait_for_embed(self, timeout: float = None): ...\n    def get_embed_model(self, timeout: float = 0, lazy_load: bool = True): ...\n    def prewarm_embed(self, timeout: int = 10) -> bool: ...\n    def ensure_model_loaded(self, name: str, force: bool = False) -> bool: ...\n    def get(self, name: str, lazy_load: bool = True): ...\n    def get_model_for_role(self, role: str, lazy_load: bool = True): ...\n    def list_models(self): ...\n    def set_status(self, name: str, status: str): ...\n    def get_idle_model(self, exclude = []): ...\n    def acquire_model(self, name: str, lazy_load: bool = True): ...\n    def release_model(self, name: str): ...\n    def release_model_for_role(self, role: str): ...\n    def _resolve_model_name_for_role(self, role: str) -> str | None: ...\n    def acquire_model_for_role(self, role: str, lazy_load: bool = True): ...\n    def forward_to_model(self, role: str, messages: list, release: bool = True, **kwargs): ...\n    def get_active_persona(self) -> PersonaAdapter: ...\n    def set_persona(self, persona): ...\n    def complete(self, name: str, prompt: str, max_tokens: int = 128, temperature: float = 0.2) -> str: ...\n    def shutdown(self) -> None: ...\n\ndef _get_sentence_transformer(): ...\ndef _read_manifest(path: Path) -> dict: ...\ndef _ensure_download(role: str, spec: dict, models_dir: Path, scripts_dir: Path, allow_autosetup: bool) -> Path: ...\ndef resolve_model_paths(config: Config) -> dict: ...\ndef _get_gpu_free_total_bytes() -> tuple: ...\ndef _choose_initial_n_gpu(desired_n_gpu: int, free_bytes: int | None) -> int: ...\n\n## FILE: gaia-core/models/dev_model.py\n# AST Summary (substitute with actual source at training time)\nclass DevModel():\n    \"\"\"A mock model that prints the prompt to the console and waits for user input.\"\"\"\n    def __init__(self, name = 'dev_model'): ...\n    def create_chat_completion(self, messages, **kwargs): ...\n\n\n## FILE: gaia-core/models/document.py\n# AST Summary (substitute with actual source at training time)\nclass DocumentProcessor():\n    \"\"\"Handles loading, preprocessing, and converting documents into structured markdown or LangChain docum\"\"\"\n    def __init__(self, config, llm = None): ...\n    def extract_text_from_file(self, filepath: str) -> Optional[str]: ...\n    def _extract_rtf(self, filepath: str) -> Optional[str]: ...\n    def _extract_docx(self, filepath: str) -> Optional[str]: ...\n    def convert_to_markdown(self, text: str) -> Optional[str]: ...\n    def save_markdown(self, filepath: str, content: str) -> bool: ...\n    def load_and_preprocess_data(self, data_path: str) -> List[Document]: ...\n    def process_raw_data(self) -> None: ...\n    def get_document_info(self, filepath: str): ...\n    def process_documents(self, directory: str, tier: Optional[str] = None, project: Optional[str] = None) -> List[Document]: ...\n    def embed_documents(self) -> int: ...\n    def generate_artifacts(self) -> int: ...\n\n\n## FILE: gaia-core/models/fine_tune_gaia.py\n# AST Summary (substitute with actual source at training time)\ndef run_fine_tuning(config: Config, dataset_path: str, base_model_name: str, new_model_name: str): ...\ndef main(): ...\n\n## FILE: gaia-core/models/gemini_model.py\n# AST Summary (substitute with actual source at training time)\nclass GeminiAPIModel():\n    \"\"\"Minimal Gemini chat wrapper using the REST API.\nExpects GOOGLE_API_KEY in env. Model name is taken f\"\"\"\n    def __init__(self, model_name: str, api_key: str): ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int, temperature: float, top_p: float, stream: bool = False, **kwargs): ...\n\n\n## FILE: gaia-core/models/groq_model.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Groq API model wrapper for GAIA.\n\nGroq provides free, fast inference on open-source models via their custom LPU hardware.\nThis wrapper provides an OpenAI-compatible interface for use as a fallback whe\"\"\"\nclass GroqAPIModel():\n    \"\"\"Groq API wrapper providing create_chat_completion interface.\n\nCompatible with GAIA's model pool and \"\"\"\n    def __init__(self, model_name: str = None, api_key: str = None, timeout: int = None): ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int = 1024, temperature: float = 0.7, top_p: float = 0.95, stream: bool = False, **kwargs) -> Dict[str, Any] | Generator[Dict[str, Any], None, None]: ...\n    def _sanitize_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, str]]: ...\n    def _stream_response(self, response_stream, start_duration: float) -> Generator[Dict[str, Any], None, None]: ...\n    def _log_usage(self, response, duration: float): ...\n    def get_stats(self) -> Dict[str, Any]: ...\n    def list_models(cls) -> Dict[str, Dict]: ...\n\ndef _ensure_groq_imported(): ...\n\n## FILE: gaia-core/models/hf_model.py\n# AST Summary (substitute with actual source at training time)\nclass HFModel():\n    \"\"\"A thin wrapper around Hugging Face transformers for text generation.\n\nProvides a create_completion(p\"\"\"\n    def __init__(self, model_ref: str, local_path: str = None, device_map: Optional[str] = 'auto', torch_dtype = None, prompt_config: Optional[Dict] = None): ...\n    def _messages_to_prompt(self, messages: List[Dict[str, Any]]) -> str: ...\n    def create_completion(self, prompt: str, max_tokens: int = 128, temperature: float = 0.2, **kwargs): ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int = 128, temperature: float = 0.2, **kwargs): ...\n\n\n## FILE: gaia-core/models/mcp_proxy_model.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"MCP-backed model adapter.\n\nImplements a minimal model-like interface expected by ModelPool consumers.\nDelegates requests to the configured MCP-Lite server via JSON-RPC.\"\"\"\nclass MCPProxyModel():\n    def __init__(self, config = None, role_name: str = 'prime'): ...\n    def _call_rpc(self, method: str, params: Dict) -> Dict: ...\n    def create_chat_completion(self, messages: List[Dict], **kwargs) -> Dict: ...\n    def create_completion(self, prompt: str, **kwargs) -> Dict: ...\n    def __repr__(self): ...\n\n\n## FILE: gaia-core/models/model_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"ModelManager: a small spine to query model status, ensure models are loaded\nand provide a safe spawn-based fallback for loading Prime (vLLM) when direct\nin-process load fails due to CUDA/multiprocessi\"\"\"\nclass ModelManager():\n    \"\"\"Singleton-ish manager around the existing model_pool.\n\nIt does not itself host models; instead it de\"\"\"\n    def __new__(cls): ...\n    def __init__(self): ...\n    def _get_pool(self): ...\n    def ensure_prime_loaded(self, force: bool = False, timeout: int = 120) -> Dict[str, Any]: ...\n    def call_model(self, role: str, *args, **kwargs) -> Dict[str, Any]: ...\n\ndef _model_manager_child_loader(q, force_flag): ...\ndef get_manager() -> ModelManager: ...\n\n## FILE: gaia-core/models/model_pool.py\n# AST Summary (substitute with actual source at training time)\ndef get_model_pool(): ...\n\n## FILE: gaia-core/models/oracle_model.py\n# AST Summary (substitute with actual source at training time)\nclass GPTAPIModel():\n    def __init__(self, model_alias: str = 'oracle_openai', config: Config = None): ...\n    def create_chat_completion(self, messages, max_tokens, temperature, top_p, stream = False): ...\n    def _stream_response(self, response_stream): ...\n    def _log_token_usage(self, response): ...\n\n\n## FILE: gaia-core/models/tts.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Text-to-speech module for GAIA D&D Campaign Assistant.\nHandles all speech synthesis functionality.\"\"\"\nclass SpeechManager():\n    \"\"\"Manages text-to-speech functionality.\"\"\"\n    def __init__(self, config): ...\n    def initialize(self) -> bool: ...\n    def _select_voice(self, voices: List) -> None: ...\n    def speak(self, text: str) -> None: ...\n    def stop(self) -> None: ...\n    def set_properties(self, rate: Optional[int] = None, volume: Optional[float] = None) -> None: ...\n\n\n## FILE: gaia-core/models/vector_store.py\n# AST Summary (substitute with actual source at training time)\nclass VectorStoreManager():\n    def __init__(self, config): ...\n    def initialize_store(self): ...\n    def persist(self): ...\n    def delete_all_documents(self): ...\n    def as_retriever(self): ...\n    def add_documents(self, documents: List[Document]): ...\n    def split_and_embed_documents(self, raw_documents: List[str], source: Optional[str] = None): ...\n\n\n## FILE: gaia-core/models/vllm_model.py\n# AST Summary (substitute with actual source at training time)\nclass LoRAAdapterInfo():\n    \"\"\"Metadata for a loaded LoRA adapter.\"\"\"\n\nclass VLLMChatModel():\n    \"\"\"Thin wrapper around vLLM that exposes GAIA's create_chat_completion interface.\n\nParameters are sourc\"\"\"\n    def __init__(self, model_config: Dict[str, Any], global_config, gpu_info: Optional[Tuple[Optional[int], Optional[int]]] = None): ...\n    def _int_from_config(self, key: str, env_override: Optional[str], default: int) -> int: ...\n    def _resolve_gpu_utilization(self) -> float: ...\n    def create_completion(self, prompt: str, max_tokens: int = 128, temperature: float = 0.2, top_p: float = 0.9, presence_penalty: float = 0.0, stream: bool = False, stop: Optional[Iterable[str]] = None, **kwargs): ...\n    def _create_chat_completion_simple(self, messages: List[Dict[str, Any]], max_tokens: int = 2048, temperature: float = 0.7, top_p: float = 0.95, **kwargs): ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int = 128, temperature: float = 0.2, top_p: float = 0.9, stream: bool = False, stop: Optional[Iterable[str]] = None, **kwargs): ...\n    def _messages_to_prompt(self, messages: List[Dict[str, Any]]) -> str: ...\n    def _build_sampling_params(self, max_tokens: int, temperature: float, top_p: float, stop: Optional[Iterable[str]], presence_penalty: float = 0.0, repetition_penalty: float = 1.0, frequency_penalty: float = 0.0) -> SamplingParams: ...\n    def shutdown(self) -> None: ...\n    def _stream_text(self, prompts: List[str], sampling_params: SamplingParams) -> Generator[Dict[str, Any], None, None]: ...\n    def _native_stream_text(self, prompts: List[str], sampling_params: SamplingParams) -> Iterator[Dict[str, Any]]: ...\n    def _chunk_text(self, text: str, chunk_size: int = 96) -> Iterator[str]: ...\n    def _extract_text(self, outputs) -> str: ...\n    def _summarize_outputs(self, outputs) -> str: ...\n    def lora_enabled(self) -> bool: ...\n    def load_adapter(self, name: str, path: str, tier: int = 3) -> bool: ...\n    def unload_adapter(self, name: str) -> bool: ...\n    def get_loaded_adapters(self) -> List[LoRAAdapterInfo]: ...\n    def get_adapter(self, name: str) -> Optional[LoRAAdapterInfo]: ...\n    def create_lora_request(self, adapter_name: str) -> Optional[Any]: ...\n    def generate_with_adapter(self, prompts: List[str], adapter_name: str, sampling_params: Optional[Any] = None, max_tokens: int = 2048, temperature: float = 0.7, top_p: float = 0.95, **kwargs) -> List[Any]: ...\n    def create_chat_completion_with_adapter(self, messages: List[Dict[str, Any]], adapter_name: str, max_tokens: int = 2048, temperature: float = 0.7, top_p: float = 0.95, **kwargs) -> Dict[str, Any]: ...\n\n\n## FILE: gaia-core/models/vllm_remote_model.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Remote vLLM model backend for GAIA.\n\nHTTP client for a standalone vLLM OpenAI-compatible API server (gaia-prime).\nReplaces in-process VLLMChatModel when PRIME_ENDPOINT is set, allowing\ngaia-core to of\"\"\"\nclass VLLMRemoteModel():\n    \"\"\"HTTP client for a remote vLLM OpenAI-compatible API server.\n\nProvides the same public interface as V\"\"\"\n    def __init__(self, model_config: dict, global_config = None, **kwargs): ...\n    def create_completion(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.95, stop: Optional[List[str]] = None, stream: bool = False, **kwargs) -> Dict[str, Any] | Generator[Dict[str, Any], None, None]: ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int = 1024, temperature: float = 0.7, top_p: float = 0.95, stream: bool = False, **kwargs) -> Dict[str, Any] | Generator[Dict[str, Any], None, None]: ...\n    def set_active_adapter(self, adapter_name: Optional[str]): ...\n    def create_chat_completion_with_adapter(self, adapter_name: str, messages: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]: ...\n    def health_check(self) -> bool: ...\n    def shutdown(self): ...\n    def _resolve_model_field(self) -> str: ...\n    def _post(self, path: str, payload: dict) -> dict: ...\n    def _stream_completions(self, payload: dict) -> Generator[Dict[str, Any], None, None]: ...\n    def _stream_chat(self, payload: dict) -> Generator[Dict[str, Any], None, None]: ...\n    def _sanitize_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, str]]: ...\n    def _log_usage(self, resp: dict, duration: float): ...\n\n\n## FILE: gaia-core/utils/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.utils - Utility modules for the GAIA cognitive engine.\n\nThis package provides:\n- prompt_builder: Construct prompts from CognitionPackets\n- packet_builder: Build and manipulate CognitionPacke\"\"\"\n\n## FILE: gaia-core/utils/dev_matrix_analyzer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"DevMatrixAnalyzer - Analyzes task completion status for GAIA's dev_matrix.\n\nThis module provides automated task completion detection for GAIA's self-development\nroadmap. Each task type has specific ve\"\"\"\nclass DevMatrixAnalyzer():\n    \"\"\"Analyzes and updates dev_matrix task completion status.\n\nUses file-based verification instead of she\"\"\"\n    def __init__(self, config): ...\n    def analyze_and_update(self) -> List[Dict]: ...\n    def is_task_completed(self, task: Dict) -> bool: ...\n    def get_task_status_report(self) -> Dict: ...\n    def _verify_discord_integration(self) -> bool: ...\n    def _verify_thought_seed_tooling(self) -> bool: ...\n    def _verify_self_reflection(self) -> bool: ...\n    def _verify_gcp_fragmentation(self) -> bool: ...\n\ndef analyze_dev_matrix(config) -> Dict: ...\n\n## FILE: gaia-core/utils/dev_matrix_utils.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"dev_matrix_utils.py\n- Utilities for loading, diffing, and updating dev_matrix.json\n- Enforces absolute path, atomic writes, and audit logging\n- Designed for use in self-review and approval flows\"\"\"\ndef load_dev_matrix(path: Path = DEV_MATRIX_PATH) -> Any: ...\ndef save_dev_matrix(data: Any, path: Path = DEV_MATRIX_PATH) -> None: ...\ndef diff_dev_matrix(old: Any, new: Any) -> str: ...\ndef mark_task_complete(task_key: str, prompt: str = None, path: Path = DEV_MATRIX_PATH) -> Tuple[Any, Any, str]: ...\n\n## FILE: gaia-core/utils/gaia_rescue_helper.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA Rescue Helper (production copy)\n------------------------------------\n- Central, Config-safe utilities used by the router and rescue shell\n- Provides GAIARescueHelper class (expected by output_rou\"\"\"\nclass GAIARescueHelper():\n    \"\"\"Config-aware faÃ§ade for:\n  - Blueprints / Cheatsheets under knowledge/system_reference/...\n  - Sketc\"\"\"\n    def __init__(self, config: Config, llm: Optional[Any] = None): ...\n    def load_blueprint(self, blueprint_id: str) -> str: ...\n    def load_cheatsheet(self, cheatsheet_id: str) -> str: ...\n    def sketchpad_write(self, title: str, content: str) -> str: ...\n    def sketchpad_read(self, key: str = '') -> str: ...\n    def sketchpad_clear(self) -> str: ...\n    def _load_fragments_store(self) -> Dict[str, Any]: ...\n    def _save_fragments_store(self, data: Dict[str, Any]) -> bool: ...\n    def fragment_write(self, parent_request_id: str, sequence: int, content: str, continuation_hint: str = '', is_complete: bool = False, token_count: int = 0) -> Dict[str, Any]: ...\n    def fragment_read(self, parent_request_id: str) -> List[Dict[str, Any]]: ...\n    def fragment_assemble(self, parent_request_id: str, seam_overlap_check: bool = True) -> Dict[str, Any]: ...\n    def fragment_clear(self, parent_request_id: Optional[str] = None) -> str: ...\n    def fragment_list_pending(self) -> List[str]: ...\n    def _load_memory_store(self) -> Dict[str, Any]: ...\n    def _write_memory_store(self, data: Dict[str, Any]) -> None: ...\n    def remember_fact(self, key: str, value: str, note: str = '') -> str: ...\n    def recall_fact(self, key: str = '', limit: int = 5) -> str: ...\n    def get_recent_facts(self, limit: int = 5) -> List[Dict[str, str]]: ...\n    def queue_thought_seed(self, prompt: str, note: str = '', priority: str = 'normal') -> str: ...\n    def run_shell_safe(self, command: str) -> str: ...\n    def buffer_and_execute_shell(self, content: str) -> None: ...\n    def _validate_read_path(self, path: str) -> str: ...\n    def code_read(self, path: str) -> Dict[str, Any]: ...\n    def code_span(self, path: str, start: int, end: int) -> Dict[str, Any]: ...\n    def code_symbol(self, path: str, symbol: str) -> Dict[str, Any]: ...\n    def code_summarize(self, src: Dict[str, Any], max_tokens: int = 256) -> Dict[str, Any]: ...\n\ndef _safe_read_text(path: Path) -> str: ...\ndef _find_first_with_ext(base: Path, stem: str, exts: tuple[str, ...]) -> Optional[Path]: ...\ndef _helper() -> GAIARescueHelper: ...\ndef sketch(title: str, content: str) -> str: ...\ndef sketchpad_write(title: str, content: str) -> str: ...\ndef show_sketchpad(key: str = '') -> str: ...\ndef sketchpad_read(key: str = '') -> str: ...\ndef clear_sketchpad() -> str: ...\ndef sketchpad_clear() -> str: ...\ndef load_blueprint(blueprint_id: str) -> str: ...\ndef load_cheatsheet(cheatsheet_id: str) -> str: ...\ndef queue_thought_seed(prompt: str, note: str = '', priority: str = 'normal') -> str: ...\ndef run_shell_safe(command: str) -> str: ...\ndef buffer_and_execute_shell(content: str) -> None: ...\ndef code_read(path: str) -> Dict[str, Any]: ...\ndef code_span(path: str, start: int, end: int) -> Dict[str, Any]: ...\ndef code_symbol(path: str, symbol: str) -> Dict[str, Any]: ...\ndef code_summarize(src: Dict[str, Any], max_tokens: int = 256) -> Dict[str, Any]: ...\ndef remember_fact(key: str, value: str, note: str = '') -> str: ...\ndef recall_fact(key: str = '', limit: int = 5) -> str: ...\ndef get_recent_facts(limit: int = 5) -> List[Dict[str, str]]: ...\n\n## FILE: gaia-core/utils/mcp_client.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA MCP-Lite Client\n\nThis module is responsible for dispatching sidecar actions from a CognitionPacket\nto the MCP-lite server.\"\"\"\ndef _normalize_endpoint(ep: str) -> str: ...\ndef call_jsonrpc(method: str, params: Dict, endpoint: str = None, timeout: int = 20) -> Dict: ...\ndef dispatch_sidecar_actions(packet: CognitionPacket, config: Config) -> List[Dict]: ...\ndef ai_read(path: str) -> Dict: ...\ndef ai_write(path: str, content: str) -> Dict: ...\ndef ai_execute(command: str, timeout: int = 30, shell: bool = False, dry_run: bool = False) -> Dict: ...\ndef embedding_query(query: str, top_k: int = 5, knowledge_base_name: str = 'system') -> Dict: ...\ndef request_approval_via_mcp(method: str, params: Dict) -> Dict: ...\ndef approve_action_via_mcp(action_id: str, approval: str) -> Dict: ...\ndef get_pending_action(action_id: str) -> Dict: ...\ndef discover(endpoint: str = None, timeout: int = 3) -> Dict: ...\n\n## FILE: gaia-core/utils/output_router.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Output Router - Central hub for parsing and dispatching all directives from LLM output.\n\nHandles:\n- Parsing LLM output into CognitionPacket structures\n- Safety gate checking before execution\n- Sidecar\"\"\"\ndef _strip_think_tags_robust(text: str) -> str: ...\ndef _strip_stray_cjk(text: str) -> str: ...\ndef _get_destination_registry(): ...\ndef route_output(response_text: str, packet: CognitionPacket, ai_manager, session_id: str, destination: str) -> Dict[str, Any]: ...\ndef _legacy_destination_to_enum(destination: str) -> Optional[OutputDestination]: ...\ndef _strip_gcp_metadata(text: str) -> str: ...\ndef _parse_llm_output_into_packet(response_text: str, packet: CognitionPacket): ...\n\n## FILE: gaia-core/utils/packet_builder.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Utility to build small CognitionPacket snapshots for grounding summarization.\n\nProduces a compact dict snapshot (not a full CognitionPacket instance) which\nmatches the shape expected by `ConversationS\"\"\"\ndef build_packet_snapshot(session_id: str, persona_id: str, original_prompt: str, history: List[Dict[str, Any]] = None, mcp_info: Optional[str] = None) -> Dict[str, Any]: ...\n\n## FILE: gaia-core/utils/packet_templates.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Helpers for rendering GAIA Cognition Packets (GCP) as structured templates.\n\nThe template is intentionally compact: only populated sections are emitted,\nand lengthy text fields are trimmed so the rend\"\"\"\ndef _trim(value: Any, limit: int = MAX_INLINE_LEN) -> Any: ...\ndef _clean_dict(payload: Dict[str, Any]) -> Dict[str, Any]: ...\ndef packet_to_template_dict(packet: CognitionPacket, processed_data_field_keys: Optional[set] = None) -> Dict[str, Any]: ...\ndef render_gaia_packet_template(packet: CognitionPacket, indent: str = '  ', processed_data_field_keys: Optional[set] = None, sections: Optional[tuple] = None) -> str: ...\n\n## FILE: gaia-core/utils/prompt_builder.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Prompt Builder (robust, persona/context-aware)\n- Assembles the LLM prompt with identity, persona, context, constraints, history, and memory.\n- Actively manages the token budget to prevent context over\"\"\"\ndef build_from_packet(packet: CognitionPacket, task_instruction_key: str = None) -> List[Dict]: ...\ndef _build_prompt_core(config, persona_instructions: str, session_id: str, history: List[Dict], user_input: str, task_instruction: str = None, token_budget: int = 4096, packet: 'CognitionPacket' = None) -> List[Dict]: ...\ndef build_prompt(*args, **kwargs): ...\n\n## FILE: gaia-core/utils/resource_monitor.py\n# AST Summary (substitute with actual source at training time)\nclass ResourceMonitor():\n    def __new__(cls, *args, **kwargs): ...\n    def _ensure_nvml(): ...\n    def __init__(self, poll_interval: int = 5): ...\n    def start(self): ...\n    def stop(self): ...\n    def _monitor(self): ...\n    def is_distracted(self) -> bool: ...\n    def check_and_clear_distracted(self) -> bool: ...\n    def get_instance(cls, *args, **kwargs): ...\n\ndef shutdown_monitor(): ...\n\n## FILE: gaia-core/utils/stream_observer.py\n# AST Summary (substitute with actual source at training time)\nclass Interrupt():\n\nclass StreamObserver():\n    def __init__(self, config: Config, llm, name: str = 'AgentCore-Observer'): ...\n    def observe(self, packet: Optional[CognitionPacket], output: str) -> Interrupt: ...\n    def fast_check(self, buffer: str) -> bool: ...\n    def check_response_quality(response: str, user_prompt: str) -> Optional[Interrupt]: ...\n    def _verify_citations_against_rag(self, output: str, packet) -> Dict: ...\n    def _validate_code_paths(self, text_content: str) -> List[Dict]: ...\n    def verify_side_effects(self, packet: Optional[CognitionPacket], route_result: Dict[str, Any], llm_output: str = '') -> Interrupt: ...\n\n\n## FILE: gaia-core/utils/temporal_context.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Temporal Context Builder â€” assembles a rich temporal snapshot for prompt injection.\n\nProduces a concise text block (~100-150 tokens) injected into the system prompt\nso GAIA has awareness of time, her \"\"\"\ndef build_temporal_context(timeline_store: Optional[Any] = None, sleep_manager_status: Optional[Dict[str, Any]] = None, session_id: Optional[str] = None, session_created_at: Optional[datetime] = None, session_message_count: int = 0, last_message_ts: Optional[datetime] = None, code_evolution_path: str = ...) -> str: ...\ndef _semantic_time(dt: datetime) -> str: ...\ndef _format_duration(seconds: float) -> str: ...\ndef _wake_cycle_summary(timeline_store: Any) -> str: ...\ndef _session_summary(session_id: Optional[str], session_created_at: Optional[datetime], message_count: int, last_message_ts: Optional[datetime]) -> str: ...\ndef _activity_summary(timeline_store: Any) -> str: ...\ndef _state_summary(status: Dict[str, Any]) -> str: ...\ndef _code_evolution_summary(path: str) -> str: ...\n\n## FILE: gaia-core/utils/world_state.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Lightweight dynamic world-state snapshot for prompts.\n\nThis module intentionally avoids heavy dependencies. It gathers a short,\nbounded view of:\n- Clock/uptime\n- Host load/memory (coarse)\n- Active mod\"\"\"\ndef _uptime_seconds() -> float: ...\ndef _mem_summary() -> str: ...\ndef _load_avg() -> str: ...\ndef _model_paths() -> Dict[str, str]: ...\ndef _mcp_tools_sample(limit: int = 6) -> List[str]: ...\ndef _mcp_tools_full(limit: int = 50) -> List[str]: ...\ndef world_state_snapshot() -> Dict: ...\ndef world_state_detail() -> Dict: ...\ndef _capability_affordances(tools: List[str]) -> List[str]: ...\ndef format_world_state_snapshot(max_lines: int = 12, output_context: Dict = None) -> str: ...\n", "fidelity": 0.85, "weight": 1.5}
