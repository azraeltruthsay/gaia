{"pair_id": "6a875284-0c9a-4551-9e5f-395b4c75da66", "service_id": "gaia-core", "pair_type": "retroactive", "instruction": "You are GAIA's code-architect. Generate Python source code that faithfully\nimplements the following blueprint. Your output must satisfy all contract,\ndependency, failure mode, and intent specifications exactly.\n\nBLUEPRINT:\narchitecture:\n  components:\n  - consumes_interfaces: []\n    description: 'FastAPI entrypoint and endpoint routers. Receives HTTP requests,\n      deserializes CognitionPackets, and routes to the cognition engine. Includes\n      GPU lifecycle endpoints (release/reclaim) and sleep/wake control endpoints used\n      by gaia-orchestrator and gaia-web respectively.\n\n      '\n    exposes_interfaces:\n    - process_packet\n    - health\n    - root\n    - status\n    - gpu_status\n    - gpu_release\n    - gpu_reclaim\n    - sleep_wake\n    - sleep_status\n    - sleep_study_handoff\n    - sleep_distracted_check\n    - sleep_shutdown\n    id: api_layer\n    key_classes:\n    - AIManagerShim\n    - GPUReleaseRequest\n    - GPUReclaimRequest\n    key_functions:\n    - initialize_cognitive_system()\n    - process_packet(packet_data)\n    - gpu_status()\n    - gpu_release(request)\n    - gpu_reclaim(request)\n    - receive_wake_signal(request)\n    - get_sleep_status()\n    - study_handoff(request)\n    label: API Layer\n    source_files:\n    - candidates/gaia-core/gaia_core/main.py\n    - candidates/gaia-core/gaia_core/api/gpu_endpoints.py\n    - candidates/gaia-core/gaia_core/api/sleep_endpoints.py\n  - consumes_interfaces: []\n    description: 'Core reasoning loop. AgentCore runs multi-turn reasoning with self-improvement.\n      CognitiveDispatcher processes execution results. GoalDetector identifies user\n      intent via fast-path heuristics or LLM classification. ToolSelector decides\n      if/which MCP tools are needed. ExternalVoice generates LLM responses. SelfReflection\n      reviews output quality before delivery.\n\n      '\n    exposes_interfaces: []\n    id: cognition_engine\n    key_classes:\n    - AgentCore\n    - GoalDetector\n    - ExternalVoice\n    key_functions:\n    - AgentCore.run_turn()\n    - AgentCore.run_self_improvement()\n    - process_execution_results()\n    - GoalDetector.detect()\n    - needs_tool_routing()\n    - select_tool()\n    - ExternalVoice.stream_response()\n    - reflect_and_refine()\n    - enhance_packet()\n    label: Cognition Engine\n    source_files:\n    - candidates/gaia-core/gaia_core/cognition/agent_core.py\n    - candidates/gaia-core/gaia_core/cognition/cognitive_dispatcher.py\n    - candidates/gaia-core/gaia_core/cognition/goal_detector.py\n    - candidates/gaia-core/gaia_core/cognition/tool_selector.py\n    - candidates/gaia-core/gaia_core/cognition/external_voice.py\n    - candidates/gaia-core/gaia_core/cognition/self_reflection.py\n    - candidates/gaia-core/gaia_core/cognition/knowledge_enhancer.py\n    - candidates/gaia-core/gaia_core/cognition/knowledge_ingestion.py\n  - consumes_interfaces: []\n    description: \"Manages ACTIVE \\u2192 DROWSY \\u2192 ASLEEP \\u2192 WAKING lifecycle.\\\n      \\ SleepCycleLoop is the main event loop driving state transitions. SleepWakeManager\\\n      \\ is the state machine (GaiaState enum). PrimeCheckpointManager writes LLM-generated\\\n      \\ introspective checkpoints (prime.md) on sleep and marks them consumed on wake.\\\n      \\ SleepTaskScheduler runs background tasks (conversation curation, thought seed\\\n      \\ review, blueprint validation) during sleep windows.\\n\"\n    exposes_interfaces: []\n    id: sleep_system\n    key_classes:\n    - SleepCycleLoop\n    - GaiaState\n    - SleepWakeManager\n    - PrimeCheckpointManager\n    - SleepTaskScheduler\n    - SleepTask\n    key_functions:\n    - SleepCycleLoop.start()\n    - SleepWakeManager.initiate_drowsy()\n    - SleepWakeManager.receive_wake_signal()\n    - SleepWakeManager.complete_wake()\n    - PrimeCheckpointManager.create_checkpoint()\n    - PrimeCheckpointManager.mark_consumed()\n    - SleepTaskScheduler.execute_task()\n    label: Sleep/Wake System\n    source_files:\n    - candidates/gaia-core/gaia_core/cognition/sleep_cycle_loop.py\n    - candidates/gaia-core/gaia_core/cognition/sleep_wake_manager.py\n    - candidates/gaia-core/gaia_core/cognition/prime_checkpoint.py\n    - candidates/gaia-core/gaia_core/cognition/sleep_task_scheduler.py\n  - consumes_interfaces: []\n    description: 'Four-tier memory architecture. SessionManager handles per-conversation\n      history with summarize-and-archive. SemanticCodex provides mid-term cross-session\n      recall via symbol-indexed entries. SessionHistoryIndexer builds per-session\n      vector indices for semantic retrieval within a conversation. MemoryManager is\n      the top-level singleton coordinating short/long-term access. CodexWriter generates\n      structured codex entries from conversations using LLM refinement.\n\n      '\n    exposes_interfaces: []\n    id: memory_system\n    key_classes:\n    - SessionManager\n    - Session\n    - SemanticCodex\n    - CodexEntry\n    - MemoryManager\n    - SessionHistoryIndexer\n    - CodexWriter\n    key_functions:\n    - SessionManager.get_or_create_session()\n    - SessionManager.add_message()\n    - SessionManager.summarize_and_archive()\n    - SemanticCodex.write_entry()\n    - SemanticCodex.search()\n    - MemoryManager.query_long()\n    label: Memory System\n    source_files:\n    - candidates/gaia-core/gaia_core/memory/session_manager.py\n    - candidates/gaia-core/gaia_core/memory/semantic_codex.py\n    - candidates/gaia-core/gaia_core/memory/memory_manager.py\n    - candidates/gaia-core/gaia_core/memory/session_history_indexer.py\n    - candidates/gaia-core/gaia_core/memory/codex_writer.py\n  - consumes_interfaces:\n    - prime_inference\n    - prime_health\n    description: 'Unified inference abstraction over multiple LLM backends. ModelPool\n      provides role-based model selection (primary, fallback, lite, oracle). VLLMRemoteModel\n      calls gaia-prime''s OpenAI-compatible API. GroqAPIModel provides free-tier cloud\n      fallback. GPTAPIModel and GeminiAPIModel are oracle backends for high-quality\n      reasoning. All backends implement create_chat_completion() for uniform access.\n      Supports LoRA adapter hot-loading and health-checked failover.\n\n      '\n    exposes_interfaces: []\n    id: model_pool\n    key_classes:\n    - ModelPool\n    - ModelManager\n    - VLLMRemoteModel\n    - GroqAPIModel\n    - GPTAPIModel\n    - GeminiAPIModel\n    key_functions:\n    - ModelPool.get_model_for_role()\n    - ModelPool.forward_to_model()\n    - ModelPool.load_prime_only()\n    - VLLMRemoteModel.create_chat_completion()\n    - VLLMRemoteModel.health_check()\n    label: Model Pool\n    source_files:\n    - candidates/gaia-core/gaia_core/models/_model_pool_impl.py\n    - candidates/gaia-core/gaia_core/models/model_manager.py\n    - candidates/gaia-core/gaia_core/models/vllm_remote_model.py\n    - candidates/gaia-core/gaia_core/models/groq_model.py\n    - candidates/gaia-core/gaia_core/models/oracle_model.py\n    - candidates/gaia-core/gaia_core/models/gemini_model.py\n  - consumes_interfaces:\n    - mcp_dispatch\n    - mcp_approval\n    description: 'JSON-RPC client for communicating with gaia-mcp. Provides typed\n      wrappers for all MCP tool methods: file I/O, shell execution, vector queries,\n      knowledge management, and fragment assembly. Includes approval workflow integration\n      for sensitive operations.\n\n      '\n    exposes_interfaces: []\n    id: tool_dispatch\n    key_classes: []\n    key_functions:\n    - call_jsonrpc()\n    - dispatch_sidecar_actions()\n    - ai_read()\n    - ai_write()\n    - ai_execute()\n    - embedding_query()\n    - request_approval_via_mcp()\n    - discover()\n    label: Tool Dispatch (MCP Client)\n    source_files:\n    - candidates/gaia-core/gaia_core/utils/mcp_client.py\n  - consumes_interfaces: []\n    description: \"Builds the final LLM prompt from conversation history, knowledge\\\n      \\ context, persona config, sleep checkpoint injection, and system instructions.\\\n      \\ OutputRouter post-processes LLM output \\u2014 strips think tags, CJK artifacts,\\\n      \\ and GCP metadata, then parses structured fields back into the CognitionPacket.\\n\"\n    exposes_interfaces: []\n    id: prompt_assembly\n    key_classes: []\n    key_functions:\n    - build_from_packet()\n    - build_prompt()\n    - route_output()\n    - _parse_llm_output_into_packet()\n    label: Prompt Assembly & Output Routing\n    source_files:\n    - candidates/gaia-core/gaia_core/utils/prompt_builder.py\n    - candidates/gaia-core/gaia_core/utils/output_router.py\n  - consumes_interfaces: []\n    description: \"Detects and recovers from cognitive loops \\u2014 repeated tool calls,\\\n      \\ similar outputs, state oscillation, error cycles, and token pattern repetition.\\\n      \\ LoopDetector aggregates 5 specialized detectors. LoopRecoveryManager captures\\\n      \\ state and injects recovery context. StreamObserver monitors output quality\\\n      \\ in real-time. ResourceMonitor tracks system resource pressure for distracted-state\\\n      \\ detection.\\n\"\n    exposes_interfaces: []\n    id: resilience\n    key_classes:\n    - LoopDetector\n    - LoopDetectionAggregator\n    - LoopRecoveryManager\n    - StreamObserver\n    - ResourceMonitor\n    key_functions:\n    - LoopDetector.check()\n    - LoopDetector.trigger_reset()\n    - LoopRecoveryManager.check_and_handle()\n    - StreamObserver.observe()\n    - ResourceMonitor.is_distracted()\n    label: Resilience & Loop Detection\n    source_files:\n    - candidates/gaia-core/gaia_core/cognition/loop_detector.py\n    - candidates/gaia-core/gaia_core/cognition/loop_recovery.py\n    - candidates/gaia-core/gaia_core/cognition/loop_patterns.py\n    - candidates/gaia-core/gaia_core/utils/stream_observer.py\n    - candidates/gaia-core/gaia_core/utils/resource_monitor.py\n  edges:\n  - data_flow: CognitionPacket\n    from_component: api_layer\n    label: dispatches CognitionPackets to\n    to_component: cognition_engine\n    transport: function_call\n  - data_flow: wake signal, sleep commands\n    from_component: api_layer\n    label: routes sleep/wake/GPU endpoints to\n    to_component: sleep_system\n    transport: function_call\n  - data_flow: \"messages[] \\u2192 completion text\"\n    from_component: cognition_engine\n    label: requests LLM completions via\n    to_component: model_pool\n    transport: function_call\n  - data_flow: JSON-RPC method + params\n    from_component: cognition_engine\n    label: sends tool execution requests to\n    to_component: tool_dispatch\n    transport: function_call\n  - data_flow: session_id, messages, codex entries\n    from_component: cognition_engine\n    label: reads/writes session state and knowledge\n    to_component: memory_system\n    transport: function_call\n  - data_flow: \"context \\u2192 prompt, output \\u2192 parsed packet\"\n    from_component: cognition_engine\n    label: builds prompts and routes output via\n    to_component: prompt_assembly\n    transport: function_call\n  - data_flow: tool calls, outputs, state transitions\n    from_component: cognition_engine\n    label: monitors for loops and quality issues\n    to_component: resilience\n    transport: function_call\n  - data_flow: \"metacognitive prompt \\u2192 checkpoint text\"\n    from_component: sleep_system\n    label: generates LLM checkpoint via lite model\n    to_component: model_pool\n    transport: function_call\n  - data_flow: session summary text\n    from_component: sleep_system\n    label: reads evolving summary for checkpoint context\n    to_component: memory_system\n    transport: function_call\n  - data_flow: \"SleepTask \\u2192 task results\"\n    from_component: sleep_system\n    label: runs sleep tasks (curation, thought seeds)\n    to_component: cognition_engine\n    transport: function_call\n  - data_flow: session history, codex results\n    from_component: prompt_assembly\n    label: retrieves history and knowledge context\n    to_component: memory_system\n    transport: function_call\ndependencies:\n  external_apis:\n  - name: groq\n    purpose: inference_fallback_when_prime_unavailable\n    required: false\n  - name: openai\n    purpose: oracle_tier_inference\n    required: false\n  - name: gemini\n    purpose: oracle_tier_inference\n    required: false\n  services:\n  - fallback: groq-api\n    id: gaia-prime\n    required: false\n    role: inference\n  - fallback: null\n    id: gaia-mcp\n    required: false\n    role: tool_execution\n  - fallback: null\n    id: gaia-web\n    required: true\n    role: output_routing\n  - fallback: null\n    id: gaia-orchestrator\n    required: false\n    role: gpu_lifecycle\n  volumes:\n  - access: rw\n    mount_path: /shared\n    name: gaia-shared\n    purpose: Session state, cognitive checkpoints, prime.md sleep notes\n  - access: ro\n    mount_path: /knowledge\n    name: knowledge\n    purpose: Blueprints, semantic codex, recitable docs\n  - access: rw\n    mount_path: /vector_store\n    name: vector_store\n    purpose: FAISS vector indices for long-term memory\n  - access: ro\n    mount_path: /models\n    name: models\n    purpose: LoRA adapters (json-architect, persona weights)\n  - access: rw\n    mount_path: /logs\n    name: logs\n    purpose: Structured cognition logs, heartbeat telemetry\nfailure_modes:\n- auto_recovers: true\n  condition: gaia-prime unavailable\n  response: Falls back to Groq API; if Groq unavailable, falls back to local GGUF\n    model\n  severity: degraded\n- auto_recovers: true\n  condition: gaia-mcp unavailable\n  response: Tool calls skipped; responds with capability_unavailable in packet\n  severity: degraded\n- auto_recovers: false\n  condition: All inference backends unavailable\n  response: Returns error packet to gaia-web; session preserved for retry\n  severity: partial\n- auto_recovers: true\n  condition: Session state corruption\n  response: Clears session, starts fresh, logs incident to heartbeat\n  severity: degraded\n- auto_recovers: true\n  condition: gaia-orchestrator unreachable\n  response: Sleep/wake GPU handoff skipped; gaia-core retains GPU, study cycle deferred\n  severity: degraded\n- auto_recovers: true\n  condition: Checkpoint write failure\n  response: Sleep proceeds without checkpoint; next wake has no prime.md restoration\n    context\n  severity: degraded\nid: gaia-core\nintent:\n  cognitive_role: The Brain\n  design_decisions:\n  - CPU-only runtime enables GPU handoffs between prime and study without blocking\n    the cognition loop\n  - \"Falls back through groq \\u2192 gguf rather than hard-failing \\u2014 uptime over\\\n    \\ raw capability\"\n  - gaia-web owns output routing so core remains interface-agnostic\n  - 'Four-tier memory: session (short-term), semantic codex (mid-term), vector store\n    (long-term), prime.md checkpoint (sleep continuity)'\n  - Guided decoding via json-architect LoRA adapter for reliable structured output\n    from smaller models\n  - \"Sleep/wake cognitive continuity \\u2014 LLM-generated checkpoint captures introspective\\\n    \\ state, consumed-sentinel prevents stale injection\"\n  - \"Parallel wake strategy \\u2014 GPU reclaim and checkpoint load happen concurrently\\\n    \\ for faster wake\"\n  - Human-in-the-loop approval flow for destructive tool calls via gaia-mcp /request_approval\n  open_questions:\n  - Should reflection loop depth be dynamic based on query complexity or fixed per\n    persona?\n  - \"Semantic codex hot-reload interval is hardcoded \\u2014 should it be configurable\\\n    \\ via gaia_constants.json?\"\n  purpose: 'The cognitive engine of GAIA. Runs the full reasoning loop: intent detection,\n    tool routing, multi-step reflection, and response generation. Deliberately CPU-only\n    to allow gaia-prime and gaia-study to share the GPU without interrupting cognition.\n    All inference is delegated to gaia-prime, with graceful fallback chains preserving\n    uptime across backend failures.\n\n    '\ninterfaces:\n- description: Primary cognition entry point. Receives CognitionPackets from gaia-web,\n    runs the full reasoning loop, returns completed packet.\n  direction: inbound\n  id: process_packet\n  status: active\n  transport:\n    input_schema: CognitionPacket\n    method: POST\n    output_schema: CognitionPacket\n    path: /process_packet\n    type: http_rest\n- description: Container health check endpoint.\n  direction: inbound\n  id: health\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\n- description: \"Root endpoint \\u2014 returns list of available endpoints for service\\\n    \\ discovery.\"\n  direction: inbound\n  id: root\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /\n    type: http_rest\n- description: \"Cognitive status \\u2014 current state, uptime, active sessions.\"\n  direction: inbound\n  id: status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /status\n    type: http_rest\n- description: \"GPU allocation state \\u2014 owned, released, or unavailable.\"\n  direction: inbound\n  id: gpu_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /gpu/status\n    type: http_rest\n- description: Release GPU to orchestrator pool. Called by gaia-orchestrator.\n  direction: inbound\n  id: gpu_release\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/release\n    type: http_rest\n- description: Reclaim GPU from orchestrator pool. Called by gaia-orchestrator.\n  direction: inbound\n  id: gpu_reclaim\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/reclaim\n    type: http_rest\n- description: \"Wake signal from gaia-web \\u2014 triggers transition from ASLEEP to\\\n    \\ WAKING.\"\n  direction: inbound\n  id: sleep_wake\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /sleep/wake\n    type: http_rest\n- description: Query current sleep state (ACTIVE, DROWSY, ASLEEP, WAKING).\n  direction: inbound\n  id: sleep_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /sleep/status\n    type: http_rest\n- description: \"Study handoff endpoint \\u2014 orchestrator signals study cycle complete,\\\n    \\ GPU available.\"\n  direction: inbound\n  id: sleep_study_handoff\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /sleep/study-handoff\n    type: http_rest\n- description: Check if GAIA is asleep and should return a canned/distracted response.\n  direction: inbound\n  id: sleep_distracted_check\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /sleep/distracted-check\n    type: http_rest\n- description: \"Graceful shutdown \\u2014 completes current cycle, writes checkpoint,\\\n    \\ enters sleep.\"\n  direction: inbound\n  id: sleep_shutdown\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /sleep/shutdown\n    type: http_rest\n- description: LLM inference requests to gaia-prime via OpenAI-compatible API.\n  direction: outbound\n  id: prime_inference\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /v1/chat/completions\n    type: http_rest\n- description: \"Health probe to gaia-prime on boot \\u2014 sets prime_available flag.\"\n  direction: outbound\n  id: prime_health\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\n- description: Tool execution requests dispatched to gaia-mcp via JSON-RPC.\n  direction: outbound\n  id: mcp_dispatch\n  status: active\n  transport:\n    methods:\n    - run_shell\n    - write_file\n    - read_file\n    - vector_query\n    - memory_rebuild_index\n    - request_approval\n    target_service: gaia-mcp\n    type: mcp\n- description: Tool approval requests sent to gaia-mcp for human-in-the-loop confirmation.\n  direction: outbound\n  id: mcp_approval\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /request_approval\n    type: http_rest\n- description: \"Notify orchestrator that gaia-core is entering sleep \\u2014 GPU available\\\n    \\ for study.\"\n  direction: outbound\n  id: orchestrator_gpu_sleep\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/sleep\n    type: http_rest\n- description: \"Notify orchestrator that gaia-core is waking \\u2014 request GPU reclamation.\"\n  direction: outbound\n  id: orchestrator_gpu_wake\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /gpu/wake\n    type: http_rest\n- description: \"Presence updates to gaia-web \\u2014 online/typing/sleeping status\\\n    \\ for Discord.\"\n  direction: outbound\n  id: web_presence\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /presence\n    type: http_rest\nmeta:\n  blueprint_version: '0.2'\n  confidence:\n    contract: high\n    dependencies: high\n    failure_modes: medium\n    intent: medium\n    runtime: high\n  created_at: '2026-02-21T15:55:35.767496Z'\n  divergence_score: null\n  generated_by: manual_seed\n  genesis: true\n  last_reflected: null\n  promoted_at: null\n  reflection_notes: null\n  schema_version: '1.0'\n  status: candidate\nrole: The Brain (Cognition)\nruntime:\n  base_image: python:3.11-slim\n  compose_service: gaia-core\n  dockerfile: gaia-core/Dockerfile\n  gpu: false\n  gpu_count: null\n  health_check: curl -f http://localhost:6415/health\n  port: 6415\n  security: null\n  startup_cmd: uvicorn gaia_core.main:app --host 0.0.0.0 --port 6415\n  user: ${UID}:${GID}\nservice_status: live\nsource_files:\n- file_type: python\n  path: candidates/gaia-core/gaia_core/main.py\n  role: entrypoint\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/cognitive_dispatcher.py\n  role: core_logic\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/tool_selector.py\n  role: tool_routing\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/goal_detector.py\n  role: intent_detection\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/sleep_cycle_loop.py\n  role: sleep_lifecycle\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/sleep_wake_manager.py\n  role: sleep_state_machine\n- file_type: python\n  path: candidates/gaia-core/gaia_core/cognition/prime_checkpoint.py\n  role: cognitive_checkpoint\n- file_type: python\n  path: candidates/gaia-core/gaia_core/api/gpu_endpoints.py\n  role: gpu_api\n- file_type: python\n  path: candidates/gaia-core/gaia_core/api/sleep_endpoints.py\n  role: sleep_api\n- file_type: python\n  path: candidates/gaia-core/gaia_core/memory/session_manager.py\n  role: session_management\n- file_type: python\n  path: candidates/gaia-core/gaia_core/memory/semantic_codex.py\n  role: mid_term_memory\n- file_type: python\n  path: candidates/gaia-core/gaia_core/models/vllm_remote_model.py\n  role: inference_client\n- file_type: python\n  path: candidates/gaia-core/gaia_core/utils/mcp_client.py\n  role: tool_dispatch\n- file_type: python\n  path: candidates/gaia-core/gaia_core/utils/prompt_builder.py\n  role: prompt_assembly\n- file_type: json\n  path: gaia-common/gaia_common/constants/gaia_constants.json\n  role: config\nversion: '0.5'\n\n\nAVAILABLE GAIA IDIOMS (from reference implementations):\n\n### gaia-orchestrator\n  config.py: classes=['OrchestratorConfig'], functions=['load_yaml_config', 'get_config', 'reset_config']\n  docker_manager.py: classes=['DockerManager'], functions=[]\n  gpu_manager.py: classes=['GPUManager'], functions=[]\n  handoff_manager.py: classes=['HandoffManager'], functions=[]\n  health_watchdog.py: classes=['HealthWatchdog'], functions=[]\n  main.py: classes=[], functions=['lifespan', 'health_check', 'root', 'get_status', 'get_gpu_status', '_acquire_gpu_inner', 'acquire_gpu', 'release_gpu', 'wait_for_gpu', 'get_container_status', 'stop_live_stack', 'start_live_stack', 'stop_candidate_stack', 'start_candidate_stack', 'swap_service', 'handoff_prime_to_study', 'handoff_study_to_prime', 'get_handoff_status', 'gpu_sleep', 'gpu_wake', 'notify_oracle_fallback', 'websocket_notifications', 'main']\n  models/schemas.py: classes=['GPUAcquireRequest', 'GPUAcquireResponse', 'GPUMemoryInfo', 'GPUStatus', 'ServiceHealth', 'ContainerStatus', 'ContainerStartRequest', 'ContainerSwapRequest', 'HandoffRequest', 'HandoffStatus', 'OracleNotification', 'Notification', 'OrchestratorState'], functions=[]\n  notification_manager.py: classes=['NotificationManager'], functions=[]\n  state.py: classes=['StateManager'], functions=['get_state_manager', 'reset_state_manager']\n\n### gaia-mcp\n  approval.py: classes=['ApprovalStore'], functions=[]\n  main.py: classes=[], functions=['startup_event', 'shutdown_event']\n  server.py: classes=[], functions=['get_study_service', 'health_check', 'create_app', 'dispatch_tool', '_send_discord_message_impl', '_ai_write_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_write_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl', 'jsonrpc_endpoint', 'read_root', 'request_approval', 'pending_approvals', 'approve_action']\n  tools.py: classes=[], functions=['list_tools', 'describe_tool', 'execute_tool', '_ai_write_impl', '_write_file_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_get_study_client', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl']\n  web_tools.py: classes=['SourceTrustConfig', '_RateLimiter'], functions=['_get_config', 'web_search', '_ddg_search', 'web_fetch', '_extract_content', '_extract_title_bs4', '_domain_from_url']\n\nGenerate the implementation for: gaia-core", "output": "## FILE: gaia-core/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia-core: The Brain - Cognitive loop and reasoning engine.\n\nThis service is the heart of GAIA, responsible for:\n- Agent cognitive loop (reason-act-reflect)\n- Model pool orchestration (Prime/Lite/Embe\"\"\"\n\n## FILE: gaia-core/api/gpu_endpoints.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GPU management endpoints for gaia-core.\n\nThese endpoints allow the orchestrator to manage gaia-core's model pool\nwhen GPU ownership changes. The orchestrator handles the actual container\nstop/start fo\"\"\"\nclass GPUReleaseRequest(BaseModel):\n\nclass GPUReclaimRequest(BaseModel):\n\ndef _prime_endpoint() -> str: ...\ndef _get_model_pool(): ...\ndef gpu_status(): ...\ndef gpu_release(request: GPUReleaseRequest = GPUReleaseRequest()): ...\ndef gpu_reclaim(request: GPUReclaimRequest = GPUReclaimRequest()): ...\n\n## FILE: gaia-core/api/sleep_endpoints.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Sleep cycle HTTP endpoints for gaia-core.\n\nFollows the existing pattern from gpu_endpoints.py:\n  - Separate router file with APIRouter(prefix=\"/sleep\")\n  - Registered in main.py via app.include_router\"\"\"\ndef receive_wake_signal(request: Request): ...\ndef voice_state(request: Request): ...\ndef get_sleep_status(request: Request): ...\ndef study_handoff(request: Request): ...\ndef distracted_check(request: Request): ...\ndef shutdown(request: Request): ...\n\n## FILE: gaia-core/behavior/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.behavior - Persona and behavioral adaptation modules.\n\nThis package provides:\n- persona_manager: Load and manage persona definitions\n- persona_adapter: Adapt responses based on active person\"\"\"\n\n## FILE: gaia-core/behavior/persona_adapter.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Persona Adapter (pillar-compliant, robust)\n- Adapts/merges persona config with current pipeline context.\n- Ensures context has correct template, instructions, and allows future persona behaviors.\"\"\"\nclass PersonaAdapter():\n    \"\"\"Adapts/wraps raw persona data into a consistent object for use throughout GAIA.\nEnsures persona attr\"\"\"\n    def __init__(self, persona_data: dict, config = None): ...\n    def get_full_instructions(self) -> str: ...\n    def __repr__(self): ...\n    def __str__(self): ...\n\n\n## FILE: gaia-core/behavior/persona_manager.py\n# AST Summary (substitute with actual source at training time)\nclass PersonaManager():\n    \"\"\"Manages loading and listing of GAIA's personas from disk.\nThis class is a stateless service for retr\"\"\"\n    def __init__(self, personas_dir: str): ...\n    def load_persona_data(self, name: str) -> Optional[Dict]: ...\n    def list_personas(self) -> List[str]: ...\n    def get_persona(self, name: str): ...\n\n\n## FILE: gaia-core/behavior/persona_switcher.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"This module contains the logic for dynamically switching GAIA's persona based on user intent.\"\"\"\ndef _normalize_text(text: str) -> str: ...\ndef _load_persona_config(persona_name: str) -> Optional[dict]: ...\ndef get_knowledge_base_for_persona(persona_name: str) -> Optional[str]: ...\ndef get_persona_for_knowledge_base(kb_name: str) -> Optional[str]: ...\ndef get_persona_for_request(user_input: str) -> Tuple[str, Optional[str]]: ...\n\n## FILE: gaia-core/behavior/persona_writer.py\n# AST Summary (substitute with actual source at training time)\nclass PersonaWriter():\n    \"\"\"Handles creation of persona folders and writing JSON + instruction overlays to disk.\nUsed during int\"\"\"\n    def __init__(self, vectordb_client, personas_dir = '/personas'): ...\n    def create_persona_from_template(self, template: Dict, instructions: Optional[Dict[str, str]] = None) -> bool: ...\n    def _summarize_persona(self, template: Dict) -> str: ...\n    def _embed_to_vectordb(self, summary_text: str, tag: str) -> None: ...\n\n\n## FILE: gaia-core/cognition/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.cognition - Cognitive processing and reasoning modules.\n\nThis package provides:\n- agent_core: Main cognitive loop (AgentCore class)\n- cognitive_dispatcher: Route and dispatch cognitive tasks\"\"\"\n\n## FILE: gaia-core/cognition/adapter_trigger_system.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Adapter Trigger System - Automatic LoRA adapter activation based on content\n\nMonitors user input for keywords/patterns and automatically loads relevant\nadapters to enhance GAIA's knowledge for specifi\"\"\"\nclass TriggerRule():\n    \"\"\"A rule for triggering adapter activation.\"\"\"\n\nclass TriggerMatch():\n    \"\"\"Result of a trigger match.\"\"\"\n\nclass AdapterTriggerSystem():\n    \"\"\"Monitors input and determines which adapters should be activated.\n\nFeatures:\n- Keyword matching (cas\"\"\"\n    def __init__(self, adapter_base_dir: str = '/models/lora_adapters', max_concurrent_adapters: int = 3): ...\n    def set_load_callback(self, callback: Callable[[str, str, int], bool]): ...\n    def set_unload_callback(self, callback: Callable[[str], bool]): ...\n    def load_rules_from_adapters(self) -> int: ...\n    def add_rule(self, rule: TriggerRule): ...\n    def remove_rule(self, adapter_name: str) -> bool: ...\n    def check_triggers(self, text: str) -> List[TriggerMatch]: ...\n    def process_input(self, text: str, auto_load: bool = True) -> Tuple[List[TriggerMatch], List[str], List[str]]: ...\n    def tick_cooldowns(self): ...\n    def deactivate_adapter(self, adapter_name: str) -> bool: ...\n    def get_active_adapters(self) -> List[str]: ...\n    def get_suggested_adapters(self, text: str, top_k: int = 3) -> List[Dict[str, Any]]: ...\n\ndef get_trigger_system(adapter_base_dir: str = '/models/lora_adapters', force_new: bool = False) -> AdapterTriggerSystem: ...\n\n## FILE: gaia-core/cognition/agent_core.py\n# AST Summary (substitute with actual source at training time)\nclass AgentCore():\n    \"\"\"Encapsulates the core \"Reason-Act-Reflect\" loop for GAIA.\nThis class is UI-agnostic and yields struc\"\"\"\n    def __init__(self, ai_manager, ethical_sentinel = None): ...\n    def _emit_timeline_message(self, session_id: str, role: str, source: str = '') -> None: ...\n    def _build_output_routing(self, source: str, destination: str, metadata: dict) -> OutputRouting: ...\n    def _create_initial_packet(self, user_input: str, session_id: str, history: List[Dict[str, Any]], selected_model_name: str, source: str = 'cli', destination: str = 'cli_chat', metadata: dict = None) -> CognitionPacket: ...\n    def _run_pre_generation_safety_check(self, packet: CognitionPacket, assembled_prompt: str) -> (bool, str): ...\n    def run_turn(self, user_input: str, session_id: str, destination: str = 'cli_chat', source: str = 'cli', metadata: dict = None) -> Generator[Dict[str, Any], None, None]: ...\n    def _knowledge_acquisition_workflow(self, packet: CognitionPacket) -> CognitionPacket: ...\n    def _suppress_repetition(self, text: str, max_repeat: int = 2) -> str: ...\n    def _dedup_block(text: str, min_block: int = 120, similarity_threshold: float = 0.85) -> str: ...\n    def _build_response_header(self, model_name: str, packet, observer_instance, active_stream_observer, post_run_observer) -> str: ...\n    def _should_escalate_to_thinker(self, text: str) -> bool: ...\n    def _should_use_slim_prompt(self, plan: Plan, user_input: str) -> bool: ...\n    def _run_slim_prompt(self, selected_model_name: str, user_input: str, history: List[Dict[str, Any]], intent: str = '', session_id: str = '', source: str = 'cli', metadata: dict = None, packet: CognitionPacket = None) -> str: ...\n    def _build_recitation_search_query(user_input: str) -> str: ...\n    def _validate_recitation_content(self, content: str, user_input: str) -> bool: ...\n    def _web_retrieve_for_recitation(self, user_input: str, session_id: str) -> Optional[Dict[str, str]]: ...\n    def _run_with_document_recitation(self, user_input: str, document: Dict[str, str], selected_model_name: str, history: List[Dict[str, Any]], session_id: str = '', output_as_file: bool = False) -> str: ...\n    def _run_with_fragmentation(self, user_input: str, selected_model_name: str, history: List[Dict[str, Any]], session_id: str = '', max_fragments: int = 5, output_as_file: bool = False, output_filename: Optional[str] = None) -> str: ...\n    def _read_fragments_from_sketchpad(self, fragment_keys: List[str], memory_fallback: Optional[Dict[str, str]] = None) -> str: ...\n    def _run_assembly_turn(self, original_request: str, fragment_keys: List[str], selected_model_name: str, session_id: str = '', memory_fallback: Optional[Dict[str, str]] = None) -> str: ...\n    def _write_assembled_to_file(self, content: str, original_request: str, request_id: str, filename: Optional[str] = None) -> str: ...\n    def _assemble_fragments(self, fragments: List[str]) -> str: ...\n    def _run_mcp_list_tree(self) -> str: ...\n    def _run_mcp_list_files(self) -> str: ...\n    def detect_truncation(self, response: str, max_tokens: int = 1000) -> Dict[str, Any]: ...\n    def build_continuation_prompt(self, original_request: str, previous_content: str, continuation_hint: str = '') -> str: ...\n    def assess_task_confidence(self, intent: str, user_input: str, model_name: str = 'lite', session_id: str = '') -> Dict[str, Any]: ...\n    def reflect_on_truncation(self, original_request: str, truncated_output: str, model_name: str = 'lite') -> Dict[str, Any]: ...\n    def _check_topic_alignment(self, original_request: str, continuation_prompt: str) -> tuple: ...\n    def _build_grounded_continuation(self, original_request: str, truncated_output: str) -> str: ...\n    def _find_relevant_files(self, topic: str, max_files: int = 10) -> List[Dict[str, Any]]: ...\n    def _analyze_code_for_topic(self, topic: str, file_paths: List[str], task_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]: ...\n    def _propose_code_fix(self, file_path: str, issue_description: str, suggestion: Optional[str] = None) -> Dict[str, Any]: ...\n    def _apply_code_fix(self, file_path: str, new_content: str, reason: str, run_syntax_check: bool = True) -> Dict[str, Any]: ...\n    def _update_dev_matrix_task(self, task_context: Dict[str, Any], topic: str, fixes_applied: int, files_modified: List[str], analysis_summary: str) -> Dict[str, Any]: ...\n    def run_self_improvement(self, topic: str, auto_apply: bool = False, max_files: int = 5) -> Generator[Dict[str, Any], None, None]: ...\n    def _run_tool_routing_loop(self, packet: CognitionPacket, user_input: str, session_id: str = '', source: str = 'cli', metadata: dict = None) -> CognitionPacket: ...\n    def _execute_mcp_tool(self, tool: SelectedTool) -> ToolExecutionResult: ...\n    def _should_use_tool_routing(self, plan: Plan, user_input: str) -> bool: ...\n\ndef _format_retrieved_session_context(results: dict) -> str: ...\ndef find_recitable_document(user_input: str) -> Optional[Dict[str, str]]: ...\n\n## FILE: gaia-core/cognition/cognition_packet_v0.2_backup.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"CognitionPacket – dynamic state for GAIA’s self-reflection loop.\n\nSchema:\n  prompt:         str            # original user prompt\n  persona:        str            # active persona ID\n  identity:      \"\"\"\nclass CognitionPacket():\n    def __init__(self, session_id: str, packet_id: str, time_date: str, packet_type: str, intent: str, intent_confidence: float, identity: str, persona: str, contextual_instructions: str, prompt: str, history: List[Dict[str, Any]], reflection: str, reflection_confidence: float, execution: str, execution_confidence: float, response: str, response_confidence: float, data_fields: Dict[str, Any], sub_packet_id: str = None, config: Config = None): ...\n    def to_json(self) -> str: ...\n    def from_json(data: str | Dict) -> CognitionPacket: ...\n\ndef create_packet(config: Config, prompt: str, session_id: str, history: List[Dict[str, Any]], persona_instructions: List[str]) -> CognitionPacket: ...\n\n## FILE: gaia-core/cognition/cognitive_audit.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Cognitive Self-Audit — Phase 1 of Reflective Self-Talk\n\nInserts a structured self-assessment between planning and reflection.\nThe model reads its own plan + packet state and writes evaluations,\nsketch\"\"\"\ndef _build_audit_context(packet: CognitionPacket) -> str: ...\ndef _parse_audit_output(text: str, packet: CognitionPacket) -> None: ...\ndef run_cognitive_self_audit(packet: CognitionPacket, plan_text: str, config, llm) -> None: ...\n\n## FILE: gaia-core/cognition/cognitive_dispatcher.py\n# AST Summary (substitute with actual source at training time)\ndef process_execution_results(execution_results, session_manager, session_id, packet: CognitionPacket): ...\n\n## FILE: gaia-core/cognition/conversation_curator.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Auto-append notable Discord conversations to the knowledge examples file.\n\nHooks into SessionManager.summarize_and_archive() to evaluate each archived\nconversation for \"notability\" using simple heuris\"\"\"\nclass ConversationCurator():\n    \"\"\"Evaluates archived conversations and appends notable ones to the examples file.\"\"\"\n    def __init__(self, output_dir: Optional[str] = None): ...\n    def curate(self, session_id: str, messages: List[Dict]) -> bool: ...\n    def is_notable(self, messages: List[Dict]) -> bool: ...\n    def _detect_channel_type(self, session_id: str) -> str: ...\n    def _format_conversation(self, session_id: str, messages: List[Dict]) -> str: ...\n    def _append_to_file(self, formatted: str) -> None: ...\n    def _trim_oldest(self, needed_bytes: int) -> None: ...\n\n\n## FILE: gaia-core/cognition/external_voice.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"external_voice.py — handles all inbound/outbound chat traffic for GAIA\n(streaming, observer hooks, basic logging).  This module is the *sole*\nentry and exit for chat-based interactions.\"\"\"\nclass ExternalVoice():\n    def __init__(self, model, model_pool, config: Config, thought: Optional[str] = None, messages: Optional[List[Dict]] = None, context: Optional[Dict] = None, session_id: str = 'shell', source: str = 'web', observer: Optional[StreamObserver] = None) -> None: ...\n    def stream_response(self, user_input: Optional[str] = None) -> Generator[str | Dict[str, Any], None, None]: ...\n    def generate_full_response(self, user_input: Optional[str] = None) -> str: ...\n    def from_thought(cls, model, thought: str, **kw): ...\n    def from_messages(cls, model, messages: List[Dict], **kw): ...\n    def one_shot(cls, model, prompt: str, **kw) -> str: ...\n    def _apply_stream_spacing(self, token: str, prev_char: str) -> str: ...\n    def _get_first_visible_char(text: str) -> str: ...\n    def _get_last_visible_char(text: str) -> str: ...\n\ndef suppress_llama_stderr() -> Generator[None, None, None]: ...\ndef extract_and_format_execute_blocks(response_text: str) -> str: ...\n\n## FILE: gaia-core/cognition/goal_detector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Goal Detection Module — detects and carries user goals across conversation turns.\n\nThree detection paths:\n  1. Fast-path — self-evident intents map directly to a goal\n  2. Session-carry — active goal \"\"\"\nclass GoalDetector():\n    \"\"\"Detects and manages user goals across conversation turns.\"\"\"\n    def __init__(self, config = None): ...\n    def detect(self, packet: CognitionPacket, session_manager, session_id: str, model_pool = None) -> GoalState: ...\n    def _fast_path_detect(self, intent: str, user_input: str) -> Optional[DetectedGoal]: ...\n    def _session_carry(self, session_manager, session_id: str) -> Optional[GoalState]: ...\n    def _llm_detect(self, packet: CognitionPacket, model_pool) -> Optional[DetectedGoal]: ...\n    def _parse_llm_response(text: str) -> Optional[DetectedGoal]: ...\n    def handle_goal_shift(new_goal_desc: str, packet: CognitionPacket, session_manager, session_id: str): ...\n    def _persist_goal(self, session_manager, session_id: str, state: GoalState): ...\n    def _persist_goal_static(session_manager, session_id: str, state: GoalState): ...\n\n\n## FILE: gaia-core/cognition/heartbeat.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Thought Seed Heartbeat — regular-interval daemon that triages dormant seeds.\n\nRuns independently of the sleep cycle on a configurable timer (default 20 min).\nFor each unreviewed seed, Lite performs a \"\"\"\nclass ThoughtSeedHeartbeat():\n    \"\"\"Daemon thread that triages thought seeds on a regular interval.\"\"\"\n    def __init__(self, config, model_pool = None, agent_core = None, sleep_wake_manager = None, timeline_store = None, session_manager = None) -> None: ...\n    def start(self) -> None: ...\n    def stop(self) -> None: ...\n    def _run(self) -> None: ...\n    def _tick(self) -> None: ...\n    def _run_temporal_tasks(self) -> tuple[bool, bool, bool]: ...\n    def _triage_seed(self, llm, seed_data: Dict[str, Any]) -> tuple[str, str]: ...\n    def _do_archive(self, filename: str) -> None: ...\n    def _do_defer(self, filename: str) -> None: ...\n    def _act_on_seed(self, llm, seed_filename: str, seed_data: Dict[str, Any]) -> None: ...\n    def _expand_seed(self, llm, seed_data: Dict[str, Any]) -> str: ...\n    def _ensure_active(self, seed_filename: str) -> bool: ...\n    def _emit_heartbeat_tick(self, seeds_found: int, archived: int, deferred: int, acted: int, journal_written: bool = False, state_baked: bool = False, interview_conducted: bool = False) -> None: ...\n\n\n## FILE: gaia-core/cognition/history_review.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"History Review — Pre-injection audit of conversation history.\n\nBefore history is injected into the LLM prompt, each assistant message\nis checked for epistemic violations:\n  - Fabricated file paths (ci\"\"\"\ndef _count_violations(text: str) -> Tuple[int, List[str]]: ...\ndef _is_user_correction(text: str) -> bool: ...\ndef _redact_message(original: str, reasons: List[str]) -> str: ...\ndef _build_correction_summary(user_msg: str, assistant_msg: str, reasons: List[str]) -> Optional[str]: ...\ndef review_history(history: List[Dict[str, str]], config: Optional[dict] = None, session_id: str = '') -> List[Dict[str, str]]: ...\n\n## FILE: gaia-core/cognition/initiative_engine.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Initiative Engine — ported from archive/gaia-assistant-monolith/run_gil.py.\n\nExecutes a single autonomous thought cycle: picks the highest-priority topic\nfrom the topic cache and feeds a self-generate\"\"\"\nclass InitiativeEngine():\n    \"\"\"Autonomous thought engine driven by the topic manager.\"\"\"\n    def __init__(self, config, agent_core = None) -> None: ...\n    def execute_turn(self) -> Optional[Dict[str, Any]]: ...\n    def _build_self_prompt(topic: Dict[str, Any]) -> str: ...\n\n\n## FILE: gaia-core/cognition/knowledge_enhancer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"This module is responsible for enhancing the CognitionPacket with relevant knowledge from knowledge bases.\"\"\"\ndef enhance_packet(packet: CognitionPacket): ...\n\n## FILE: gaia-core/cognition/knowledge_ingestion.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Knowledge Ingestion Pipeline for D&D Campaign Content\n\nDetects incoming knowledge dumps (explicit save commands or heuristic auto-detect),\nclassifies content, checks for duplicates, formats as structu\"\"\"\ndef detect_save_command(user_input: str) -> Optional[Dict[str, str]]: ...\ndef detect_knowledge_dump(user_input: str, kb_name: str) -> bool: ...\ndef classify_content(text: str) -> Dict[str, str]: ...\ndef check_dedup(content: str, kb_name: str) -> Optional[Dict]: ...\ndef _sanitize_filename(text: str) -> str: ...\ndef format_document(content: str, classification: Dict[str, str], subject: str = '') -> Tuple[str, str]: ...\ndef write_and_embed(filename: str, doc_content: str, kb_name: str) -> Dict[str, object]: ...\ndef run_explicit_save(user_input: str, kb_name: str) -> Optional[Dict]: ...\ndef run_auto_detect(user_input: str, kb_name: str) -> Optional[Dict]: ...\ndef detect_knowledge_update(user_input: str, kb_name: str) -> Optional[Dict[str, str]]: ...\ndef retrieve_entity_document(entity: str, kb_name: str) -> Optional[Dict]: ...\ndef run_update_detect(user_input: str, kb_name: str) -> Optional[Dict]: ...\n\n## FILE: gaia-core/cognition/lite_journal.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Lite Cognitive Journal — running introspective log written by the Lite model.\n\nMirrors the PrimeCheckpointManager pattern: regular writes, timestamped entries,\nrotation to history directory when the j\"\"\"\nclass LiteJournal():\n    \"\"\"Manages Lite's introspective journal (Lite.md).\"\"\"\n    def __init__(self, config, model_pool = None, timeline_store = None, sleep_wake_manager = None) -> None: ...\n    def write_entry(self) -> Optional[str]: ...\n    def load_latest(self) -> str: ...\n    def load_recent_entries(self, n: int = 5) -> List[str]: ...\n    def rotate(self) -> None: ...\n    def get_entry_count(self) -> int: ...\n    def _generate_entry(self, llm) -> str: ...\n    def _build_journal_prompt(self) -> tuple[str, str]: ...\n    def _append_entry(self, entry_text: str) -> None: ...\n    def _format_duration(seconds: float) -> str: ...\n    def _summarize_event(data: Dict[str, Any]) -> str: ...\n\n\n## FILE: gaia-core/cognition/loop_detector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Loop Detection System for GAIA Cognitive Pipeline.\n\nDetects when the model enters generation loops and provides signals for\ngraceful recovery. Uses multiple parallel detectors that vote on loop presen\"\"\"\nclass LoopDetectorConfig():\n    \"\"\"Configuration for loop detection thresholds.\"\"\"\n\nclass DetectionResult():\n    \"\"\"Result from a single detector.\"\"\"\n\nclass AggregatedResult():\n    \"\"\"Combined result from all detectors.\"\"\"\n\nclass ToolCallRecord():\n    \"\"\"Record of a single tool call for tracking.\"\"\"\n\nclass ErrorRecord():\n    \"\"\"Record of an error for tracking.\"\"\"\n\nclass ToolCallRepetitionDetector():\n    \"\"\"Detects repeated tool calls with same/similar arguments.\n\nCatches:\n- Exact repetition: Same tool + a\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def record(self, tool: str, args: Dict[str, Any], result: str = '') -> None: ...\n    def detect(self) -> DetectionResult: ...\n    def _detect_exact_repetition(self) -> DetectionResult: ...\n    def _detect_ping_pong(self) -> DetectionResult: ...\n    def _detect_same_results(self) -> DetectionResult: ...\n    def _hash_args(self, args: Dict[str, Any]) -> str: ...\n    def _summarize_args(self, args: Dict[str, Any]) -> str: ...\n    def reset(self) -> None: ...\n\nclass OutputSimilarityDetector():\n    \"\"\"Detects nearly identical outputs across turns.\n\nUses multi-strategy similarity:\n- Jaccard on word se\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def record(self, output: str) -> None: ...\n    def detect(self) -> DetectionResult: ...\n    def _similarity(self, a: str, b: str) -> float: ...\n    def _ngram_similarity(self, a: str, b: str, n: int = 3) -> float: ...\n    def _structural_similarity(self, a: str, b: str) -> float: ...\n    def _normalize(self, text: str) -> str: ...\n    def reset(self) -> None: ...\n\nclass StateOscillationDetector():\n    \"\"\"Detects oscillating states without progress.\n\nTracks:\n- Goal changes\n- File modification patterns\n- \"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def record(self, goal: str = '', modified_files: Optional[Set[str]] = None, state_snapshot: Optional[Dict[str, Any]] = None) -> None: ...\n    def detect(self) -> DetectionResult: ...\n    def _detect_goal_oscillation(self) -> DetectionResult: ...\n    def reset(self) -> None: ...\n\nclass ErrorCycleDetector():\n    \"\"\"Detects recurring error patterns.\n\nCatches:\n- Same error repeated\n- Same fix attempted multiple time\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def record(self, error_type: str, error_message: str, attempted_fix: str = '', was_success: bool = False) -> None: ...\n    def detect(self) -> DetectionResult: ...\n    def _detect_fix_repetition(self) -> DetectionResult: ...\n    def _detect_whack_a_mole(self) -> DetectionResult: ...\n    def reset(self) -> None: ...\n\nclass TokenPatternDetector():\n    \"\"\"Detects repetitive patterns during token streaming.\n\nCatches:\n- Exact phrase repetition (\"I'll help.\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def add_tokens(self, tokens: str) -> Optional[DetectionResult]: ...\n    def detect(self) -> DetectionResult: ...\n    def _detect_phrase_repetition(self) -> DetectionResult: ...\n    def _detect_word_repetition(self) -> DetectionResult: ...\n    def _detect_structural_repetition(self) -> DetectionResult: ...\n    def reset(self) -> None: ...\n\nclass LoopDetectionAggregator():\n    \"\"\"Combines signals from all detectors to determine if a loop is occurring.\n\nTriggering rules:\n1. Any s\"\"\"\n    def __init__(self, config: LoopDetectorConfig): ...\n    def evaluate(self) -> AggregatedResult: ...\n    def _should_warn(self) -> bool: ...\n    def mark_warned(self) -> None: ...\n    def mark_reset(self) -> None: ...\n    def record_tool_call(self, tool: str, args: Dict[str, Any], result: str = '') -> None: ...\n    def record_output(self, output: str) -> None: ...\n    def record_state(self, goal: str = '', modified_files: Optional[Set[str]] = None, state_snapshot: Optional[Dict[str, Any]] = None) -> None: ...\n    def record_error(self, error_type: str, error_message: str, attempted_fix: str = '', was_success: bool = False) -> None: ...\n    def add_tokens(self, tokens: str) -> Optional[DetectionResult]: ...\n    def reset(self) -> None: ...\n\nclass LoopDetector():\n    \"\"\"Main interface for loop detection in GAIA cognitive pipeline.\n\nUsage:\n    detector = LoopDetector.ge\"\"\"\n    def __init__(self, config: Optional[LoopDetectorConfig] = None): ...\n    def get_instance(cls, config: Optional[LoopDetectorConfig] = None) -> LoopDetector: ...\n    def reset_instance(cls) -> None: ...\n    def enabled(self) -> bool: ...\n    def enabled(self, value: bool) -> None: ...\n    def reset_count(self) -> int: ...\n    def record_tool_call(self, tool: str, args: Dict[str, Any], result: str = '') -> None: ...\n    def record_output(self, output: str) -> None: ...\n    def record_state(self, goal: str = '', modified_files: Optional[Set[str]] = None, state_snapshot: Optional[Dict[str, Any]] = None) -> None: ...\n    def record_error(self, error_type: str, error_message: str, attempted_fix: str = '', was_success: bool = False) -> None: ...\n    def add_tokens(self, tokens: str) -> Optional[DetectionResult]: ...\n    def check(self) -> AggregatedResult: ...\n    def mark_warned(self) -> None: ...\n    def trigger_reset(self) -> None: ...\n    def reset_detectors(self) -> None: ...\n    def full_reset(self) -> None: ...\n\n\n## FILE: gaia-core/cognition/loop_patterns.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Loop Pattern Classification and Description System.\n\nGenerates human-readable descriptions of detected loops for:\n1. Brief - Status line / notifications (~50 chars)\n2. Summary - User-facing display (2\"\"\"\nclass DescriptionTemplate():\n    \"\"\"Template for loop description at different verbosity levels.\"\"\"\n\nclass ClassifiedPattern():\n    \"\"\"A classified loop pattern with description.\"\"\"\n\nclass PatternClassifier():\n    \"\"\"Classifies detected loops into specific pattern types and generates\nhuman-readable descriptions.\"\"\"\n    def classify(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_tool_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_output_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_state_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_error_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_token_pattern(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _classify_generic(self, result: AggregatedResult) -> ClassifiedPattern: ...\n    def _build_full_template(self, title: str, pattern: str, details: List[str], what_went_wrong: str, suggestions: List[str]) -> str: ...\n\nclass PatternRenderer():\n    \"\"\"Renders classified patterns at different verbosity levels.\"\"\"\n    def __init__(self): ...\n    def render(self, result: AggregatedResult, format: str = 'summary', reset_count: int = 0) -> str: ...\n    def _render_model_context(self, classified: ClassifiedPattern, reset_count: int) -> str: ...\n    def _get_urgency(self, reset_count: int) -> str: ...\n    def get_notification(self, result: AggregatedResult, reset_count: int = 0) -> Dict[str, Any]: ...\n    def _get_override_warning(self, reset_count: int) -> Optional[str]: ...\n\n\n## FILE: gaia-core/cognition/loop_recovery.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Loop Recovery System for GAIA Cognitive Pipeline.\n\nOrchestrates the reset flow when a loop is detected:\n1. Capture current packet state\n2. Inject recovery context into next prompt\n3. Manage escalation\"\"\"\nclass LoopMetadata():\n    \"\"\"Metadata about a detected loop, attached to packets for context.\"\"\"\n    def to_dict(self) -> Dict[str, Any]: ...\n    def from_dict(cls, data: Dict[str, Any]) -> LoopMetadata: ...\n\nclass CapturedState():\n    \"\"\"Captured state before reset for context preservation.\"\"\"\n    def to_dict(self) -> Dict[str, Any]: ...\n\nclass LoopRecoveryManager():\n    \"\"\"Manages the loop detection and recovery lifecycle.\n\nResponsibilities:\n- Coordinate detection checks\n\"\"\"\n    def __init__(self, config: Optional[LoopDetectorConfig] = None, on_warn: Optional[Callable[[AggregatedResult], None]] = None, on_block: Optional[Callable[[AggregatedResult], None]] = None, on_escalate: Optional[Callable[[int], None]] = None): ...\n    def enabled(self) -> bool: ...\n    def enabled(self, value: bool) -> None: ...\n    def reset_count(self) -> int: ...\n    def check_and_handle(self, session_id: str = '', packet_id: str = '', goal: str = '', last_output: str = '') -> Optional[AggregatedResult]: ...\n    def _capture_state(self, session_id: str, packet_id: str, goal: str, last_output: str, result: AggregatedResult) -> None: ...\n    def _prepare_recovery_context(self, result: AggregatedResult) -> None: ...\n    def get_recovery_context(self) -> Optional[str]: ...\n    def clear_recovery_context(self) -> None: ...\n    def _mark_recovery_success(self) -> None: ...\n    def override_detection(self, duration_seconds: float = 300) -> None: ...\n    def cancel_override(self) -> None: ...\n    def get_notification(self, result: AggregatedResult) -> Dict[str, Any]: ...\n    def should_require_user_intervention(self) -> bool: ...\n    def get_escalation_message(self) -> str: ...\n    def record_tool_call(self, tool: str, args: Dict[str, Any], result: str = '') -> None: ...\n    def record_output(self, output: str) -> None: ...\n    def record_state(self, goal: str = '', modified_files: Optional[set] = None, state_snapshot: Optional[Dict[str, Any]] = None) -> None: ...\n    def record_error(self, error_type: str, error_message: str, attempted_fix: str = '', was_success: bool = False) -> None: ...\n    def add_tokens(self, tokens: str) -> Optional[Any]: ...\n\nclass LoopInterrupt():\n    \"\"\"Interrupt signal for streaming loop detection.\nCompatible with existing Interrupt class from stream_\"\"\"\n    def from_detection(cls, result: AggregatedResult, is_warn: bool = True) -> LoopInterrupt: ...\n\nclass LoopDetectorObserver():\n    \"\"\"Observer adapter for integration with ExternalVoice streaming.\n\nMonitors token stream for loop patte\"\"\"\n    def __init__(self, manager: Optional[LoopRecoveryManager] = None, think_tag_char_threshold: int = 500, think_tag_ratio_threshold: float = 0.9): ...\n    def _check_think_tag_ratio(self) -> Optional[LoopInterrupt]: ...\n    def on_token(self, token: str) -> Optional[LoopInterrupt]: ...\n    def reset(self) -> None: ...\n\ndef get_recovery_manager() -> LoopRecoveryManager: ...\ndef inject_recovery_context_if_needed(prompt: str) -> str: ...\ndef build_loop_detection_config_from_constants(constants: Dict[str, Any]) -> LoopDetectorConfig: ...\n\n## FILE: gaia-core/cognition/nlu/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.cognition.nlu - Natural Language Understanding modules.\n\nThis package provides:\n- intent_detection: Fast reflex, LLM-powered, and embedding-based intent detection\n- embed_intent_classifier: \"\"\"\n\n## FILE: gaia-core/cognition/nlu/embed_intent_classifier.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Embedding-based intent classifier for GAIA.\n\nReplaces the keyword-heuristic fallback in intent_detection.py with\ncosine-similarity classification against a bank of labeled exemplar\nphrases.  Uses the \"\"\"\nclass EmbedIntentClassifier():\n    \"\"\"Singleton embedding-based intent classifier.\"\"\"\n    def __init__(self): ...\n    def instance(cls) -> 'EmbedIntentClassifier': ...\n    def initialise(self, embed_model, config: Optional[dict] = None) -> bool: ...\n    def classify(self, text: str, confidence_threshold: float = 0.45, top_k: int = 3) -> Tuple[str, float]: ...\n    def ready(self) -> bool: ...\n\n\n## FILE: gaia-core/cognition/nlu/intent_detection.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Intent Detection (pillar-compliant, robust)\n- Fast reflex/regex path for autonomic commands (help, exit, shell, etc).\n- LLM-powered detection for all ambiguous/natural input.\n- Returns simple intent l\"\"\"\nclass Plan():\n\ndef fast_intent_check(text): ...\ndef _detect_direct_list_tools(text: str) -> bool: ...\ndef _detect_tree_request(text: str) -> bool: ...\ndef _detect_list_files_request(text: str) -> bool: ...\ndef _detect_read_file_request(text: str) -> bool: ...\ndef _mentions_file_like_action(text: str) -> bool: ...\ndef _detect_fragmentation_request(text: str) -> bool: ...\ndef _detect_tool_routing_request(text: str) -> bool: ...\ndef _keyword_intent_classify(text: str, probe_context: str = '') -> str: ...\ndef _fast_track_intent_detection(text: str) -> Optional[str]: ...\ndef model_intent_detection(text, config, lite_llm = None, full_llm = None, fallback_llm = None, probe_context = '', embed_model = None): ...\ndef detect_intent(text, config, lite_llm = None, full_llm = None, fallback_llm = None, probe_context = '', embed_model = None) -> Plan: ...\n\n## FILE: gaia-core/cognition/nlu/intent_service.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Public façade for intent detection.\n\nOther code should import ONLY from this file:\n    from gaia_core.cognition.nlu.intent_service import detect_intent\n\nBehind the curtain we forward to the real detec\"\"\"\n\n## FILE: gaia-core/cognition/packet_upgrade.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Non-destructive CognitionPacket upgrader — DEPRECATED.\n\nThis module was part of the v0.2 → v0.3 migration path. The attributes it\nsets (cot, scratch, cheats, proposed_actions, etc.) do not exist on th\"\"\"\ndef _ensure(obj: object, name: str, default): ...\ndef _ensure_slots(dct, keys): ...\ndef upgrade_packet(packet, config) -> object: ...\n\n## FILE: gaia-core/cognition/packet_utils.py\n# AST Summary (substitute with actual source at training time)\ndef is_execution_safe(packet: CognitionPacket) -> bool: ...\ndef upgrade_v2_to_v3_packet(old_packet_data: Dict) -> CognitionPacket: ...\n\n## FILE: gaia-core/cognition/prime_checkpoint.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Prime model cognitive state checkpointing.\n\nManages the prime.md checkpoint file that preserves GAIA's working memory\nacross GPU sleep/wake cycles.  This is the natural-language replacement for\nKV cac\"\"\"\nclass PrimeCheckpointManager():\n    \"\"\"Manages Prime model's cognitive state checkpointing.\"\"\"\n    def __init__(self, config, timeline_store = None): ...\n    def create_checkpoint(self, packet = None, model_pool = None) -> Path: ...\n    def rotate_checkpoints(self) -> None: ...\n    def load_latest(self) -> str: ...\n    def is_consumed(self) -> bool: ...\n    def mark_consumed(self) -> None: ...\n    def get_checkpoint_history(self, limit: int = 10) -> list: ...\n    def _generate_checkpoint(self, packet, model_pool) -> str: ...\n    def _generate_with_llm(self, llm, ctx: dict) -> str: ...\n    def _build_template(self, ctx: dict) -> str: ...\n    def _emit_checkpoint(self, packet, method: str) -> None: ...\n    def _extract_context(self, packet) -> dict: ...\n    def _load_evolving_summary(self, session_id: str) -> str: ...\n    def _truncate(text: str, max_len: int) -> str: ...\n\n\n## FILE: gaia-core/cognition/self_reflection.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Self Reflection Processor (model-powered, robust pipeline)\n- Calls LLM for post-generation analysis and hallucination/error detection.\n- Integrates config-driven safety and can fallback to rule-based \"\"\"\ndef reflect_and_refine(packet: CognitionPacket, output: str, config, llm, ethical_sentinel) -> str: ...\ndef run_self_reflection(packet: CognitionPacket, output: str, config = None, llm = None): ...\n\n## FILE: gaia-core/cognition/self_review_worker.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Self-review worker: reviews thought seeds and proposes dev_matrix updates.\n\nThis worker runs in proposal-only mode: it will create a pending MCP action to\nupdate `knowledge/system_reference/dev_matrix\"\"\"\ndef _get_model_pool(): ...\ndef _load_dev_matrix(): ...\ndef _save_dev_matrix(data): ...\ndef run_review_once(config: Config = None): ...\ndef run_review_with_prompt(prompt: str, task_key: str = 'thought_seed_system', session_id: str = 'rescue-shell', persona_id: str = 'RescueOperator'): ...\n\n## FILE: gaia-core/cognition/semantic_probe.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Semantic Probe — Pre-cognition vector lookup for context discovery.\n\nRuns BEFORE intent detection and persona selection. Extracts interesting\nphrases from user input, probes all indexed vector collect\"\"\"\nclass ProbeHit():\n    \"\"\"A single vector match from the probe.\"\"\"\n    def to_dict(self) -> dict: ...\n\nclass SemanticProbeResult():\n    \"\"\"Aggregated result from probing all collections.\"\"\"\n    def to_dict(self) -> dict: ...\n    def has_hits(self) -> bool: ...\n    def to_metrics_dict(self) -> Dict: ...\n\nclass SessionProbeCache():\n    \"\"\"Per-session cache of phrase → probe hits with turn-based eviction.\"\"\"\n    def get(self, phrase: str) -> Optional[List[ProbeHit]]: ...\n    def put(self, phrase: str, hits: List[ProbeHit]): ...\n    def advance_turn(self): ...\n\nclass ProbeSessionStats():\n    \"\"\"Cumulative probe effectiveness stats for a session.\"\"\"\n    def record(self, result: 'SemanticProbeResult', was_skipped: bool = False): ...\n    def hit_rate(self) -> float: ...\n    def avg_probe_time_ms(self) -> float: ...\n    def to_dict(self) -> Dict: ...\n\ndef _load_probe_config() -> Dict: ...\ndef _get_session_cache(session_id: str) -> SessionProbeCache: ...\ndef _get_session_stats(session_id: str) -> ProbeSessionStats: ...\ndef get_session_probe_stats(session_id: str) -> Optional[Dict]: ...\ndef extract_candidate_phrases(text: str) -> List[str]: ...\ndef _probe_single_collection(phrases: List[str], collection_name: str, top_k: int = 3) -> List[ProbeHit]: ...\ndef _determine_primary_and_supplemental(hits: List[ProbeHit]) -> Tuple[Optional[str], List[str]]: ...\ndef probe_collections(phrases: List[str], knowledge_bases: Dict[str, dict], session_id: str = '', top_k_per_phrase: int = 3) -> SemanticProbeResult: ...\ndef should_skip_probe(user_input: str) -> bool: ...\ndef run_semantic_probe(user_input: str, knowledge_bases: Dict[str, dict], session_id: str = '', top_k_per_phrase: int = ...) -> SemanticProbeResult: ...\n\n## FILE: gaia-core/cognition/sleep_cycle_loop.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Sleep cycle loop — runs as a daemon thread in gaia-core.\n\nUses gaia-common primitives (IdleMonitor) for idle detection but owns\nall sleep/wake orchestration logic.  Replaces the legacy\nBackgroundProce\"\"\"\nclass SleepCycleLoop():\n    \"\"\"Background thread that monitors idle state and drives sleep/wake.\"\"\"\n    def __init__(self, config, discord_connector = None, model_pool = None, agent_core = None, session_manager = None) -> None: ...\n    def start(self) -> None: ...\n    def stop(self) -> None: ...\n    def initiate_shutdown(self) -> None: ...\n    def _run(self) -> None: ...\n    def _handle_active(self, idle_minutes: float) -> None: ...\n    def _handle_asleep(self) -> None: ...\n    def _handle_dreaming(self) -> None: ...\n    def _handle_distracted(self) -> None: ...\n    def _release_gpu_for_sleep(self) -> None: ...\n    def _reclaim_gpu_for_wake(self) -> None: ...\n    def _update_presence(self, status_text: Optional[str], sleeping: bool = False, offline: bool = False, status_override: Optional[str] = None) -> None: ...\n\n\n## FILE: gaia-core/cognition/sleep_task_scheduler.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Sleep Task Scheduler — orchestrates autonomous maintenance during SLEEPING state.\n\nRegistered tasks are executed one-at-a-time in priority order (lowest number = highest\npriority), with least-recently\"\"\"\nclass SleepTask():\n    \"\"\"A single registerable sleep-time task.\"\"\"\n\nclass SleepTaskScheduler():\n    \"\"\"Priority-based scheduler for sleep-time maintenance tasks.\"\"\"\n    def __init__(self, config, model_pool = None, agent_core = None, timeline_store = None) -> None: ...\n    def register_task(self, task: SleepTask) -> None: ...\n    def _register_default_tasks(self) -> None: ...\n    def get_next_task(self) -> Optional[SleepTask]: ...\n    def execute_task(self, task: SleepTask) -> bool: ...\n    def get_status(self) -> List[Dict[str, Any]]: ...\n    def _run_conversation_curation(self) -> None: ...\n    def _run_blueprint_validation(self) -> None: ...\n    def _validate_yaml_blueprints(self) -> int: ...\n    def _validate_legacy_blueprints(self) -> int: ...\n    def _extract_facts(source_files: List[str], source_roots: List[Path]) -> Dict[str, List[str]]: ...\n    def _check_facts(facts: Dict[str, List[str]], bp_text: str) -> List[str]: ...\n    def _append_update_notes(bp_path: Path, bp_text: str, missing: List[str]) -> None: ...\n    def _rebuild_blueprint_embeddings(self) -> None: ...\n    def _run_code_evolution(self) -> None: ...\n    def _emit_task_exec(self, task_id: str, task_type: str, elapsed: float, success: bool, error: str = '') -> None: ...\n\n\n## FILE: gaia-core/cognition/sleep_wake_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA Sleep/Wake State Machine.\n\nManages six public states + two internal transient phases:\n\nPublic states:\n    OFFLINE → ACTIVE → DROWSY → ASLEEP → DREAMING / DISTRACTED\n\nInternal phases (not in the p\"\"\"\nclass SleepWakeManager():\n    \"\"\"Manages GAIA's sleep/wake state transitions with cognitive continuity.\"\"\"\n    def __init__(self, config, model_pool = None, idle_monitor = None, timeline_store = None) -> None: ...\n    def get_state(self) -> GaiaState: ...\n    def should_transition_to_drowsy(self, idle_minutes: float) -> bool: ...\n    def _emit_state_change(self, from_state: str, to_state: str, reason: str = '') -> None: ...\n    def _notify_audio_state(self, to_state: str) -> None: ...\n    def initiate_drowsy(self, current_packet = None) -> bool: ...\n    def receive_wake_signal(self) -> None: ...\n    def set_voice_active(self, active: bool) -> None: ...\n    def transition_to_waking(self) -> None: ...\n    def complete_wake(self) -> Dict[str, Any]: ...\n    def enter_dreaming(self, handoff_id: str) -> bool: ...\n    def exit_dreaming(self) -> bool: ...\n    def enter_distracted(self) -> bool: ...\n    def exit_distracted(self) -> bool: ...\n    def initiate_offline(self) -> None: ...\n    def get_status(self) -> Dict[str, Any]: ...\n    def get_canned_response(self) -> Optional[str]: ...\n    def _format_checkpoint_as_review(checkpoint: str) -> str: ...\n\n\n## FILE: gaia-core/cognition/telemetric_senses.py\n# AST Summary (substitute with actual source at training time)\ndef tick(): ...\ndef update_token_usage(count: int): ...\ndef scan_files(): ...\ndef get_gpu_usage() -> dict[str, any]: ...\ndef get_hardware_profile() -> dict[str, any]: ...\ndef get_system_resources() -> dict[str, any]: ...\ndef system_health(): ...\ndef get_telemetry_summary() -> str: ...\ndef full_sense_sweep(): ...\n\n## FILE: gaia-core/cognition/temporal_interviewer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Temporal Interviewer — Prime interviews past-Lite via KV cache state swapping.\n\nPhase 2 of the Temporal Awareness Framework.  Prime formulates structured\nquestions about a past moment, Lite answers fr\"\"\"\nclass TemporalInterviewer():\n    \"\"\"Orchestrates Prime-interviews-past-Lite sessions.\"\"\"\n    def __init__(self, config, model_pool = None, temporal_state_manager = None, lite_journal = None, timeline_store = None) -> None: ...\n    def conduct_interview(self, state_id: Optional[str] = None) -> Optional[Dict[str, Any]]: ...\n    def _select_interview_target(self) -> Optional[Dict[str, Any]]: ...\n    def _has_transcript(self, state_id: str) -> bool: ...\n    def _run_interview_rounds(self, llm, state_metadata: Dict[str, Any]) -> List[Dict[str, str]]: ...\n    def _prime_ask(self, previous_rounds: List[Dict[str, str]], state_ts: str, round_idx: int) -> str: ...\n    def _lite_answer(self, llm, previous_rounds: List[Dict[str, str]], question: str) -> str: ...\n    def _analyze_coherence(self, transcript_rounds: List[Dict[str, str]], state_metadata: Dict[str, Any]) -> Dict[str, Any]: ...\n    def _get_journal_for_state(self, state_metadata: Dict[str, Any]) -> str: ...\n    def _parse_coherence(self, text: str) -> Dict[str, Any]: ...\n    def _default_coherence() -> Dict[str, Any]: ...\n    def _build_transcript(self, state_id: str, state_metadata: Dict[str, Any], rounds: List[Dict[str, str]], coherence: Dict[str, Any], duration_ms: int) -> Dict[str, Any]: ...\n    def _save_transcript(self, transcript: Dict[str, Any]) -> Optional[Path]: ...\n    def _emit_interview_event(self, transcript: Dict[str, Any]) -> None: ...\n\n\n## FILE: gaia-core/cognition/temporal_state_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Temporal State Manager — KV cache state baking, storage, and restoration for Lite.\n\nManages segmented KV cache snapshots that capture Lite's cognitive state at specific\npoints in time.  These snapshot\"\"\"\nclass TemporalStateManager():\n    \"\"\"Manages Lite KV cache state snapshots for temporal self-awareness.\"\"\"\n    def __init__(self, config, model_pool = None, timeline_store = None, session_manager = None, lite_journal = None) -> None: ...\n    def bake_state(self) -> Optional[Path]: ...\n    def load_state(self, state_id: str) -> bool: ...\n    def restore_current(self) -> bool: ...\n    def list_states(self) -> List[Dict[str, Any]]: ...\n    def get_state_metadata(self, state_id: str) -> Optional[Dict[str, Any]]: ...\n    def cleanup_old_states(self) -> int: ...\n    def _build_bake_context(self) -> List[Dict[str, str]]: ...\n    def _reconstruct_wake_cycle(self) -> str: ...\n    def _reconstruct_timeline_context(self) -> str: ...\n    def _reconstruct_conversation_context(self) -> str: ...\n    def _reconstruct_world_state(self) -> str: ...\n    def _reconstruct_journal_content(self) -> str: ...\n    def _save_lite_state(self, llm, metadata: Dict[str, Any]) -> Path: ...\n    def save_current_state_memory(self, llm) -> Any: ...\n    def restore_state_memory(self, llm, state_data) -> None: ...\n    def _load_lite_state(self, llm, state_path: Path) -> bool: ...\n    def _build_metadata(self, bake_duration_ms: int) -> Dict[str, Any]: ...\n    def _delete_state_files(self, bin_path: Path) -> None: ...\n    def _event_summary(data: Dict[str, Any]) -> str: ...\n\n\n## FILE: gaia-core/cognition/tests/test_goal_detector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for GoalDetector.\"\"\"\nclass TestFastPath():\n    def test_greeting_maps_to_casual_conversation(self): ...\n    def test_question_maps_to_information_seeking(self): ...\n    def test_tool_use_maps_to_task_execution(self): ...\n    def test_help_request_maps_to_task_assistance(self): ...\n\nclass TestSessionCarry():\n    def test_carries_active_goal(self): ...\n    def test_carry_decays_after_max_turns(self): ...\n\nclass TestLLMDetect():\n    def test_llm_detection_parses_response(self): ...\n    def test_llm_detection_graceful_on_failure(self): ...\n\nclass TestGoalShift():\n    def test_goal_shift_archives_previous(self): ...\n    def test_goal_shift_without_previous_goal(self): ...\n\nclass TestEdgeCases():\n    def test_no_goal_on_empty_input(self): ...\n    def test_fast_path_takes_priority_over_carry(self): ...\n    def test_parse_llm_response_handles_garbage(self): ...\n    def test_persistence_round_trip(self): ...\n\ndef _make_packet(user_intent: str = 'greeting', user_input: str = 'Hello!') -> CognitionPacket: ...\ndef _make_session_manager(meta: dict | None = None): ...\n\n## FILE: gaia-core/cognition/tests/test_heartbeat.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for ThoughtSeedHeartbeat — GAIA's thought seed triage daemon.\"\"\"\nclass TestSeedDirectoryOps():\n    def test_archive_seed_moves_file(self, _patch_seeds_dirs): ...\n    def test_archive_nonexistent_returns_false(self, _patch_seeds_dirs): ...\n    def test_defer_seed_moves_file(self, _patch_seeds_dirs): ...\n    def test_defer_seed_with_revisit_after(self, _patch_seeds_dirs): ...\n    def test_pending_due_promotes_back(self, _patch_seeds_dirs): ...\n    def test_pending_not_due_stays(self, _patch_seeds_dirs): ...\n    def test_pending_with_future_revisit_stays(self, _patch_seeds_dirs): ...\n    def test_pending_with_past_revisit_promotes(self, _patch_seeds_dirs): ...\n\nclass TestTriageSeed():\n    def test_archive_decision(self): ...\n    def test_pending_decision(self): ...\n    def test_act_decision(self): ...\n    def test_unparseable_defaults_to_pending(self): ...\n    def test_llm_failure_defaults_to_pending(self): ...\n\nclass TestActOnSeed():\n    def test_act_when_active(self, _patch_seeds_dirs): ...\n    def test_act_defers_when_dreaming(self, _patch_seeds_dirs): ...\n    def test_seed_archived_after_act(self, _patch_seeds_dirs): ...\n\nclass FakeConfig():\n\nclass TestHeartbeatLifecycle():\n    def test_start_creates_thread(self): ...\n    def test_stop_terminates_thread(self): ...\n    def test_tick_emits_timeline_event(self, _patch_seeds_dirs): ...\n    def test_tick_triages_seeds(self, _patch_seeds_dirs): ...\n\nclass TestTemporalIntegration():\n    def test_tick_writes_journal_entry(self, _patch_seeds_dirs): ...\n    def test_tick_bakes_state_on_interval(self, _patch_seeds_dirs): ...\n    def test_tick_skips_bake_off_interval(self, _patch_seeds_dirs): ...\n\ndef _patch_seeds_dirs(tmp_path, monkeypatch): ...\ndef _write_seed(seeds_dir: Path, filename: str, **overrides) -> Path: ...\ndef _mock_llm(response_text: str) -> MagicMock: ...\n\n## FILE: gaia-core/cognition/tests/test_initiative_engine.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for InitiativeEngine.\"\"\"\nclass FakeConfig():\n\nclass TestNoTopics():\n    def test_returns_none_when_no_topics(self, mock_pt, config, mock_agent_core): ...\n\nclass TestNoAgentCore():\n    def test_returns_none_without_agent_core(self, config): ...\n\nclass TestSuccessfulTurn():\n    def test_executes_turn_with_topic(self, mock_pt, config, mock_agent_core): ...\n    def test_uses_gil_session_id(self, mock_pt, config, mock_agent_core): ...\n\nclass TestSelfPrompt():\n    def test_prompt_contains_topic_description(self): ...\n    def test_prompt_contains_metadata(self): ...\n    def test_prompt_contains_reflection_header(self): ...\n\nclass TestErrorHandling():\n    def test_agent_core_error_returns_error_status(self, mock_pt, config): ...\n\ndef config(): ...\ndef mock_agent_core(): ...\n\n## FILE: gaia-core/cognition/tests/test_lite_journal.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for LiteJournal — Lite's introspective journal system.\"\"\"\nclass TestJournalLifecycle():\n    def test_write_creates_journal_file(self, mock_config): ...\n    def test_write_appends_to_existing(self, mock_config): ...\n    def test_returns_none_without_llm(self, mock_config): ...\n    def test_returns_none_without_model_pool(self, mock_config): ...\n    def test_entry_format_has_timestamp(self, mock_config): ...\n    def test_entry_includes_state_metadata(self, mock_config): ...\n\nclass TestRotation():\n    def test_rotate_when_max_entries_exceeded(self, mock_config): ...\n    def test_history_dir_created_on_rotate(self, mock_config): ...\n    def test_rotated_file_is_timestamped(self, mock_config): ...\n\nclass TestLoadEntries():\n    def test_load_latest_returns_full_content(self, mock_config): ...\n    def test_load_recent_entries_returns_n(self, mock_config): ...\n    def test_load_recent_entries_empty_journal(self, mock_config): ...\n    def test_get_entry_count(self, mock_config): ...\n\ndef mock_config(tmp_path): ...\ndef _mock_llm(response: str = ...) -> MagicMock: ...\ndef _mock_pool(response: str = ...) -> MagicMock: ...\ndef _mock_swm(state: str = 'active', seconds: float = 3600.0) -> MagicMock: ...\n\n## FILE: gaia-core/cognition/tests/test_sleep_gpu_integration.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for sleep cycle GPU release/reclaim and Discord presence wiring.\n\nValidates that SleepCycleLoop correctly:\n- Calls orchestrator /gpu/sleep when entering sleep\n- Calls orchestrator /gpu/wake when\"\"\"\nclass TestGPUReleaseOnSleep():\n    def test_gpu_release_called_on_sleep(self, loop): ...\n    def test_gpu_release_failure_nonfatal(self, loop): ...\n    def test_no_gpu_release_when_drowsy_cancelled(self, loop): ...\n\nclass TestGPUReclaimOnWake():\n    def test_gpu_reclaim_called_on_wake(self, loop): ...\n    def test_gpu_reclaim_failure_nonfatal(self, loop): ...\n\nclass TestPresenceDuringSleep():\n    def test_presence_sleeping_during_sleep(self, loop, mock_discord): ...\n    def test_presence_sleeping_during_task(self, loop, mock_discord): ...\n    def test_presence_resets_on_wake(self, loop, mock_discord): ...\n\nclass TestDreamingPresence():\n    def test_dreaming_shows_dnd_studying(self, loop, mock_discord): ...\n\nclass TestSOAPresence():\n    def test_soa_presence_calls_web_endpoint(self, mock_config): ...\n    def test_soa_presence_online_when_not_sleeping(self, mock_config): ...\n    def test_soa_presence_failure_nonfatal(self, mock_config): ...\n    def test_soa_presence_invisible_on_offline(self, mock_config): ...\n    def test_soa_presence_dnd_override(self, mock_config): ...\n\ndef mock_config(): ...\ndef mock_discord(): ...\ndef loop(mock_config, mock_discord): ...\n\n## FILE: gaia-core/cognition/tests/test_sleep_task_scheduler.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for SleepTaskScheduler.\"\"\"\nclass FakeConfig():\n\nclass TestRegistration():\n    def test_default_tasks_registered(self, scheduler): ...\n    def test_default_task_ids(self, scheduler): ...\n    def test_register_custom_task(self, bare_scheduler): ...\n\nclass TestScheduling():\n    def test_priority_ordering(self, bare_scheduler): ...\n    def test_lru_within_same_priority(self, bare_scheduler): ...\n    def test_never_run_beats_recently_run(self, bare_scheduler): ...\n    def test_empty_scheduler_returns_none(self, bare_scheduler): ...\n\nclass TestExecution():\n    def test_successful_execution(self, bare_scheduler): ...\n    def test_failed_execution(self, bare_scheduler): ...\n    def test_run_count_increments(self, bare_scheduler): ...\n    def test_failed_task_does_not_crash_scheduler(self, bare_scheduler): ...\n\nclass TestStatus():\n    def test_status_shape(self, scheduler): ...\n    def test_status_reflects_execution(self, bare_scheduler): ...\n\nclass TestBlueprintValidation():\n    def test_task_registered(self, scheduler): ...\n    def test_task_priority(self, scheduler): ...\n    def test_extract_enums(self, tmp_path): ...\n    def test_extract_endpoints(self, tmp_path): ...\n    def test_extract_constants(self, tmp_path): ...\n    def test_detects_stale_enum(self, tmp_path): ...\n    def test_append_update_notes(self, tmp_path): ...\n    def test_no_false_positives_when_current(self, tmp_path): ...\n\ndef config(): ...\ndef scheduler(config): ...\ndef bare_scheduler(config): ...\n\n## FILE: gaia-core/cognition/tests/test_sleep_wake_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for the GAIA sleep/wake state machine.\n\nTests the 6-state + 2-phase lifecycle:\n    ACTIVE → DROWSY → ASLEEP → DREAMING / DISTRACTED / OFFLINE\n    Internal phases: _FINISHING_TASK, _WAKING\n\n\"\"\"\nclass TestInitialState():\n    def test_starts_active(self, manager): ...\n    def test_no_pending_wake(self, manager): ...\n    def test_prime_not_available(self, manager): ...\n    def test_phase_none(self, manager): ...\n\nclass TestDrowsyThreshold():\n    def test_below_threshold(self, manager): ...\n    def test_at_threshold(self, manager): ...\n    def test_above_threshold(self, manager): ...\n    def test_not_when_asleep(self, manager): ...\n    def test_not_when_dreaming(self, manager): ...\n    def test_not_when_distracted(self, manager): ...\n\nclass TestInitiateDrowsy():\n    def test_happy_path(self, manager): ...\n    def test_checkpoint_written(self, manager, mock_config, tmp_path): ...\n    def test_rejects_from_asleep(self, manager): ...\n    def test_rejects_from_dreaming(self, manager): ...\n    def test_rejects_from_distracted(self, manager): ...\n    def test_cancels_on_wake_during_checkpoint(self, manager): ...\n    def test_rotation_before_create(self, manager): ...\n    def test_previous_checkpoint_preserved_in_backup(self, manager, mock_config): ...\n    def test_consumed_sentinel_cleared_on_new_checkpoint(self, manager, mock_config): ...\n\nclass TestReceiveWakeSignal():\n    def test_wake_while_active(self, manager): ...\n    def test_wake_while_asleep_transitions_to_waking(self, manager): ...\n    def test_wake_while_asleep_non_interruptible(self, manager): ...\n    def test_wake_while_drowsy_sets_flag(self, manager): ...\n    def test_wake_while_dreaming_defers(self, manager): ...\n    def test_wake_while_distracted_notes(self, manager): ...\n\nclass TestCompleteWake():\n    def test_restores_from_checkpoint(self, manager): ...\n    def test_complete_wake_no_checkpoint(self, manager, mock_config, tmp_path): ...\n    def test_complete_wake_wrong_state(self, manager): ...\n    def test_complete_wake_marks_consumed(self, manager, mock_config): ...\n    def test_complete_wake_no_consumed_when_no_checkpoint(self, manager, mock_config): ...\n\nclass TestStatus():\n    def test_status_fields(self, manager): ...\n    def test_status_reflects_state_change(self, manager): ...\n    def test_status_includes_dreaming_handoff_id(self, manager): ...\n\nclass TestFormatCheckpoint():\n    def test_empty_checkpoint(self): ...\n    def test_review_framing(self): ...\n\nclass TestTransitionToWaking():\n    def test_from_asleep(self, manager): ...\n    def test_rejects_from_active(self, manager): ...\n\nclass TestCannedResponses():\n    def test_no_canned_when_active(self, manager): ...\n    def test_no_canned_when_asleep(self, manager): ...\n    def test_canned_when_dreaming(self, manager): ...\n    def test_canned_when_distracted(self, manager): ...\n\nclass TestDreamingTransition():\n    def test_enter_dreaming_from_asleep(self, manager): ...\n    def test_enter_dreaming_rejects_from_active(self, manager): ...\n    def test_exit_dreaming_to_asleep(self, manager): ...\n    def test_exit_dreaming_triggers_pending_wake(self, manager): ...\n    def test_exit_dreaming_rejects_from_active(self, manager): ...\n\nclass TestDistractedTransition():\n    def test_enter_distracted_from_asleep(self, manager): ...\n    def test_enter_distracted_rejects_from_active(self, manager): ...\n    def test_exit_distracted_to_asleep(self, manager): ...\n    def test_exit_distracted_triggers_pending_wake(self, manager): ...\n    def test_exit_distracted_rejects_from_active(self, manager): ...\n\nclass TestOffline():\n    def test_offline_from_active(self, manager): ...\n    def test_offline_from_asleep(self, manager): ...\n    def test_offline_from_dreaming(self, manager): ...\n    def test_offline_from_distracted(self, manager): ...\n\nclass TestLLMCheckpoint():\n    def test_llm_checkpoint_with_model_pool(self, mock_config): ...\n    def test_llm_fallback_on_no_lite_model(self, mock_config): ...\n    def test_llm_fallback_on_exception(self, mock_config): ...\n    def test_template_without_model_pool(self, manager, mock_config): ...\n\nclass TestCheckpointConsumed():\n    def test_is_consumed_false_initially(self, manager): ...\n    def test_mark_consumed_creates_sentinel(self, manager, mock_config): ...\n    def test_create_checkpoint_clears_consumed(self, manager, mock_config): ...\n\nclass TestDrowsyCancelBehavior():\n    def test_early_cancel_skips_checkpoint_entirely(self, mock_config): ...\n    def test_late_cancel_marks_checkpoint_consumed(self, mock_config): ...\n\nclass TestWakeSignalIdleMonitor():\n    def test_receive_wake_signal_resets_idle_monitor(self, mock_config): ...\n    def test_receive_wake_signal_without_idle_monitor(self, mock_config): ...\n    def test_wake_signal_while_active_does_not_reset_idle(self, mock_config): ...\n    def test_wake_signal_during_drowsy_resets_idle(self, mock_config): ...\n\ndef mock_config(tmp_path): ...\ndef manager(mock_config): ...\n\n## FILE: gaia-core/cognition/tests/test_stream_observer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Unit tests for StreamObserver.verify_side_effects.\n\nTests the post-execution verification layer that checks whether side\neffects reported by route_output() actually produced the expected\nartifacts (fi\"\"\"\nclass DummyLLM():\n    \"\"\"Minimal LLM stub that satisfies StreamObserver.__init__.\"\"\"\n    def create_chat_completion(self, **kwargs): ...\n\ndef _make_packet() -> CognitionPacket: ...\ndef mock_config(): ...\ndef observer(mock_config): ...\ndef packet(): ...\ndef test_verify_thought_seed_file_exists(observer, packet, tmp_path): ...\ndef test_verify_thought_seed_file_missing(observer, packet, tmp_path): ...\ndef test_verify_thought_seed_file_empty(observer, packet, tmp_path): ...\ndef test_verify_sidecar_action_success(observer, packet): ...\ndef test_verify_sidecar_action_error(observer, packet): ...\ndef test_verify_goal_shift_ok(observer, packet): ...\ndef test_verify_no_side_effects(observer, packet): ...\ndef test_verify_disabled_by_config(mock_config, packet): ...\ndef test_verify_fallback_thought_seed(observer, packet, tmp_path, monkeypatch): ...\ndef test_verify_appends_reflection_log(observer, packet): ...\ndef test_verify_multiple_issues(observer, packet, tmp_path): ...\ndef test_verify_no_packet_returns_ok(observer): ...\n\n## FILE: gaia-core/cognition/tests/test_temporal_context.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for the Temporal Context Builder.\"\"\"\nclass TestSemanticTime():\n    def test_includes_day_of_week(self): ...\n    def test_morning(self): ...\n    def test_afternoon(self): ...\n    def test_evening(self): ...\n    def test_night(self): ...\n    def test_early_morning(self): ...\n\nclass TestFormatDuration():\n    def test_sub_minute(self): ...\n    def test_minutes(self): ...\n    def test_hours_and_minutes(self): ...\n    def test_exact_hours(self): ...\n\nclass TestSessionSummary():\n    def test_full_session(self): ...\n    def test_no_data_returns_empty(self): ...\n\nclass TestStateSummary():\n    def test_active_state(self): ...\n    def test_empty_returns_empty(self): ...\n\nclass TestCodeEvolutionSummary():\n    def test_reads_snapshot(self, tmp_path): ...\n    def test_no_changes(self, tmp_path): ...\n    def test_missing_file(self): ...\n\nclass TestBuildTemporalContext():\n    def test_minimal_output(self): ...\n    def test_with_session_data(self): ...\n    def test_graceful_with_broken_timeline(self): ...\n\n\n## FILE: gaia-core/cognition/tests/test_temporal_interviewer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for TemporalInterviewer — Prime interviews past-Lite via KV cache swapping.\"\"\"\nclass FakeLlamaState():\n    \"\"\"Picklable stand-in for llama_cpp.LlamaState.\"\"\"\n    def __init__(self, label: str = 'default'): ...\n\nclass TestInterviewTargetSelection():\n    def test_selects_oldest_uninterviewed_state(self, interviewer): ...\n    def test_skips_most_recent_state(self, mock_config, mock_model_pool): ...\n    def test_falls_back_to_already_interviewed(self, interviewer, tsm_with_states): ...\n    def test_returns_none_with_no_states(self, mock_config, mock_model_pool): ...\n\nclass TestInterviewFlow():\n    def test_full_interview_cycle(self, interviewer, mock_llm, mock_model_pool): ...\n    def test_state_restored_on_interview_error(self, interviewer, mock_llm): ...\n    def test_returns_none_without_model_pool(self, mock_config, tsm_with_states): ...\n    def test_returns_none_without_tsm(self, mock_config, mock_model_pool): ...\n\nclass TestLockBehavior():\n    def test_interview_holds_lite_lock(self, interviewer, mock_llm): ...\n    def test_lock_released_after_interview(self, interviewer): ...\n\nclass TestNarrativeCoherence():\n    def test_coherence_analysis_called(self, interviewer, mock_model_pool): ...\n    def test_coherence_parsing(self, interviewer): ...\n    def test_coherence_graceful_on_parse_failure(self, interviewer): ...\n\nclass TestTranscriptStorage():\n    def test_transcript_saved_as_json(self, interviewer): ...\n    def test_transcript_contains_all_rounds(self, interviewer): ...\n    def test_transcript_dir_created(self, mock_config, mock_model_pool, tsm_with_states): ...\n\nclass FakeConfig():\n\nclass TestHeartbeatIntegration():\n    def test_interview_triggered_on_interval(self): ...\n    def test_interview_not_triggered_off_interval(self): ...\n    def test_interview_skipped_when_sleeping(self): ...\n    def test_interview_failure_doesnt_crash_heartbeat(self): ...\n\ndef mock_config(tmp_path): ...\ndef mock_llm(): ...\ndef mock_model_pool(mock_llm): ...\ndef _create_baked_state(state_dir: Path, ts_label: str) -> str: ...\ndef tsm_with_states(mock_config, mock_model_pool, tmp_path): ...\ndef mock_journal(): ...\ndef mock_timeline(): ...\ndef interviewer(mock_config, mock_model_pool, tsm_with_states, mock_journal, mock_timeline): ...\n\n## FILE: gaia-core/cognition/tests/test_temporal_state_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for TemporalStateManager — KV cache state baking and restoration.\"\"\"\nclass FakeLlamaState():\n    \"\"\"Picklable stand-in for llama_cpp.LlamaState.\"\"\"\n    def __init__(self): ...\n\nclass TestStateDirectory():\n    def test_creates_state_dir(self, mock_config, mock_model_pool): ...\n    def test_list_states_empty(self, mock_config, mock_model_pool): ...\n\nclass TestBakeState():\n    def test_bake_creates_bin_file(self, mock_config, mock_model_pool, mock_llm): ...\n    def test_bake_creates_json_sidecar(self, mock_config, mock_model_pool): ...\n    def test_bake_returns_none_without_llm(self, mock_config): ...\n    def test_bake_returns_none_without_model_pool(self, mock_config): ...\n    def test_bake_state_is_picklable(self, mock_config, mock_model_pool): ...\n    def test_bake_includes_journal_content(self, mock_config, mock_model_pool, mock_llm, mock_journal): ...\n\nclass TestLoadState():\n    def test_load_existing_state(self, mock_config, mock_model_pool, mock_llm): ...\n    def test_load_nonexistent_returns_false(self, mock_config, mock_model_pool): ...\n    def test_corrupt_state_renamed(self, mock_config, mock_model_pool, mock_llm): ...\n    def test_restore_current_loads_latest(self, mock_config, mock_model_pool, mock_llm): ...\n\nclass TestRotation():\n    def test_cleanup_enforces_max_files(self, mock_config, mock_model_pool): ...\n    def test_cleanup_enforces_max_bytes(self, mock_config, mock_model_pool): ...\n    def test_cleanup_deletes_sidecar_too(self, mock_config, mock_model_pool): ...\n\nclass TestContextReconstruction():\n    def test_reconstruct_timeline_context(self, mock_config, mock_model_pool, mock_timeline): ...\n    def test_reconstruct_conversation_context(self, mock_config, mock_model_pool, mock_session_manager): ...\n    def test_list_states_with_metadata(self, mock_config, mock_model_pool): ...\n\ndef mock_config(tmp_path): ...\ndef mock_llm(): ...\ndef mock_model_pool(mock_llm): ...\ndef mock_timeline(): ...\ndef mock_session_manager(): ...\ndef mock_journal(): ...\n\n## FILE: gaia-core/cognition/thought_seed.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Thought Seed System (GAIA pillar-compliant).\n- Saves, stores, reviews, and processes thought seeds.\n- Seeds are now generated by the main LLM via the THOUGHT_SEED: directive\n- and parsed by the output\"\"\"\ndef save_thought_seed(seed_text: str, packet: CognitionPacket, config: Config) -> Dict[str, Any] | None: ...\ndef list_unreviewed_seeds(): ...\ndef get_seed_by_id(seed_id: str) -> Dict[str, Any] | None: ...\ndef update_seed(seed_id: str, seed_data: Dict[str, Any]) -> bool: ...\ndef review_and_process_seeds(config = None, llm = None, auto_act = False): ...\ndef refine_seed(seed_id, refinement_prompt, config = None, llm = None): ...\ndef link_seeds(source_seed_id, target_seed_id, relationship): ...\ndef maybe_review_seeds(config, llm = None): ...\ndef archive_seed(seed_filename: str) -> bool: ...\ndef defer_seed(seed_filename: str, revisit_after: str | None = None) -> bool: ...\ndef list_pending_seeds_due() -> list[tuple[Path, dict]]: ...\n\n## FILE: gaia-core/cognition/tool_selector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tool Selector Module\n\nResponsible for:\n1. Determining if a request needs MCP tool usage\n2. Selecting the appropriate tool with low-temperature generation\n3. Extracting structured parameters\n4. Providi\"\"\"\ndef _structured_json_kwargs(model: Any, schema: Dict[str, Any]) -> Dict[str, Any]: ...\ndef _registry_to_catalog(registry: Dict[str, Any]) -> Dict[str, Any]: ...\ndef needs_tool_routing(packet: CognitionPacket, user_input: str) -> bool: ...\ndef select_tool(packet: CognitionPacket, user_input: str, model, temperature: float = 0.15) -> Tuple[Optional[SelectedTool], List[SelectedTool]]: ...\ndef review_selection(packet: CognitionPacket, selected_tool: SelectedTool, model, temperature: float = 0.3) -> Tuple[float, str]: ...\ndef _build_tool_catalog() -> str: ...\ndef _extract_content(result) -> str: ...\ndef _extract_json_from_response(content: str) -> str: ...\ndef initialize_tool_routing(packet: CognitionPacket) -> CognitionPacket: ...\ndef inject_tool_result_into_packet(packet: CognitionPacket) -> CognitionPacket: ...\n\n## FILE: gaia-core/cognition/topic_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Manages the creation, update, resolution, prioritization, and pruning of GAIA's topic cache.\n\nSupports the Initiative Loop (GIL) by maintaining a prioritized, well-structured list of emergent discussi\"\"\"\ndef _load_topic_cache(path: str) -> List[Dict[str, Any]]: ...\ndef _save_topic_cache(path: str, cache: List[Dict[str, Any]]): ...\ndef add_topic(path: str, topic: Dict[str, Any]) -> None: ...\ndef resolve_topic(path: str, topic_id: str) -> bool: ...\ndef update_topic(path: str, topic_id: str, updates: Dict[str, Any]) -> bool: ...\ndef prune_resolved_topics(path: str) -> None: ...\ndef list_topics(path: str, include_resolved: bool = False) -> List[Dict[str, Any]]: ...\ndef prioritize_topics(path: str, top_n: Optional[int] = 5) -> List[Dict[str, Any]]: ...\n\n## FILE: gaia-core/config.py\n# AST Summary (substitute with actual source at training time)\nclass Config():\n    \"\"\"A simplified configuration class for gaia-core.\nValues should be injected at runtime or loaded from \"\"\"\n    def __post_init__(self): ...\n    def _load_constants(self): ...\n    def get_api_key(self, provider: str) -> str: ...\n    def get_persona_instructions(self) -> str: ...\n    def get_model_name(self, model_alias: str) -> str: ...\n    def get_instance(cls) -> Config: ...\n    def _load_cheat_sheet(self): ...\n\ndef get_config() -> Config: ...\n\n## FILE: gaia-core/ethics/__init__.py\n# AST Summary (substitute with actual source at training time)\n\n## FILE: gaia-core/ethics/consent_protocol.py\n# AST Summary (substitute with actual source at training time)\nclass ConsentProtocol():\n    \"\"\"Verifies GAIA's explicit consent to operate under current identity, context, and system state.\nMust \"\"\"\n    def request_consent(reason = 'Initial boot') -> bool: ...\n\n\n## FILE: gaia-core/ethics/core_identity_guardian.py\n# AST Summary (substitute with actual source at training time)\nclass CoreIdentityGuardian():\n    \"\"\"Verifies prompt behavior and session instructions against GAIA's immutable Tier I identity.\nOperates\"\"\"\n    def __init__(self, config): ...\n    def load_identity(self) -> Optional[dict]: ...\n    def validate_prompt_stack(self, persona_traits: dict, instructions: List[str], prompt: str) -> bool: ...\n\n\n## FILE: gaia-core/ethics/ethical_sentinel.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"ethics/ethical_sentinel.py\n\nThe Ethical Sentinel monitors system health and cognitive strain for GAIA.\"\"\"\nclass EthicalSentinel():\n    \"\"\"Monitors system health, loop safety, error logs, and optionally Tier I identity violations.\nWorks al\"\"\"\n    def __init__(self, identity_guardian = None): ...\n    def check_system_resources(self) -> bool: ...\n    def check_loop_counter(self) -> bool: ...\n    def check_recent_errors(self) -> bool: ...\n    def register_error(self, exc: Exception): ...\n    def reset_loop(self): ...\n    def run_full_safety_check(self, persona_traits = None, instructions = None, prompt = None) -> bool: ...\n\n\n## FILE: gaia-core/integrations/discord_connector.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Discord Connector for GAIA Spinal Column\n\nThis module provides Discord integration as both:\n- Output destination (send responses to Discord channels via webhook or bot)\n- Input source (listen for @GAI\"\"\"\nclass DiscordConnector(DestinationConnector):\n    \"\"\"Discord connector for the GAIA spinal column.\nSupports both webhook output and (future) bot-based bi\"\"\"\n    def __init__(self, config: Optional[DiscordConfig] = None): ...\n    def send(self, content: str, target: DestinationTarget, packet: Optional[CognitionPacket] = None) -> bool: ...\n    def send_stream(self, token_generator: Generator[str, None, None], target: DestinationTarget, packet: Optional[CognitionPacket] = None) -> bool: ...\n    def is_available(self) -> bool: ...\n    def _send_via_webhook(self, content: str, target: DestinationTarget, packet: Optional[CognitionPacket] = None) -> bool: ...\n    def _send_via_bot(self, content: str, target: DestinationTarget, packet: Optional[CognitionPacket] = None) -> bool: ...\n    def _split_message_for_discord(self, content: str) -> List[str]: ...\n    def _is_bot_connected(self) -> bool: ...\n    def set_message_callback(self, callback: Callable[[str, str, Dict[str, Any]], None]) -> None: ...\n    def generate_dm_session_id(user_id: str) -> str: ...\n    def is_dm_session(session_id: str) -> bool: ...\n    def start_bot_listener(self) -> bool: ...\n    def stop_bot_listener(self) -> None: ...\n\n\n## FILE: gaia-core/main.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia-core FastAPI application entry point.\n\nProvides the HTTP API for the cognitive loop service.\nThis is The Brain - Cognitive loop and reasoning.\"\"\"\nclass AIManagerShim():\n    \"\"\"A lightweight shim providing the interface AgentCore expects from ai_manager.\n\nAgentCore requires:\n-\"\"\"\n    def __init__(self, config, model_pool, session_manager): ...\n    def initialize(self, persona_name: str = 'prime'): ...\n\nclass MinimalPersona():\n    \"\"\"Minimal persona object when full persona loading fails or from dict data.\"\"\"\n    def __init__(self, name: str, data: Dict[str, Any] = None): ...\n\ndef initialize_cognitive_system(): ...\ndef lifespan(app: FastAPI): ...\ndef _write_shutdown_checkpoints(app: FastAPI) -> dict: ...\ndef health_check(): ...\ndef root(): ...\ndef get_status(): ...\ndef cognition_checkpoint(): ...\ndef process_packet(packet_data: Dict[str, Any]): ...\n\n## FILE: gaia-core/memory/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.memory - Memory and state management modules.\n\nThis package provides:\n- dev_matrix: Development matrix for tracking agent state\n- conversation: Conversation memory subpackage\"\"\"\n\n## FILE: gaia-core/memory/codex_writer.py\n# AST Summary (substitute with actual source at training time)\nclass CodexWriter():\n    def __init__(self, config: Config, semantic_codex: SemanticCodex): ...\n    def document_information(self, packet: CognitionPacket, info_to_document: str, symbol: str, title: str, tags: Optional[List[str]] = None, llm_model: Optional[Model] = None) -> Optional[Path]: ...\n    def _refine_with_llm(self, raw_info: str, llm_model: Model, packet: CognitionPacket, symbol: str, title: str) -> str: ...\n\n\n## FILE: gaia-core/memory/conversation/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.memory.conversation - Conversation memory management.\n\nThis package provides:\n- summarizer: Conversation summarization for context management\"\"\"\n\n## FILE: gaia-core/memory/conversation/archiver.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"conversation/archiver.py\n\nHandles saving and loading archived conversations in Markdown format.\nManages storage, file formatting, and retrieval operations.\"\"\"\nclass ConversationArchiver():\n    \"\"\"Saves a conversation history to disk, structured by persona + session ID.\"\"\"\n    def __init__(self, config): ...\n    def archive_conversation(self, session_id: str, persona: str, messages: List[dict], summary: str, keywords: List[str]): ...\n\n\n## FILE: gaia-core/memory/conversation/keywords.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"conversation/keywords.py\n\nHandles keyword extraction from conversation history.\nLightweight heuristics for identifying important terms.\"\"\"\nclass ConversationKeywordExtractor():\n    \"\"\"Extracts high-value keywords from a conversation history.\"\"\"\n    def extract_keywords(self, messages: List[dict], max_keywords: int = 10) -> List[str]: ...\n\n\n## FILE: gaia-core/memory/conversation/manager.py\n# AST Summary (substitute with actual source at training time)\nclass ConversationManager():\n    \"\"\"Tracks user-assistant message history and triggers summarization + archiving\nafter N messages. Suppo\"\"\"\n    def __init__(self, config, llm = None, embed_model = None): ...\n    def set_persona(self, persona: str): ...\n    def add_message(self, role: str, content: str): ...\n    def get_recent_messages(self, count: int = 10) -> List[dict]: ...\n    def summarize_and_archive(self): ...\n    def reset(self): ...\n    def build_smart_history(self, current_input: str, max_recent: int = 3, max_salient: int = 2) -> List[Dict]: ...\n\n\n## FILE: gaia-core/memory/conversation/summarizer.py\n# AST Summary (substitute with actual source at training time)\nclass ConversationSummarizer():\n    \"\"\"Uses the LLM to summarize a conversation history.\nFalls back to placeholder text if no LLM is availa\"\"\"\n    def __init__(self, llm = None, embed_model = None): ...\n    def generate_summary(self, messages: List[dict], packet: object = None) -> str: ...\n    def build_smart_history(self, full_history: List[Dict], current_input: str, max_recent: int = 3, max_salient: int = 2) -> List[Dict]: ...\n\ndef _get_model_pool(): ...\n\n## FILE: gaia-core/memory/dev_matrix.py\n# AST Summary (substitute with actual source at training time)\nclass GAIADevMatrix():\n    \"\"\"Persistent manager for GAIA's self-development tasks.\nStores and retrieves structured roadmap tasks.\"\"\"\n    def __init__(self, config): ...\n    def _load(self): ...\n    def _save(self): ...\n    def add_task(self, label: str, purpose: str, urgency: str = 'medium', impact: str = 'medium', source: str = 'manual') -> None: ...\n    def get_open_tasks(self) -> List[Dict]: ...\n    def resolve_task(self, label: str) -> bool: ...\n    def dump(self) -> List[Dict]: ...\n\n\n## FILE: gaia-core/memory/knowledge_integrity.py\n# AST Summary (substitute with actual source at training time)\ndef hash_file(filepath): ...\ndef check_or_generate_hash_manifest(): ...\n\n## FILE: gaia-core/memory/memory_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"MemoryManager facade to unify short-term, session, and long-term memory.\n\nShort-term: in-process dict (fast cache)\nWorking: SessionManager (persistent session history)\nLong-term: VectorIndexer via MCP\"\"\"\nclass MemoryManager():\n    def __init__(self, config: Config = None): ...\n    def instance(cls, config: Config = None) -> 'MemoryManager': ...\n    def set_short(self, key: str, value: Any): ...\n    def get_short(self, key: str, default = None): ...\n    def add_message(self, session_id: str, role: str, content: str): ...\n    def get_history(self, session_id: str): ...\n    def query_long(self, query: str, top_k: int = 5): ...\n\n\n## FILE: gaia-core/memory/priority_manager.py\n# AST Summary (substitute with actual source at training time)\nclass GAIAPriorityManager():\n    \"\"\"Centralized persistent task and priority memory for GAIA.\nTracks open tasks from all sources (manual\"\"\"\n    def __init__(self, config): ...\n    def _load(self): ...\n    def _save(self): ...\n    def add_task(self, label: str, details: str, urgency: str = 'medium', impact: str = 'medium', source: str = 'manual') -> None: ...\n    def get_open_tasks(self) -> List[Dict]: ...\n    def get_top_tasks(self, limit: int = 5) -> List[Dict]: ...\n    def resolve_task(self, label: str) -> bool: ...\n    def clear_resolved(self): ...\n    def dump(self) -> List[Dict]: ...\n\n\n## FILE: gaia-core/memory/semantic_codex.py\n# AST Summary (substitute with actual source at training time)\nclass CodexEntry():\n\nclass SemanticCodex():\n    \"\"\"Side-car memory for semantically compressed concepts.\nLoads JSON/YAML files from Config.KNOWLEDGE_CO\"\"\"\n    def __init__(self, config): ...\n    def instance(cls, config): ...\n    def write_entry(self, entry: CodexEntry) -> Path: ...\n    def _iter_files(self): ...\n    def _checksum(self, path: Path) -> str: ...\n    def _load_one(self, path: Path): ...\n    def _load_all(self): ...\n    def hot_reload(self) -> bool: ...\n    def get(self, symbol: str) -> Optional[CodexEntry]: ...\n    def search(self, query: str, limit: int = 10) -> List[CodexEntry]: ...\n\n\n## FILE: gaia-core/memory/session_history_indexer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Per-session vector index for conversation turns and topic summaries.\n\nProvides semantic retrieval over session history so that older turns\ncan be recalled by relevance rather than recency. Designed to\"\"\"\nclass SessionHistoryIndexer():\n    \"\"\"Per-session vector index for conversation turns and topic summaries.\"\"\"\n    def __init__(self, session_id: str, persist_dir: str = _DEFAULT_PERSIST_DIR): ...\n    def instance(cls, session_id: str) -> 'SessionHistoryIndexer': ...\n    def _get_model(self): ...\n    def _encode(self, text: str) -> Optional[np.ndarray]: ...\n    def index_turn(self, turn_idx: int, user_msg: str, assistant_msg: str): ...\n    def retrieve(self, query: str, top_k_turns: int = 3, top_k_topics: int = 2, exclude_recent_n: int = 6) -> Dict: ...\n    def _maybe_generate_topic_summary(self): ...\n    def archive_and_reset(self): ...\n    def _save(self): ...\n    def _save_to(self, path: str): ...\n    def _load(self): ...\n\ndef _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float: ...\ndef _get_embed_model(): ...\n\n## FILE: gaia-core/memory/session_manager.py\n# AST Summary (substitute with actual source at training time)\nclass Session():\n    \"\"\"A dedicated data class to hold the state for a single conversation session.\nUsing a class instead of\"\"\"\n    def __init__(self, session_id: str, persona: str = 'default'): ...\n    def to_dict(self) -> Dict: ...\n    def from_dict(cls, data: Dict) -> 'Session': ...\n    def last_message_timestamp(self): ...\n\nclass SessionManager():\n    \"\"\"Manages loading, saving, and accessing all persistent conversation sessions.\nThis is the single sour\"\"\"\n    def __init__(self, config, llm = None, embed_model = None): ...\n    def _load_state(self) -> Dict[str, Session]: ...\n    def _sanitize_for_json(obj): ...\n    def _save_state(self): ...\n    def get_or_create_session(self, session_id: str, persona: str = 'default') -> Session: ...\n    def add_message(self, session_id: str, role: str, content: str): ...\n    def summarize_and_archive(self, session_id: str): ...\n    def get_session_meta(self, session_id: str, key: str, default = None): ...\n    def set_session_meta(self, session_id: str, key: str, value): ...\n    def get_history(self, session_id: str) -> List[Dict]: ...\n    def reset_session(self, session_id: str): ...\n    def sanitize_sessions(self, vector_dir: str = 'data/shared/session_vectors', max_age_days: int = 7, max_active_messages: int = 0) -> Dict[str, int]: ...\n    def record_last_activity(self): ...\n\n\n## FILE: gaia-core/memory/status_tracker.py\n# AST Summary (substitute with actual source at training time)\nclass GAIAStatus():\n    \"\"\"Thread-safe global status manager for GAIA boot and runtime state.\nAllows concurrent modules to upda\"\"\"\n    def update(cls, key: str, value): ...\n    def get(cls, key: str, default = None): ...\n    def as_dict(cls): ...\n    def clear(cls): ...\n\n\n## FILE: gaia-core/memory/tests/test_semantic_codex.py\n# AST Summary (substitute with actual source at training time)\ndef temp_knowledge_dir(tmp_path): ...\ndef mock_config(temp_knowledge_dir): ...\ndef semantic_codex(mock_config): ...\ndef test_write_entry_creates_markdown_file(semantic_codex, temp_knowledge_dir): ...\ndef test_load_one_markdown_with_front_matter(semantic_codex, temp_knowledge_dir): ...\ndef test_load_one_markdown_missing_symbol(semantic_codex, temp_knowledge_dir, caplog): ...\ndef test_load_one_markdown_invalid_yaml(semantic_codex, temp_knowledge_dir, caplog): ...\ndef test_load_one_json_still_works(semantic_codex, temp_knowledge_dir): ...\ndef test_iter_files_includes_self_generated_docs(semantic_codex, temp_knowledge_dir): ...\ndef test_hot_reload_updates_markdown_entry(semantic_codex, temp_knowledge_dir): ...\n\n## FILE: gaia-core/memory/tests/test_session_history_indexer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Tests for SessionHistoryIndexer — the per-session vector index that powers\nthe RAG component of the rolling history feature.\n\nTests cover:\n1. Instantiation and singleton pattern\n2. Turn indexing (with\"\"\"\nclass FakeEmbedModel():\n    \"\"\"A deterministic fake embedding model for testing.\n\nEncodes text by hashing characters into a fixed-d\"\"\"\n    def encode(self, texts, show_progress_bar = False): ...\n\nclass TestInstantiation():\n    def test_creates_empty_index(self, indexer): ...\n    def test_singleton_returns_same_instance(self, persist_dir, patched_model): ...\n    def test_singleton_different_sessions_are_different(self, persist_dir, patched_model): ...\n\nclass TestTurnIndexing():\n    def test_index_one_turn(self, indexer): ...\n    def test_index_multiple_turns(self, indexer): ...\n    def test_duplicate_turn_idx_is_skipped(self, indexer): ...\n    def test_long_messages_are_truncated(self, indexer): ...\n    def test_embedding_is_numpy_array(self, indexer): ...\n\nclass TestRetrieval():\n    def _populate(self, indexer, n = 10): ...\n    def test_retrieve_returns_dict_with_turns_and_topics(self, indexer): ...\n    def test_retrieve_empty_index_returns_empty(self, indexer): ...\n    def test_retrieve_excludes_recent_turns(self, indexer): ...\n    def test_retrieve_returns_similarity_scores(self, indexer): ...\n    def test_retrieve_respects_top_k(self, indexer): ...\n    def test_retrieve_filters_by_minimum_threshold(self, indexer): ...\n\nclass TestTopicSummaries():\n    def test_no_topic_before_interval(self, indexer): ...\n    def test_topic_generated_at_interval(self, indexer): ...\n    def test_topic_has_correct_turn_range(self, indexer): ...\n    def test_multiple_topics_generated(self, indexer): ...\n    def test_topic_embedding_stored(self, indexer): ...\n    def test_topics_retrievable(self, indexer): ...\n\nclass TestPersistence():\n    def test_save_creates_json_file(self, indexer, persist_dir): ...\n    def test_load_restores_turns(self, persist_dir, patched_model): ...\n    def test_load_restores_topics(self, persist_dir, patched_model): ...\n    def test_load_restores_last_topic_turn_idx(self, persist_dir, patched_model): ...\n    def test_corrupt_file_starts_fresh(self, persist_dir, patched_model): ...\n\nclass TestArchiveAndReset():\n    def test_archive_creates_archive_file(self, indexer, persist_dir): ...\n    def test_archive_clears_index(self, indexer): ...\n    def test_archive_preserves_data_in_archive_file(self, indexer, persist_dir): ...\n    def test_archive_empty_index_is_noop(self, indexer, persist_dir): ...\n    def test_new_turns_after_archive(self, indexer): ...\n\nclass TestGracefulDegradation():\n    def test_index_turn_is_noop_without_model(self, indexer_no_model): ...\n    def test_retrieve_returns_empty_without_model(self, indexer_no_model): ...\n    def test_archive_works_without_model(self, indexer_no_model): ...\n    def test_no_topics_generated_without_model(self, indexer_no_model): ...\n\nclass TestCosineSimilarity():\n    def test_identical_vectors(self): ...\n    def test_orthogonal_vectors(self): ...\n    def test_zero_vector(self): ...\n    def test_opposite_vectors(self): ...\n\ndef clear_singletons(): ...\ndef persist_dir(tmp_path): ...\ndef fake_model(): ...\ndef patched_model(fake_model): ...\ndef no_model(): ...\ndef indexer(persist_dir, patched_model): ...\ndef indexer_no_model(persist_dir, no_model): ...\n\n## FILE: gaia-core/memory/tests/test_session_rag_integration.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Integration tests for the RAG + Rolling History pipeline.\n\nTests the interaction between:\n- SessionManager indexing hook (add_message → index_turn)\n- AgentCore sliding window + RAG retrieval (_create_\"\"\"\nclass FakeEmbedModel():\n    \"\"\"Deterministic fake embedding model.\"\"\"\n    def encode(self, texts, show_progress_bar = False): ...\n\nclass TestSessionManagerIndexingHook():\n    \"\"\"Verify that SessionManager.add_message() triggers turn indexing.\"\"\"\n    def test_assistant_message_triggers_indexing(self, tmp_path, persist_dir, fake_model): ...\n    def test_user_message_alone_does_not_index(self, tmp_path, persist_dir, fake_model): ...\n    def test_indexing_failure_does_not_block_message(self, tmp_path): ...\n\nclass TestFormatRetrievedContext():\n    def test_empty_results(self): ...\n    def test_turns_only(self): ...\n    def test_topics_only(self): ...\n    def test_both_turns_and_topics(self): ...\n\nclass TestPromptBuilderTier15():\n    \"\"\"Test that Tier 1.5 (retrieved_session_context) is correctly injected.\"\"\"\n    def test_rag_content_appears_in_prompt(self): ...\n    def test_rag_content_truncated_when_over_budget(self): ...\n    def test_empty_rag_content_produces_no_prompt(self): ...\n\nclass TestSlidingWindow():\n    \"\"\"Test that the sliding window correctly limits history in the packet.\"\"\"\n    def test_short_history_fully_included(self): ...\n    def test_long_history_windowed(self): ...\n    def test_rag_only_triggers_beyond_window(self): ...\n\nclass TestArchiveFlowIntegration():\n    \"\"\"Test that summarize_and_archive correctly archives the vector index.\"\"\"\n    def test_archive_called_during_summarize(self, tmp_path, fake_model): ...\n\ndef clear_singletons(): ...\ndef persist_dir(tmp_path): ...\ndef fake_model(): ...\n\n## FILE: gaia-core/models/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.models - Model pool and LLM backend implementations.\n\nThis package provides:\n- model_pool: Unified model pool interface (Prime/Lite/Embedding)\n- model_manager: Model lifecycle management\n- v\"\"\"\n\n## FILE: gaia-core/models/_model_pool_impl.py\n# AST Summary (substitute with actual source at training time)\nclass ModelPool():\n    def __init__(self, config: Config = None): ...\n    def register_dev_model(self, name: str): ...\n    def enable_prime_load(self): ...\n    def load_models(self, use_oracle = False): ...\n    def load_prime_only(self, force: bool = False) -> bool: ...\n    def _prime_guard_allows(self, force: bool = False) -> bool: ...\n    def _auto_set_gpu_layers(self): ...\n    def _start_embed_loader(self): ...\n    def _apply_env_model_overrides(self): ...\n    def _ordered_model_keys(self) -> List[str]: ...\n    def _load_model_entry(self, model_name: str, use_oracle: bool = False, force: bool = False) -> bool: ...\n    def _promote_prime_aliases(self): ...\n    def wait_for_embed(self, timeout: float = None): ...\n    def get_embed_model(self, timeout: float = 0, lazy_load: bool = True): ...\n    def prewarm_embed(self, timeout: int = 10) -> bool: ...\n    def ensure_model_loaded(self, name: str, force: bool = False) -> bool: ...\n    def get(self, name: str, lazy_load: bool = True): ...\n    def get_model_for_role(self, role: str, lazy_load: bool = True): ...\n    def list_models(self): ...\n    def set_status(self, name: str, status: str): ...\n    def get_idle_model(self, exclude = []): ...\n    def acquire_model(self, name: str, lazy_load: bool = True): ...\n    def release_model(self, name: str): ...\n    def release_model_for_role(self, role: str): ...\n    def _resolve_model_name_for_role(self, role: str) -> str | None: ...\n    def acquire_model_for_role(self, role: str, lazy_load: bool = True): ...\n    def forward_to_model(self, role: str, messages: list, release: bool = True, **kwargs): ...\n    def get_active_persona(self) -> PersonaAdapter: ...\n    def set_persona(self, persona): ...\n    def complete(self, name: str, prompt: str, max_tokens: int = 128, temperature: float = 0.2) -> str: ...\n    def shutdown(self) -> None: ...\n\ndef _get_sentence_transformer(): ...\ndef _read_manifest(path: Path) -> dict: ...\ndef _ensure_download(role: str, spec: dict, models_dir: Path, scripts_dir: Path, allow_autosetup: bool) -> Path: ...\ndef resolve_model_paths(config: Config) -> dict: ...\ndef _get_gpu_free_total_bytes() -> tuple: ...\ndef _choose_initial_n_gpu(desired_n_gpu: int, free_bytes: int | None) -> int: ...\n\n## FILE: gaia-core/models/dev_model.py\n# AST Summary (substitute with actual source at training time)\nclass DevModel():\n    \"\"\"A mock model that prints the prompt to the console and waits for user input.\"\"\"\n    def __init__(self, name = 'dev_model'): ...\n    def create_chat_completion(self, messages, **kwargs): ...\n\n\n## FILE: gaia-core/models/document.py\n# AST Summary (substitute with actual source at training time)\nclass DocumentProcessor():\n    \"\"\"Handles loading, preprocessing, and converting documents into structured markdown or LangChain docum\"\"\"\n    def __init__(self, config, llm = None): ...\n    def extract_text_from_file(self, filepath: str) -> Optional[str]: ...\n    def _extract_rtf(self, filepath: str) -> Optional[str]: ...\n    def _extract_docx(self, filepath: str) -> Optional[str]: ...\n    def convert_to_markdown(self, text: str) -> Optional[str]: ...\n    def save_markdown(self, filepath: str, content: str) -> bool: ...\n    def load_and_preprocess_data(self, data_path: str) -> List[Document]: ...\n    def process_raw_data(self) -> None: ...\n    def get_document_info(self, filepath: str): ...\n    def process_documents(self, directory: str, tier: Optional[str] = None, project: Optional[str] = None) -> List[Document]: ...\n    def embed_documents(self) -> int: ...\n    def generate_artifacts(self) -> int: ...\n\n\n## FILE: gaia-core/models/fine_tune_gaia.py\n# AST Summary (substitute with actual source at training time)\ndef run_fine_tuning(config: Config, dataset_path: str, base_model_name: str, new_model_name: str): ...\ndef main(): ...\n\n## FILE: gaia-core/models/gemini_model.py\n# AST Summary (substitute with actual source at training time)\nclass GeminiAPIModel():\n    \"\"\"Minimal Gemini chat wrapper using the REST API.\nExpects GOOGLE_API_KEY in env. Model name is taken f\"\"\"\n    def __init__(self, model_name: str, api_key: str): ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int, temperature: float, top_p: float, stream: bool = False, **kwargs): ...\n\n\n## FILE: gaia-core/models/groq_model.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Groq API model wrapper for GAIA.\n\nGroq provides free, fast inference on open-source models via their custom LPU hardware.\nThis wrapper provides an OpenAI-compatible interface for use as a fallback whe\"\"\"\nclass GroqAPIModel():\n    \"\"\"Groq API wrapper providing create_chat_completion interface.\n\nCompatible with GAIA's model pool and \"\"\"\n    def __init__(self, model_name: str = None, api_key: str = None, timeout: int = None): ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int = 1024, temperature: float = 0.7, top_p: float = 0.95, stream: bool = False, **kwargs) -> Dict[str, Any] | Generator[Dict[str, Any], None, None]: ...\n    def _sanitize_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, str]]: ...\n    def _stream_response(self, response_stream, start_duration: float) -> Generator[Dict[str, Any], None, None]: ...\n    def _log_usage(self, response, duration: float): ...\n    def get_stats(self) -> Dict[str, Any]: ...\n    def list_models(cls) -> Dict[str, Dict]: ...\n\ndef _ensure_groq_imported(): ...\n\n## FILE: gaia-core/models/hf_model.py\n# AST Summary (substitute with actual source at training time)\nclass HFModel():\n    \"\"\"A thin wrapper around Hugging Face transformers for text generation.\n\nProvides a create_completion(p\"\"\"\n    def __init__(self, model_ref: str, local_path: str = None, device_map: Optional[str] = 'auto', torch_dtype = None, prompt_config: Optional[Dict] = None): ...\n    def _messages_to_prompt(self, messages: List[Dict[str, Any]]) -> str: ...\n    def create_completion(self, prompt: str, max_tokens: int = 128, temperature: float = 0.2, **kwargs): ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int = 128, temperature: float = 0.2, **kwargs): ...\n\n\n## FILE: gaia-core/models/mcp_proxy_model.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"MCP-backed model adapter.\n\nImplements a minimal model-like interface expected by ModelPool consumers.\nDelegates requests to the configured MCP-Lite server via JSON-RPC.\"\"\"\nclass MCPProxyModel():\n    def __init__(self, config = None, role_name: str = 'prime'): ...\n    def _call_rpc(self, method: str, params: Dict) -> Dict: ...\n    def create_chat_completion(self, messages: List[Dict], **kwargs) -> Dict: ...\n    def create_completion(self, prompt: str, **kwargs) -> Dict: ...\n    def __repr__(self): ...\n\n\n## FILE: gaia-core/models/model_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"ModelManager: a small spine to query model status, ensure models are loaded\nand provide a safe spawn-based fallback for loading Prime (vLLM) when direct\nin-process load fails due to CUDA/multiprocessi\"\"\"\nclass ModelManager():\n    \"\"\"Singleton-ish manager around the existing model_pool.\n\nIt does not itself host models; instead it de\"\"\"\n    def __new__(cls): ...\n    def __init__(self): ...\n    def _get_pool(self): ...\n    def ensure_prime_loaded(self, force: bool = False, timeout: int = 120) -> Dict[str, Any]: ...\n    def call_model(self, role: str, *args, **kwargs) -> Dict[str, Any]: ...\n\ndef _model_manager_child_loader(q, force_flag): ...\ndef get_manager() -> ModelManager: ...\n\n## FILE: gaia-core/models/model_pool.py\n# AST Summary (substitute with actual source at training time)\ndef get_model_pool(): ...\n\n## FILE: gaia-core/models/oracle_model.py\n# AST Summary (substitute with actual source at training time)\nclass GPTAPIModel():\n    def __init__(self, model_alias: str = 'oracle_openai', config: Config = None): ...\n    def create_chat_completion(self, messages, max_tokens, temperature, top_p, stream = False): ...\n    def _stream_response(self, response_stream): ...\n    def _log_token_usage(self, response): ...\n\n\n## FILE: gaia-core/models/tts.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Text-to-speech module for GAIA D&D Campaign Assistant.\nHandles all speech synthesis functionality.\"\"\"\nclass SpeechManager():\n    \"\"\"Manages text-to-speech functionality.\"\"\"\n    def __init__(self, config): ...\n    def initialize(self) -> bool: ...\n    def _select_voice(self, voices: List) -> None: ...\n    def speak(self, text: str) -> None: ...\n    def stop(self) -> None: ...\n    def set_properties(self, rate: Optional[int] = None, volume: Optional[float] = None) -> None: ...\n\n\n## FILE: gaia-core/models/vector_store.py\n# AST Summary (substitute with actual source at training time)\nclass VectorStoreManager():\n    def __init__(self, config): ...\n    def initialize_store(self): ...\n    def persist(self): ...\n    def delete_all_documents(self): ...\n    def as_retriever(self): ...\n    def add_documents(self, documents: List[Document]): ...\n    def split_and_embed_documents(self, raw_documents: List[str], source: Optional[str] = None): ...\n\n\n## FILE: gaia-core/models/vllm_model.py\n# AST Summary (substitute with actual source at training time)\nclass LoRAAdapterInfo():\n    \"\"\"Metadata for a loaded LoRA adapter.\"\"\"\n\nclass VLLMChatModel():\n    \"\"\"Thin wrapper around vLLM that exposes GAIA's create_chat_completion interface.\n\nParameters are sourc\"\"\"\n    def __init__(self, model_config: Dict[str, Any], global_config, gpu_info: Optional[Tuple[Optional[int], Optional[int]]] = None): ...\n    def _int_from_config(self, key: str, env_override: Optional[str], default: int) -> int: ...\n    def _resolve_gpu_utilization(self) -> float: ...\n    def create_completion(self, prompt: str, max_tokens: int = 128, temperature: float = 0.2, top_p: float = 0.9, presence_penalty: float = 0.0, stream: bool = False, stop: Optional[Iterable[str]] = None, **kwargs): ...\n    def _create_chat_completion_simple(self, messages: List[Dict[str, Any]], max_tokens: int = 2048, temperature: float = 0.7, top_p: float = 0.95, **kwargs): ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int = 128, temperature: float = 0.2, top_p: float = 0.9, stream: bool = False, stop: Optional[Iterable[str]] = None, **kwargs): ...\n    def _messages_to_prompt(self, messages: List[Dict[str, Any]]) -> str: ...\n    def _build_sampling_params(self, max_tokens: int, temperature: float, top_p: float, stop: Optional[Iterable[str]], presence_penalty: float = 0.0, repetition_penalty: float = 1.0, frequency_penalty: float = 0.0) -> SamplingParams: ...\n    def shutdown(self) -> None: ...\n    def _stream_text(self, prompts: List[str], sampling_params: SamplingParams) -> Generator[Dict[str, Any], None, None]: ...\n    def _native_stream_text(self, prompts: List[str], sampling_params: SamplingParams) -> Iterator[Dict[str, Any]]: ...\n    def _chunk_text(self, text: str, chunk_size: int = 96) -> Iterator[str]: ...\n    def _extract_text(self, outputs) -> str: ...\n    def _summarize_outputs(self, outputs) -> str: ...\n    def lora_enabled(self) -> bool: ...\n    def load_adapter(self, name: str, path: str, tier: int = 3) -> bool: ...\n    def unload_adapter(self, name: str) -> bool: ...\n    def get_loaded_adapters(self) -> List[LoRAAdapterInfo]: ...\n    def get_adapter(self, name: str) -> Optional[LoRAAdapterInfo]: ...\n    def create_lora_request(self, adapter_name: str) -> Optional[Any]: ...\n    def generate_with_adapter(self, prompts: List[str], adapter_name: str, sampling_params: Optional[Any] = None, max_tokens: int = 2048, temperature: float = 0.7, top_p: float = 0.95, **kwargs) -> List[Any]: ...\n    def create_chat_completion_with_adapter(self, messages: List[Dict[str, Any]], adapter_name: str, max_tokens: int = 2048, temperature: float = 0.7, top_p: float = 0.95, **kwargs) -> Dict[str, Any]: ...\n\n\n## FILE: gaia-core/models/vllm_remote_model.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Remote vLLM model backend for GAIA.\n\nHTTP client for a standalone vLLM OpenAI-compatible API server (gaia-prime).\nReplaces in-process VLLMChatModel when PRIME_ENDPOINT is set, allowing\ngaia-core to of\"\"\"\nclass VLLMRemoteModel():\n    \"\"\"HTTP client for a remote vLLM OpenAI-compatible API server.\n\nProvides the same public interface as V\"\"\"\n    def __init__(self, model_config: dict, global_config = None, **kwargs): ...\n    def create_completion(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.95, stop: Optional[List[str]] = None, stream: bool = False, **kwargs) -> Dict[str, Any] | Generator[Dict[str, Any], None, None]: ...\n    def create_chat_completion(self, messages: List[Dict[str, Any]], max_tokens: int = 1024, temperature: float = 0.7, top_p: float = 0.95, stream: bool = False, **kwargs) -> Dict[str, Any] | Generator[Dict[str, Any], None, None]: ...\n    def set_active_adapter(self, adapter_name: Optional[str]): ...\n    def create_chat_completion_with_adapter(self, adapter_name: str, messages: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]: ...\n    def health_check(self) -> bool: ...\n    def shutdown(self): ...\n    def _resolve_model_field(self) -> str: ...\n    def _post(self, path: str, payload: dict) -> dict: ...\n    def _stream_completions(self, payload: dict) -> Generator[Dict[str, Any], None, None]: ...\n    def _stream_chat(self, payload: dict) -> Generator[Dict[str, Any], None, None]: ...\n    def _sanitize_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, str]]: ...\n    def _log_usage(self, resp: dict, duration: float): ...\n\n\n## FILE: gaia-core/utils/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia_core.utils - Utility modules for the GAIA cognitive engine.\n\nThis package provides:\n- prompt_builder: Construct prompts from CognitionPackets\n- packet_builder: Build and manipulate CognitionPacke\"\"\"\n\n## FILE: gaia-core/utils/dev_matrix_analyzer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"DevMatrixAnalyzer - Analyzes task completion status for GAIA's dev_matrix.\n\nThis module provides automated task completion detection for GAIA's self-development\nroadmap. Each task type has specific ve\"\"\"\nclass DevMatrixAnalyzer():\n    \"\"\"Analyzes and updates dev_matrix task completion status.\n\nUses file-based verification instead of she\"\"\"\n    def __init__(self, config): ...\n    def analyze_and_update(self) -> List[Dict]: ...\n    def is_task_completed(self, task: Dict) -> bool: ...\n    def get_task_status_report(self) -> Dict: ...\n    def _verify_discord_integration(self) -> bool: ...\n    def _verify_thought_seed_tooling(self) -> bool: ...\n    def _verify_self_reflection(self) -> bool: ...\n    def _verify_gcp_fragmentation(self) -> bool: ...\n\ndef analyze_dev_matrix(config) -> Dict: ...\n\n## FILE: gaia-core/utils/dev_matrix_utils.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"dev_matrix_utils.py\n- Utilities for loading, diffing, and updating dev_matrix.json\n- Enforces absolute path, atomic writes, and audit logging\n- Designed for use in self-review and approval flows\"\"\"\ndef load_dev_matrix(path: Path = DEV_MATRIX_PATH) -> Any: ...\ndef save_dev_matrix(data: Any, path: Path = DEV_MATRIX_PATH) -> None: ...\ndef diff_dev_matrix(old: Any, new: Any) -> str: ...\ndef mark_task_complete(task_key: str, prompt: str = None, path: Path = DEV_MATRIX_PATH) -> Tuple[Any, Any, str]: ...\n\n## FILE: gaia-core/utils/gaia_rescue_helper.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA Rescue Helper (production copy)\n------------------------------------\n- Central, Config-safe utilities used by the router and rescue shell\n- Provides GAIARescueHelper class (expected by output_rou\"\"\"\nclass GAIARescueHelper():\n    \"\"\"Config-aware façade for:\n  - Blueprints / Cheatsheets under knowledge/system_reference/...\n  - Sketc\"\"\"\n    def __init__(self, config: Config, llm: Optional[Any] = None): ...\n    def load_blueprint(self, blueprint_id: str) -> str: ...\n    def load_cheatsheet(self, cheatsheet_id: str) -> str: ...\n    def sketchpad_write(self, title: str, content: str) -> str: ...\n    def sketchpad_read(self, key: str = '') -> str: ...\n    def sketchpad_clear(self) -> str: ...\n    def _load_fragments_store(self) -> Dict[str, Any]: ...\n    def _save_fragments_store(self, data: Dict[str, Any]) -> bool: ...\n    def fragment_write(self, parent_request_id: str, sequence: int, content: str, continuation_hint: str = '', is_complete: bool = False, token_count: int = 0) -> Dict[str, Any]: ...\n    def fragment_read(self, parent_request_id: str) -> List[Dict[str, Any]]: ...\n    def fragment_assemble(self, parent_request_id: str, seam_overlap_check: bool = True) -> Dict[str, Any]: ...\n    def fragment_clear(self, parent_request_id: Optional[str] = None) -> str: ...\n    def fragment_list_pending(self) -> List[str]: ...\n    def _load_memory_store(self) -> Dict[str, Any]: ...\n    def _write_memory_store(self, data: Dict[str, Any]) -> None: ...\n    def remember_fact(self, key: str, value: str, note: str = '') -> str: ...\n    def recall_fact(self, key: str = '', limit: int = 5) -> str: ...\n    def get_recent_facts(self, limit: int = 5) -> List[Dict[str, str]]: ...\n    def queue_thought_seed(self, prompt: str, note: str = '', priority: str = 'normal') -> str: ...\n    def run_shell_safe(self, command: str) -> str: ...\n    def buffer_and_execute_shell(self, content: str) -> None: ...\n    def _validate_read_path(self, path: str) -> str: ...\n    def code_read(self, path: str) -> Dict[str, Any]: ...\n    def code_span(self, path: str, start: int, end: int) -> Dict[str, Any]: ...\n    def code_symbol(self, path: str, symbol: str) -> Dict[str, Any]: ...\n    def code_summarize(self, src: Dict[str, Any], max_tokens: int = 256) -> Dict[str, Any]: ...\n\ndef _safe_read_text(path: Path) -> str: ...\ndef _find_first_with_ext(base: Path, stem: str, exts: tuple[str, ...]) -> Optional[Path]: ...\ndef _helper() -> GAIARescueHelper: ...\ndef sketch(title: str, content: str) -> str: ...\ndef sketchpad_write(title: str, content: str) -> str: ...\ndef show_sketchpad(key: str = '') -> str: ...\ndef sketchpad_read(key: str = '') -> str: ...\ndef clear_sketchpad() -> str: ...\ndef sketchpad_clear() -> str: ...\ndef load_blueprint(blueprint_id: str) -> str: ...\ndef load_cheatsheet(cheatsheet_id: str) -> str: ...\ndef queue_thought_seed(prompt: str, note: str = '', priority: str = 'normal') -> str: ...\ndef run_shell_safe(command: str) -> str: ...\ndef buffer_and_execute_shell(content: str) -> None: ...\ndef code_read(path: str) -> Dict[str, Any]: ...\ndef code_span(path: str, start: int, end: int) -> Dict[str, Any]: ...\ndef code_symbol(path: str, symbol: str) -> Dict[str, Any]: ...\ndef code_summarize(src: Dict[str, Any], max_tokens: int = 256) -> Dict[str, Any]: ...\ndef remember_fact(key: str, value: str, note: str = '') -> str: ...\ndef recall_fact(key: str = '', limit: int = 5) -> str: ...\ndef get_recent_facts(limit: int = 5) -> List[Dict[str, str]]: ...\n\n## FILE: gaia-core/utils/mcp_client.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA MCP-Lite Client\n\nThis module is responsible for dispatching sidecar actions from a CognitionPacket\nto the MCP-lite server.\"\"\"\ndef _normalize_endpoint(ep: str) -> str: ...\ndef call_jsonrpc(method: str, params: Dict, endpoint: str = None, timeout: int = 20) -> Dict: ...\ndef dispatch_sidecar_actions(packet: CognitionPacket, config: Config) -> List[Dict]: ...\ndef ai_read(path: str) -> Dict: ...\ndef ai_write(path: str, content: str) -> Dict: ...\ndef ai_execute(command: str, timeout: int = 30, shell: bool = False, dry_run: bool = False) -> Dict: ...\ndef embedding_query(query: str, top_k: int = 5, knowledge_base_name: str = 'system') -> Dict: ...\ndef request_approval_via_mcp(method: str, params: Dict) -> Dict: ...\ndef approve_action_via_mcp(action_id: str, approval: str) -> Dict: ...\ndef get_pending_action(action_id: str) -> Dict: ...\ndef discover(endpoint: str = None, timeout: int = 3) -> Dict: ...\n\n## FILE: gaia-core/utils/output_router.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Output Router - Central hub for parsing and dispatching all directives from LLM output.\n\nHandles:\n- Parsing LLM output into CognitionPacket structures\n- Safety gate checking before execution\n- Sidecar\"\"\"\ndef _strip_think_tags_robust(text: str) -> str: ...\ndef _strip_stray_cjk(text: str) -> str: ...\ndef _get_destination_registry(): ...\ndef route_output(response_text: str, packet: CognitionPacket, ai_manager, session_id: str, destination: str) -> Dict[str, Any]: ...\ndef _legacy_destination_to_enum(destination: str) -> Optional[OutputDestination]: ...\ndef _strip_gcp_metadata(text: str) -> str: ...\ndef _parse_llm_output_into_packet(response_text: str, packet: CognitionPacket): ...\n\n## FILE: gaia-core/utils/packet_builder.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Utility to build small CognitionPacket snapshots for grounding summarization.\n\nProduces a compact dict snapshot (not a full CognitionPacket instance) which\nmatches the shape expected by `ConversationS\"\"\"\ndef build_packet_snapshot(session_id: str, persona_id: str, original_prompt: str, history: List[Dict[str, Any]] = None, mcp_info: Optional[str] = None) -> Dict[str, Any]: ...\n\n## FILE: gaia-core/utils/packet_templates.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Helpers for rendering GAIA Cognition Packets (GCP) as structured templates.\n\nThe template is intentionally compact: only populated sections are emitted,\nand lengthy text fields are trimmed so the rend\"\"\"\ndef _trim(value: Any, limit: int = MAX_INLINE_LEN) -> Any: ...\ndef _clean_dict(payload: Dict[str, Any]) -> Dict[str, Any]: ...\ndef packet_to_template_dict(packet: CognitionPacket, processed_data_field_keys: Optional[set] = None) -> Dict[str, Any]: ...\ndef render_gaia_packet_template(packet: CognitionPacket, indent: str = '  ', processed_data_field_keys: Optional[set] = None, sections: Optional[tuple] = None) -> str: ...\n\n## FILE: gaia-core/utils/prompt_builder.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Prompt Builder (robust, persona/context-aware)\n- Assembles the LLM prompt with identity, persona, context, constraints, history, and memory.\n- Actively manages the token budget to prevent context over\"\"\"\ndef build_from_packet(packet: CognitionPacket, task_instruction_key: str = None) -> List[Dict]: ...\ndef _build_prompt_core(config, persona_instructions: str, session_id: str, history: List[Dict], user_input: str, task_instruction: str = None, token_budget: int = 4096, packet: 'CognitionPacket' = None) -> List[Dict]: ...\ndef build_prompt(*args, **kwargs): ...\n\n## FILE: gaia-core/utils/resource_monitor.py\n# AST Summary (substitute with actual source at training time)\nclass ResourceMonitor():\n    def __new__(cls, *args, **kwargs): ...\n    def _ensure_nvml(): ...\n    def __init__(self, poll_interval: int = 5): ...\n    def start(self): ...\n    def stop(self): ...\n    def _monitor(self): ...\n    def is_distracted(self) -> bool: ...\n    def check_and_clear_distracted(self) -> bool: ...\n    def get_instance(cls, *args, **kwargs): ...\n\ndef shutdown_monitor(): ...\n\n## FILE: gaia-core/utils/stream_observer.py\n# AST Summary (substitute with actual source at training time)\nclass Interrupt():\n\nclass StreamObserver():\n    def __init__(self, config: Config, llm, name: str = 'AgentCore-Observer'): ...\n    def observe(self, packet: Optional[CognitionPacket], output: str) -> Interrupt: ...\n    def fast_check(self, buffer: str) -> bool: ...\n    def check_response_quality(response: str, user_prompt: str) -> Optional[Interrupt]: ...\n    def _verify_citations_against_rag(self, output: str, packet) -> Dict: ...\n    def _validate_code_paths(self, text_content: str) -> List[Dict]: ...\n    def verify_side_effects(self, packet: Optional[CognitionPacket], route_result: Dict[str, Any], llm_output: str = '') -> Interrupt: ...\n\n\n## FILE: gaia-core/utils/temporal_context.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Temporal Context Builder — assembles a rich temporal snapshot for prompt injection.\n\nProduces a concise text block (~100-150 tokens) injected into the system prompt\nso GAIA has awareness of time, her \"\"\"\ndef build_temporal_context(timeline_store: Optional[Any] = None, sleep_manager_status: Optional[Dict[str, Any]] = None, session_id: Optional[str] = None, session_created_at: Optional[datetime] = None, session_message_count: int = 0, last_message_ts: Optional[datetime] = None, code_evolution_path: str = ...) -> str: ...\ndef _semantic_time(dt: datetime) -> str: ...\ndef _format_duration(seconds: float) -> str: ...\ndef _wake_cycle_summary(timeline_store: Any) -> str: ...\ndef _session_summary(session_id: Optional[str], session_created_at: Optional[datetime], message_count: int, last_message_ts: Optional[datetime]) -> str: ...\ndef _activity_summary(timeline_store: Any) -> str: ...\ndef _state_summary(status: Dict[str, Any]) -> str: ...\ndef _code_evolution_summary(path: str) -> str: ...\n\n## FILE: gaia-core/utils/world_state.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Lightweight dynamic world-state snapshot for prompts.\n\nThis module intentionally avoids heavy dependencies. It gathers a short,\nbounded view of:\n- Clock/uptime\n- Host load/memory (coarse)\n- Active mod\"\"\"\ndef _uptime_seconds() -> float: ...\ndef _mem_summary() -> str: ...\ndef _load_avg() -> str: ...\ndef _model_paths() -> Dict[str, str]: ...\ndef _mcp_tools_sample(limit: int = 6) -> List[str]: ...\ndef _mcp_tools_full(limit: int = 50) -> List[str]: ...\ndef world_state_snapshot() -> Dict: ...\ndef world_state_detail() -> Dict: ...\ndef _capability_affordances(tools: List[str]) -> List[str]: ...\ndef format_world_state_snapshot(max_lines: int = 12, output_context: Dict = None) -> str: ...\n", "fidelity": 0.75, "weight": 1.0}
{"pair_id": "09e9dcf2-8aea-4a16-8559-c708bbba86b0", "service_id": "gaia-mcp", "pair_type": "retroactive", "instruction": "You are GAIA's code-architect. Generate Python source code that faithfully\nimplements the following blueprint. Your output must satisfy all contract,\ndependency, failure mode, and intent specifications exactly.\n\nBLUEPRINT:\narchitecture:\n  components:\n  - consumes_interfaces: []\n    description: 'FastAPI application implementing the MCP (Model Context Protocol)\n      JSON-RPC dispatch. Receives tool call requests from gaia-core, validates parameters,\n      routes to the appropriate tool implementation, and returns structured results.\n      Implements the /jsonrpc POST endpoint and root discovery endpoint.\n\n      '\n    exposes_interfaces:\n    - jsonrpc\n    - health\n    - root\n    id: jsonrpc_server\n    key_classes: []\n    key_functions:\n    - create_app()\n    - read_root()\n    label: JSON-RPC Server\n    source_files:\n    - candidates/gaia-mcp/gaia_mcp/server.py\n    - candidates/gaia-mcp/gaia_mcp/main.py\n  - consumes_interfaces: []\n    description: 'Tool catalog and implementations. Provides list_tools() for discovery\n      and describe_tool() for schema inspection. Implements file I/O tools (read,\n      write, list, find), memory tools (query, status, rebuild_index), fragment assembly\n      tools (write, read, assemble, list_pending, clear), and the AI-assisted write\n      tool that uses the rescue helper for context-aware file operations.\n\n      '\n    exposes_interfaces: []\n    id: tool_registry\n    key_classes: []\n    key_functions:\n    - list_tools()\n    - describe_tool()\n    - _ai_write_impl()\n    - _read_file_impl()\n    - _write_file_impl()\n    - _memory_query_impl()\n    - _find_relevant_documents()\n    - _fragment_assemble_impl()\n    label: Tool Registry\n    source_files:\n    - candidates/gaia-mcp/gaia_mcp/tools.py\n  - consumes_interfaces: []\n    description: 'Security-critical action approval workflow. Generates time-limited\n      challenges for sensitive operations (file writes, shell execution). ApprovalStore\n      tracks pending actions with TTL-based expiry. Challenge-response pattern prevents\n      automated bypass of the approval requirement.\n\n      '\n    exposes_interfaces:\n    - request_approval\n    - approval_status\n    id: approval_gate\n    key_classes:\n    - ApprovalStore\n    key_functions:\n    - ApprovalStore.create_pending()\n    - ApprovalStore.approve()\n    - ApprovalStore.cancel()\n    - ApprovalStore.cleanup_expired()\n    label: Approval Gate\n    source_files:\n    - candidates/gaia-mcp/gaia_mcp/approval.py\n  - consumes_interfaces: []\n    description: 'Web access tools with domain trust tier system. SourceTrustConfig\n      classifies domains into trust tiers (trusted, standard, blocked). Rate limiter\n      prevents abuse. Implements DuckDuckGo search and HTTP content fetching with\n      HTML-to-text extraction. Content is filtered by domain trust level before returning\n      to the caller.\n\n      '\n    exposes_interfaces: []\n    id: web_tools\n    key_classes:\n    - SourceTrustConfig\n    - _RateLimiter\n    key_functions:\n    - web_search()\n    - web_fetch()\n    - SourceTrustConfig.tier_for_domain()\n    - SourceTrustConfig.is_allowed()\n    label: Web Search & Fetch\n    source_files:\n    - candidates/gaia-mcp/gaia_mcp/web_tools.py\n  edges:\n  - data_flow: \"method name + params \\u2192 result\"\n    from_component: jsonrpc_server\n    label: dispatches tool calls to\n    to_component: tool_registry\n    transport: function_call\n  - data_flow: action_type, description, params\n    from_component: jsonrpc_server\n    label: checks approval for sensitive operations\n    to_component: approval_gate\n    transport: function_call\n  - data_flow: \"pending action \\u2192 challenge\"\n    from_component: tool_registry\n    label: requests approval before write/execute\n    to_component: approval_gate\n    transport: function_call\n  - data_flow: \"search query or URL \\u2192 content\"\n    from_component: tool_registry\n    label: delegates web_search/web_fetch calls\n    to_component: web_tools\n    transport: function_call\ndependencies:\n  external_apis:\n  - name: duckduckgo\n    purpose: web_search_and_fetch\n    required: false\n  - name: discord\n    purpose: webhook_messaging\n    required: false\n  services:\n  - fallback: null\n    id: gaia-study\n    required: false\n    role: lora_training_and_adapter_management\n  volumes:\n  - access: ro\n    mount_path: /gaia-common\n    name: gaia-common\n    purpose: Shared library (tools registry, config, utils)\n  - access: rw\n    mount_path: /knowledge\n    name: knowledge\n    purpose: Tool reference docs, system knowledge base, vector index\n  - access: ro\n    mount_path: /models\n    name: gaia-models\n    purpose: LoRA adapters, embedding models\n  - access: rw\n    mount_path: /sandbox\n    name: gaia-sandbox\n    purpose: Isolated workspace for tool execution\nfailure_modes:\n- auto_recovers: false\n  condition: Tool execution error\n  response: JSON-RPC error response with code -32603 and traceback in data field\n  severity: degraded\n- auto_recovers: true\n  condition: Approval timeout (>900s default TTL)\n  response: Pending action expires; client must request approval again\n  severity: degraded\n- auto_recovers: false\n  condition: File path not in allowlist\n  response: ValueError; JSON-RPC error -32603 'Path not allowed'\n  severity: degraded\n- auto_recovers: true\n  condition: Rate limit exceeded (web_search/web_fetch)\n  response: Returns ok=false with remaining count; auto-recovers after sliding window\n  severity: degraded\n- auto_recovers: false\n  condition: Blocked or unknown domain in web_fetch\n  response: Returns ok=false with domain policy error\n  severity: degraded\n- auto_recovers: false\n  condition: gaia-study unavailable\n  response: study_* and adapter_* tools return ok=false with error message\n  severity: degraded\n- auto_recovers: false\n  condition: Sensitive tool without prior approval\n  response: HTTP 403 with JSON-RPC error directing to /request_approval\n  severity: degraded\nid: gaia-mcp\nintent:\n  cognitive_role: The Hands\n  design_decisions:\n  - Separate service decouples tool execution from cognition, keeping gaia-core CPU-only\n  - Path allowlisting restricts file ops to /knowledge, /sandbox, /models; symlink\n    traversal blocked via .resolve()\n  - Approval flow enforces human-in-the-loop via 5-char challenge reversal; time-boxed\n    to configurable TTL\n  - Domain allowlisting gates web research to trusted/reliable tiers; blocked list\n    prevents social media leakage\n  - Rate limiting (20 searches/hr, 50 fetches/hr) via in-memory sliding window prevents\n    abuse\n  - Study gateway pattern (async await) enables LoRA adapter management while preserving\n    tool isolation\n  - Fragment assembly with overlap checking enables long responses split across token\n    limits\n  - JSON-RPC 2.0 for lightweight tool dispatch without REST overhead\n  - 'Graceful degradation: missing study, blocked domains, rate limits return structured\n    errors not exceptions'\n  open_questions:\n  - Should approval TTL be configurable per-tool (e.g., ai_write longer than run_shell)?\n  - Should rate limits be per-minute with burst allowance instead of per-hour?\n  - Should approval store persist to disk across container restarts for auditability?\n  purpose: 'Sandboxed execution layer enforcing security boundaries around GAIA''s\n    tool calls. Acts as both a security perimeter (preventing unauthorized file/shell\n    access via path allowlisting) and an approval gate (requiring human confirmation\n    for destructive operations). Delegation to a separate service allows gaia-core\n    to remain interface-agnostic and CPU-only.\n\n    '\ninterfaces:\n- description: Container health check endpoint.\n  direction: inbound\n  id: health\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\n- description: \"Root endpoint \\u2014 returns running status message.\"\n  direction: inbound\n  id: root\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /\n    type: http_rest\n- description: 'Primary tool dispatch via JSON-RPC 2.0. Handles ~30 tool methods:\n    file I/O (read_file, write_file, ai_write), shell (run_shell), memory/vector (memory_query,\n    memory_rebuild_index), web research (web_search, web_fetch), fragments, study\n    gateway, Discord messaging.'\n  direction: inbound\n  id: jsonrpc_dispatch\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /jsonrpc\n    type: http_rest\n- description: Create pending approval for sensitive tool (ai_write, write_file, run_shell,\n    memory_rebuild_index). Returns action_id + 5-char challenge.\n  direction: inbound\n  id: request_approval\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /request_approval\n    type: http_rest\n- description: List all pending approvals awaiting human confirmation.\n  direction: inbound\n  id: pending_approvals\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /pending_approvals\n    type: http_rest\n- description: Approve pending action by providing reversed 5-char challenge. Dispatches\n    underlying tool on success.\n  direction: inbound\n  id: approve_action\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /approve_action\n    type: http_rest\n- description: Async HTTP gateway to gaia-study for LoRA training and adapter management.\n  direction: outbound\n  id: study_gateway\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /study/start\n    type: http_rest\n- description: 'External web research via DuckDuckGo (ddgs library with Instant Answer\n    API fallback). Rate-limited: 20 searches/hr, 50 fetches/hr.'\n  direction: outbound\n  id: web_research\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /\n    type: http_rest\n- description: Optional Discord webhook for send_discord_message tool. Requires DISCORD_WEBHOOK_URL\n    env var.\n  direction: outbound\n  id: discord_webhook\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /\n    type: http_rest\nmeta:\n  blueprint_version: '0.2'\n  confidence:\n    contract: high\n    dependencies: high\n    failure_modes: medium\n    intent: medium\n    runtime: high\n  created_at: '2026-02-21T15:55:35.774388Z'\n  divergence_score: null\n  generated_by: manual_seed\n  genesis: true\n  last_reflected: null\n  promoted_at: null\n  reflection_notes: null\n  schema_version: '1.0'\n  status: candidate\nrole: The Hands (Tool Execution)\nruntime:\n  base_image: python:3.11-slim\n  compose_service: gaia-mcp\n  dockerfile: gaia-mcp/Dockerfile\n  gpu: false\n  gpu_count: null\n  health_check: curl -f http://localhost:8765/health\n  port: 8765\n  security: null\n  startup_cmd: uvicorn gaia_mcp.main:app --host 0.0.0.0 --port 8765\n  user: ${UID}:${GID}\nservice_status: live\nsource_files:\n- file_type: python\n  path: candidates/gaia-mcp/gaia_mcp/main.py\n  role: entrypoint\n- file_type: python\n  path: candidates/gaia-mcp/gaia_mcp/server.py\n  role: core_dispatcher\n- file_type: python\n  path: candidates/gaia-mcp/gaia_mcp/approval.py\n  role: approval_gate\n- file_type: python\n  path: candidates/gaia-mcp/gaia_mcp/web_tools.py\n  role: web_research\n- file_type: dockerfile\n  path: candidates/gaia-mcp/Dockerfile\n  role: build_config\nversion: '0.5'\n\n\nAVAILABLE GAIA IDIOMS (from reference implementations):\n\n### gaia-orchestrator\n  config.py: classes=['OrchestratorConfig'], functions=['load_yaml_config', 'get_config', 'reset_config']\n  docker_manager.py: classes=['DockerManager'], functions=[]\n  gpu_manager.py: classes=['GPUManager'], functions=[]\n  handoff_manager.py: classes=['HandoffManager'], functions=[]\n  health_watchdog.py: classes=['HealthWatchdog'], functions=[]\n  main.py: classes=[], functions=['lifespan', 'health_check', 'root', 'get_status', 'get_gpu_status', '_acquire_gpu_inner', 'acquire_gpu', 'release_gpu', 'wait_for_gpu', 'get_container_status', 'stop_live_stack', 'start_live_stack', 'stop_candidate_stack', 'start_candidate_stack', 'swap_service', 'handoff_prime_to_study', 'handoff_study_to_prime', 'get_handoff_status', 'gpu_sleep', 'gpu_wake', 'notify_oracle_fallback', 'websocket_notifications', 'main']\n  models/schemas.py: classes=['GPUAcquireRequest', 'GPUAcquireResponse', 'GPUMemoryInfo', 'GPUStatus', 'ServiceHealth', 'ContainerStatus', 'ContainerStartRequest', 'ContainerSwapRequest', 'HandoffRequest', 'HandoffStatus', 'OracleNotification', 'Notification', 'OrchestratorState'], functions=[]\n  notification_manager.py: classes=['NotificationManager'], functions=[]\n  state.py: classes=['StateManager'], functions=['get_state_manager', 'reset_state_manager']\n\n### gaia-study\n  indexer.py: classes=['VectorIndexer'], functions=[]\n  main.py: classes=[], functions=['startup_event', 'shutdown_event']\n  qlora_trainer.py: classes=['QLoRAConfig', 'TrainingProgress', 'QLoRATrainer'], functions=['_lazy_import', 'estimate_vram_usage']\n  server.py: classes=['IndexBuildRequest', 'DocumentAddRequest', 'QueryRequest', 'StudyStartRequest', 'AdapterLoadRequest', 'AdapterDeleteRequest'], functions=['get_study_manager', 'create_app']\n  study_mode_manager.py: classes=['TrainingConfig', 'TrainingResult', 'StudyModeManager'], functions=[]\n  training_utils.py: classes=[], functions=['get_base_model_name', 'check_for_training_delta', 'get_next_model_version', 'convert_to_gguf', 'update_training_log']\n\nGenerate the implementation for: gaia-mcp", "output": "## FILE: gaia-mcp/__init__.py\n# AST Summary (substitute with actual source at training time)\n\n## FILE: gaia-mcp/approval.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Approval Store for MCP sensitive actions.\n\nManages pending actions that require human approval before execution.\"\"\"\nclass ApprovalStore():\n    \"\"\"In-memory store for pending actions that require human approval.\n\nEach pending action is stored as:\n\"\"\"\n    def __init__(self, ttl_seconds: int = 900): ...\n    def _gen_challenge(self) -> str: ...\n    def create_pending(self, method: str, params: Dict[str, Any], proposal: Optional[str] = None, allow_pending: bool = False) -> Tuple[str, str, float, float]: ...\n    def list_pending(self) -> List[Dict[str, Any]]: ...\n    def approve(self, action_id: str, provided_challenge: str) -> Dict[str, Any]: ...\n    def cancel(self, action_id: str) -> bool: ...\n    def cleanup_expired(self) -> int: ...\n\n\n## FILE: gaia-mcp/main.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA MCP-Lite Server - Entry Point\n\nExposes GAIA's tools and primitives over a local JSON-RPC 2.0 interface.\nThis provides a secure and audited boundary between cognition and action.\n\nUsage:\n    uvico\"\"\"\ndef startup_event(): ...\ndef shutdown_event(): ...\n\n## FILE: gaia-mcp/server.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA MCP-Lite Server\n\nExposes GAIA's tools and primitives over a local JSON-RPC 2.0 interface.\nThis provides a secure and audited boundary between cognition and action.\"\"\"\ndef get_study_service(): ...\ndef health_check(): ...\ndef create_app() -> FastAPI: ...\ndef dispatch_tool(tool_name: str, params: dict) -> any: ...\ndef _send_discord_message_impl(params: dict) -> dict: ...\ndef _ai_write_impl(params: dict) -> dict: ...\ndef _list_dir_impl(params: dict): ...\ndef _list_files_impl(params: dict): ...\ndef _list_tree_impl(params: dict): ...\ndef _read_file_impl(params: dict): ...\ndef _write_file_impl(params: dict) -> dict: ...\ndef _memory_status_impl(params: dict): ...\ndef _memory_query_impl(params: dict): ...\ndef _memory_rebuild_index_impl(params: dict): ...\ndef _find_files_impl(params: dict): ...\ndef _find_relevant_documents(params: dict): ...\ndef _fragment_write_impl(params: dict) -> dict: ...\ndef _fragment_read_impl(params: dict) -> dict: ...\ndef _fragment_assemble_impl(params: dict) -> dict: ...\ndef _fragment_list_pending_impl(params: dict) -> dict: ...\ndef _fragment_clear_impl(params: dict) -> dict: ...\ndef _study_start_impl(params: dict) -> dict: ...\ndef _study_status_impl(params: dict) -> dict: ...\ndef _study_cancel_impl(params: dict) -> dict: ...\ndef _adapter_list_impl(params: dict) -> dict: ...\ndef _adapter_load_impl(params: dict) -> dict: ...\ndef _adapter_unload_impl(params: dict) -> dict: ...\ndef _adapter_delete_impl(params: dict) -> dict: ...\ndef _adapter_info_impl(params: dict) -> dict: ...\ndef jsonrpc_endpoint(request: Request): ...\ndef read_root(): ...\ndef request_approval(request: Request): ...\ndef pending_approvals(): ...\ndef approve_action(request: Request): ...\n\n## FILE: gaia-mcp/tools.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA MCP Tools - Tool Dispatcher and Implementations.\"\"\"\ndef list_tools() -> List[str]: ...\ndef describe_tool(tool_name: str) -> Dict[str, Any]: ...\ndef execute_tool(method: str, params: Dict, approval_store: ApprovalStore, pre_approved: bool = False) -> Any: ...\ndef _ai_write_impl(params: dict, gaia_helper: GAIARescueHelper) -> dict: ...\ndef _write_file_impl(params: dict) -> dict: ...\ndef _list_dir_impl(params: dict): ...\ndef _list_files_impl(params: dict): ...\ndef _list_tree_impl(params: dict): ...\ndef _read_file_impl(params: dict): ...\ndef _memory_status_impl(params: dict): ...\ndef _memory_query_impl(params: dict): ...\ndef _memory_rebuild_index_impl(params: dict): ...\ndef _find_files_impl(params: dict): ...\ndef _find_relevant_documents(params: dict): ...\ndef _fragment_write_impl(params: dict) -> dict: ...\ndef _fragment_read_impl(params: dict) -> dict: ...\ndef _fragment_assemble_impl(params: dict) -> dict: ...\ndef _fragment_list_pending_impl(params: dict) -> dict: ...\ndef _fragment_clear_impl(params: dict) -> dict: ...\ndef _get_study_client(): ...\ndef _study_start_impl(params: dict) -> dict: ...\ndef _study_status_impl(params: dict) -> dict: ...\ndef _study_cancel_impl(params: dict) -> dict: ...\ndef _adapter_list_impl(params: dict) -> dict: ...\ndef _adapter_load_impl(params: dict) -> dict: ...\ndef _adapter_unload_impl(params: dict) -> dict: ...\ndef _adapter_delete_impl(params: dict) -> dict: ...\ndef _adapter_info_impl(params: dict) -> dict: ...\n\n## FILE: gaia-mcp/web_tools.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA MCP Web Tools — web_search and web_fetch\n\nRead-only tools that give GAIA access to real, verifiable web sources.\nSafety via domain allowlist + rate limits (not SENSITIVE_TOOLS).\"\"\"\nclass SourceTrustConfig():\n    \"\"\"Tiered domain classification: trusted / reliable / blocked / unknown.\n\nLoads overrides from gaia_con\"\"\"\n    def __init__(self, config_override: Optional[Dict] = None): ...\n    def tier_for_domain(self, domain: str) -> str: ...\n    def is_allowed(self, domain: str) -> bool: ...\n    def is_blocked(self, domain: str) -> bool: ...\n    def domains_for_content_type(self, content_type: str) -> List[str]: ...\n    def _base_domain(domain: str) -> str: ...\n\nclass _RateLimiter():\n    \"\"\"Simple per-tool sliding-window rate limiter.\n\nResets on container restart (in-memory only — no Redis\"\"\"\n    def __init__(self, max_calls: int, window_seconds: int): ...\n    def allow(self) -> bool: ...\n    def remaining(self) -> int: ...\n\ndef _get_config() -> Tuple[SourceTrustConfig, _RateLimiter, _RateLimiter]: ...\ndef web_search(params: dict) -> dict: ...\ndef _ddg_search(query: str, max_results: int) -> List[dict]: ...\ndef web_fetch(params: dict) -> dict: ...\ndef _extract_content(html: str, url: str) -> Tuple[str, str]: ...\ndef _extract_title_bs4(html: str) -> str: ...\ndef _domain_from_url(url: str) -> str: ...\n", "fidelity": 0.8, "weight": 1.0}
{"pair_id": "5b5c3ddf-234b-4a71-a7b8-02bde9e48866", "service_id": "gaia-study", "pair_type": "retroactive", "instruction": "You are GAIA's code-architect. Generate Python source code that faithfully\nimplements the following blueprint. Your output must satisfy all contract,\ndependency, failure mode, and intent specifications exactly.\n\nBLUEPRINT:\narchitecture:\n  components:\n  - consumes_interfaces: []\n    description: 'FastAPI application with endpoints for vector index management,\n      GPU handoff signals, QLoRA training lifecycle, and LoRA adapter CRUD. Runs background\n      tasks for index builds and training jobs. Health endpoint reports training state\n      and index statistics.\n\n      '\n    exposes_interfaces:\n    - health\n    - root\n    - build_index\n    - add_document\n    - query_index\n    - index_status\n    - refresh_index\n    - gpu_ready\n    - gpu_release\n    - study_start\n    - study_status\n    - study_cancel\n    - adapter_list\n    - adapter_load\n    - adapter_unload\n    - adapter_delete\n    - adapter_info\n    id: api_layer\n    key_classes:\n    - IndexBuildRequest\n    - QueryRequest\n    - StudyStartRequest\n    - AdapterLoadRequest\n    key_functions:\n    - create_app()\n    - build_index()\n    - add_document()\n    - query_index()\n    - gpu_ready()\n    - gpu_release()\n    - study_start()\n    - study_status()\n    - adapter_list()\n    - adapter_load()\n    label: API Layer\n    source_files:\n    - candidates/gaia-study/gaia_study/server.py\n  - consumes_interfaces: []\n    description: 'Embedding-based vector search engine. VectorIndexer manages per-knowledge-base\n      FAISS indices with lazy-loaded sentence transformer models. Supports incremental\n      document addition, full index rebuilds, and similarity queries with configurable\n      thresholds. Singleton pattern ensures one index instance per knowledge base\n      name.\n\n      '\n    exposes_interfaces: []\n    id: vector_indexer\n    key_classes:\n    - VectorIndexer\n    key_functions:\n    - VectorIndexer.build_index_from_docs()\n    - VectorIndexer.refresh_index()\n    - VectorIndexer.add_document()\n    - VectorIndexer.query()\n    - VectorIndexer.get_status()\n    label: Vector Indexer\n    source_files:\n    - candidates/gaia-study/gaia_study/indexer.py\n  - consumes_interfaces: []\n    description: \"QLoRA (Quantized Low-Rank Adaptation) fine-tuning pipeline. StudyModeManager\\\n      \\ orchestrates the full training lifecycle: content validation \\u2192 data preparation\\\n      \\ \\u2192 LoRA training \\u2192 adapter saving. QLoRATrainer handles model loading,\\\n      \\ 4-bit quantization (BitsAndBytes), LoRA application (PEFT), dataset preparation,\\\n      \\ and training execution. Supports tiered adapter storage (experimental/stable/production)\\\n      \\ with metadata tracking.\\n\"\n    exposes_interfaces: []\n    id: qlora_trainer\n    key_classes:\n    - QLoRAConfig\n    - QLoRATrainer\n    - TrainingProgress\n    - StudyModeManager\n    - StudyModeState\n    - TrainingConfig\n    - TrainingResult\n    key_functions:\n    - QLoRATrainer.setup()\n    - QLoRATrainer.prepare_dataset()\n    - QLoRATrainer.train()\n    - QLoRATrainer.save_adapter()\n    - StudyModeManager.start_training()\n    - StudyModeManager.get_status()\n    - StudyModeManager.cancel_training()\n    - StudyModeManager.list_adapters()\n    - estimate_vram_usage()\n    label: QLoRA Training Engine\n    source_files:\n    - candidates/gaia-study/gaia_study/qlora_trainer.py\n    - candidates/gaia-study/gaia_study/study_mode_manager.py\n    - candidates/gaia-study/gaia_study/training_utils.py\n  edges:\n  - data_flow: \"IndexBuildRequest, QueryRequest \\u2192 results\"\n    from_component: api_layer\n    label: delegates index/query operations\n    to_component: vector_indexer\n    transport: function_call\n  - data_flow: \"StudyStartRequest \\u2192 TrainingResult\"\n    from_component: api_layer\n    label: starts/monitors/cancels training\n    to_component: qlora_trainer\n    transport: function_call\n  - data_flow: document content for sample generation\n    from_component: qlora_trainer\n    label: reads training data from indexed documents\n    to_component: vector_indexer\n    transport: function_call\ndependencies:\n  external_apis: []\n  services: []\n  volumes:\n  - access: ro\n    mount_path: /gaia-common\n    name: gaia-common\n    purpose: Shared library (vector client, utils)\n  - access: rw\n    mount_path: /knowledge\n    name: knowledge\n    purpose: Knowledge base documents for indexing (sole writer to vector indices)\n  - access: rw\n    mount_path: /vector_store\n    name: vector_store\n    purpose: \"Vector store \\u2014 sole writer for FAISS indices\"\n  - access: rw\n    mount_path: /models\n    name: gaia-models\n    purpose: Embedding models, LoRA adapters, base model for training\n  - access: rw\n    mount_path: /shared\n    name: gaia-shared\n    purpose: Shared state volume\n  - access: rw\n    mount_path: /logs\n    name: logs\n    purpose: Consolidated service logs\nfailure_modes:\n- auto_recovers: false\n  condition: Document directory not found\n  response: FileNotFoundError raised; index build fails gracefully\n  severity: degraded\n- auto_recovers: false\n  condition: No valid training samples\n  response: TrainingResult with success=False; adapter not created\n  severity: degraded\n- auto_recovers: false\n  condition: GPU OOM during training\n  response: Training marked failed; CUDA cache cleared\n  severity: partial\n- auto_recovers: false\n  condition: Adapter tier limit exceeded\n  response: HTTPException 409 Conflict\n  severity: degraded\n- auto_recovers: true\n  condition: Content validation failure (forbidden patterns)\n  response: Offending samples skipped with logged warning; training continues with\n    remaining\n  severity: degraded\n- auto_recovers: true\n  condition: Model load failure (missing base model)\n  response: Falls back to simulated training mode for UI/workflow testing\n  severity: degraded\n- auto_recovers: true\n  condition: Vector index corruption\n  response: Returns empty index; rebuild via /index/build\n  severity: degraded\nid: gaia-study\nintent:\n  cognitive_role: The Subconscious\n  design_decisions:\n  - \"Sole writer principle \\u2014 only gaia-study writes to /vector_store and LoRA\\\n    \\ adapter directories\"\n  - \"GPU isolation \\u2014 exclusive GPU access during training via orchestrator-managed\\\n    \\ handoff\"\n  - \"Async background processing \\u2014 long-running ops don't block HTTP responses;\\\n    \\ client polls /study/status\"\n  - 'Tiered adapter architecture: tier 1 (global, protected), tier 2 (user), tier\n    3 (session, ephemeral)'\n  - QLoRA 4-bit quantization reduces VRAM from ~50GB to ~16GB for large models\n  - Gradient checkpointing + paged AdamW (8-bit) further reduce memory footprint\n  - 'Content governance: forbidden pattern detection, size limits, sample count limits'\n  - JSON-based vector store for human readability and debugging; suitable for ~100K\n    documents\n  - Fallback to simulated training when real training dependencies unavailable\n  open_questions:\n  - Should vector store migrate to FAISS or ChromaDB for better performance at scale?\n  - Should CUDA_VISIBLE_DEVICES be dynamic based on orchestrator GPU allocation?\n  - Optimal gradient accumulation steps for RTX 5080 16GB VRAM?\n  purpose: \"Background processing service for GAIA's self-study during sleep cycles.\\\n    \\ Handles vector index building, document embedding, LoRA adapter training via\\\n    \\ QLoRA, and adapter lifecycle management. Sole writer to the vector store and\\\n    \\ LoRA adapter directories \\u2014 all other services read only.\\n\"\ninterfaces:\n- description: Container health check endpoint.\n  direction: inbound\n  id: health\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\n- description: Service status with loaded indexes and statistics.\n  direction: inbound\n  id: status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /status\n    type: http_rest\n- description: Build or rebuild vector index from documents (async background task).\n  direction: inbound\n  id: index_build\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /index/build\n    type: http_rest\n- description: Add single document to existing vector index.\n  direction: inbound\n  id: index_add\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /index/add\n    type: http_rest\n- description: Query index for semantically similar documents.\n  direction: inbound\n  id: index_query\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /index/query\n    type: http_rest\n- description: Get status of a specific knowledge base index.\n  direction: inbound\n  id: index_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /index/{knowledge_base_name}/status\n    type: http_rest\n- description: Reload index from disk.\n  direction: inbound\n  id: index_refresh\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /index/{knowledge_base_name}/refresh\n    type: http_rest\n- description: Signal from orchestrator that GPU is available for training.\n  direction: inbound\n  id: gpu_ready\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /study/gpu-ready\n    type: http_rest\n- description: Signal from orchestrator to release GPU; cancels training, cleans CUDA\n    resources.\n  direction: inbound\n  id: gpu_release\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /study/gpu-release\n    type: http_rest\n- description: 'Start LoRA adapter training from documents (async background task).\n    Supports 3 tiers: global, user, session.'\n  direction: inbound\n  id: study_start\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /study/start\n    type: http_rest\n- description: Get current training status (IDLE, PREPARING, TRAINING, COMPLETE, FAILED).\n  direction: inbound\n  id: study_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /study/status\n    type: http_rest\n- description: Cancel in-progress training.\n  direction: inbound\n  id: study_cancel\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /study/cancel\n    type: http_rest\n- description: List all LoRA adapters, optionally filtered by tier.\n  direction: inbound\n  id: adapter_list\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /adapters\n    type: http_rest\n- description: \"Load adapter for generation (stub \\u2014 requires model pool integration).\"\n  direction: inbound\n  id: adapter_load\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /adapters/load\n    type: http_rest\n- description: \"Unload adapter (stub \\u2014 requires model pool integration).\"\n  direction: inbound\n  id: adapter_unload\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /adapters/unload\n    type: http_rest\n- description: Get detailed adapter metadata.\n  direction: inbound\n  id: adapter_info\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /adapters/{adapter_name}\n    type: http_rest\n- description: Delete adapter (tier 1 protected from deletion).\n  direction: inbound\n  id: adapter_delete\n  status: active\n  transport:\n    input_schema: null\n    method: DELETE\n    output_schema: null\n    path: /adapters/{adapter_name}\n    type: http_rest\nmeta:\n  blueprint_version: '0.2'\n  confidence:\n    contract: high\n    dependencies: high\n    failure_modes: medium\n    intent: medium\n    runtime: high\n  created_at: '2026-02-21T15:55:35.798629Z'\n  divergence_score: null\n  generated_by: manual_seed\n  genesis: true\n  last_reflected: null\n  promoted_at: null\n  reflection_notes: null\n  schema_version: '1.0'\n  status: candidate\nrole: The Subconscious (Background Learning)\nruntime:\n  base_image: nvidia/cuda:12.4.0-devel-ubuntu22.04\n  compose_service: gaia-study\n  dockerfile: gaia-study/Dockerfile\n  gpu: true\n  gpu_count: all\n  health_check: curl -f http://localhost:8766/health\n  port: 8766\n  security: null\n  startup_cmd: uvicorn gaia_study.main:app --host 0.0.0.0 --port 8766\n  user: ${UID}:${GID}\nservice_status: live\nsource_files:\n- file_type: python\n  path: candidates/gaia-study/gaia_study/main.py\n  role: entrypoint\n- file_type: python\n  path: candidates/gaia-study/gaia_study/server.py\n  role: api_factory\n- file_type: python\n  path: candidates/gaia-study/gaia_study/indexer.py\n  role: vector_indexer\n- file_type: python\n  path: candidates/gaia-study/gaia_study/study_mode_manager.py\n  role: training_orchestration\n- file_type: python\n  path: candidates/gaia-study/gaia_study/qlora_trainer.py\n  role: qlora_training\n- file_type: python\n  path: candidates/gaia-study/gaia_study/training_utils.py\n  role: training_utilities\n- file_type: dockerfile\n  path: candidates/gaia-study/Dockerfile\n  role: build_config\nversion: '0.5'\n\n\nAVAILABLE GAIA IDIOMS (from reference implementations):\n\n### gaia-mcp\n  approval.py: classes=['ApprovalStore'], functions=[]\n  main.py: classes=[], functions=['startup_event', 'shutdown_event']\n  server.py: classes=[], functions=['get_study_service', 'health_check', 'create_app', 'dispatch_tool', '_send_discord_message_impl', '_ai_write_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_write_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl', 'jsonrpc_endpoint', 'read_root', 'request_approval', 'pending_approvals', 'approve_action']\n  tools.py: classes=[], functions=['list_tools', 'describe_tool', 'execute_tool', '_ai_write_impl', '_write_file_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_get_study_client', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl']\n  web_tools.py: classes=['SourceTrustConfig', '_RateLimiter'], functions=['_get_config', 'web_search', '_ddg_search', 'web_fetch', '_extract_content', '_extract_title_bs4', '_domain_from_url']\n\nGenerate the implementation for: gaia-study", "output": "## FILE: gaia-study/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia-study: The Subconscious - Background processing and learning.\n\nThis service handles all background and learning operations:\n- Vector index building and maintenance (SOLE WRITER)\n- Document embedd\"\"\"\n\n## FILE: gaia-study/indexer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Vector Indexer - Document embedding and index management.\n\nThis module handles:\n- Building vector indexes from document directories\n- Adding documents to existing indexes\n- Querying indexes for simila\"\"\"\nclass VectorIndexer():\n    \"\"\"Vector index builder and manager.\n\nThis class is the SOLE WRITER to vector indexes in the GAIA SOA.\n\"\"\"\n    def __init__(self, knowledge_base_name: str, model_path: Optional[str] = None): ...\n    def instance(cls, knowledge_base_name: str) -> 'VectorIndexer': ...\n    def model(self): ...\n    def index(self) -> Dict[str, Any]: ...\n    def load_index(self) -> Dict[str, Any]: ...\n    def save_index(self) -> None: ...\n    def refresh_index(self) -> None: ...\n    def build_index_from_docs(self) -> bool: ...\n    def add_document(self, file_path: str) -> bool: ...\n    def query(self, query: str, top_k: int = 5, min_score: float = 0.0) -> List[Dict[str, Any]]: ...\n    def _compute_similarities(self, query_embedding, doc_embeddings: List[List[float]]) -> List[float]: ...\n    def doc_count(self) -> int: ...\n    def get_status(self) -> Dict[str, Any]: ...\n\n\n## FILE: gaia-study/main.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA Study Server - Entry Point\n\nBackground processing service for GAIA:\n- Vector index building and maintenance (SOLE WRITER)\n- Document embedding\n- Conversation summarization\n- LoRA adapter training\"\"\"\ndef startup_event(): ...\ndef shutdown_event(): ...\n\n## FILE: gaia-study/qlora_trainer.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"QLoRA Trainer - Actual training implementation for GAIA Self-Study\n\nUses PEFT and bitsandbytes for memory-efficient fine-tuning on consumer GPUs.\nDesigned for RTX 5080 16GB but adaptable to other conf\"\"\"\nclass QLoRAConfig():\n    \"\"\"Configuration for QLoRA training.\"\"\"\n    def __post_init__(self): ...\n    def from_dict(cls, config: Dict[str, Any]) -> 'QLoRAConfig': ...\n\nclass TrainingProgress():\n    \"\"\"Progress information during training.\"\"\"\n\nclass QLoRATrainer():\n    \"\"\"Handles the actual QLoRA training process.\n\nManages model loading, quantization, training loop, and \"\"\"\n    def __init__(self, base_model_path: str, config: QLoRAConfig, output_dir: str, progress_callback: Optional[Callable[[TrainingProgress], None]] = None): ...\n    def setup(self) -> bool: ...\n    def _count_parameters(self) -> Tuple[int, int]: ...\n    def prepare_dataset(self, samples: List[Dict[str, str]], format_type: str = 'instruction') -> Any: ...\n    def train(self, train_dataset: Any, adapter_name: str, timeout_seconds: int = 600) -> Tuple[bool, Dict[str, Any]]: ...\n    def save_adapter(self, adapter_name: str, metadata: Optional[Dict[str, Any]] = None) -> Path: ...\n    def cleanup(self): ...\n\ndef _lazy_import(): ...\ndef estimate_vram_usage(model_params_billions: float, lora_rank: int = 8, batch_size: int = 1, seq_length: int = 512, use_4bit: bool = True) -> Dict[str, float]: ...\n\n## FILE: gaia-study/server.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"GAIA Study Server - FastAPI Application\n\nBackground processing API for vector indexing, document management,\nand LoRA adapter training (Study Mode).\"\"\"\nclass IndexBuildRequest(BaseModel):\n    \"\"\"Request to build/rebuild a vector index.\"\"\"\n\nclass DocumentAddRequest(BaseModel):\n    \"\"\"Request to add a document to the index.\"\"\"\n\nclass QueryRequest(BaseModel):\n    \"\"\"Request to query the vector index.\"\"\"\n\nclass StudyStartRequest(BaseModel):\n    \"\"\"Request to start a study/training session.\"\"\"\n\nclass AdapterLoadRequest(BaseModel):\n    \"\"\"Request to load an adapter.\"\"\"\n\nclass AdapterDeleteRequest(BaseModel):\n    \"\"\"Request to delete an adapter.\"\"\"\n\ndef get_study_manager() -> StudyModeManager: ...\ndef create_app() -> FastAPI: ...\n\n## FILE: gaia-study/study_mode_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"StudyModeManager - GAIA Self-Study System\n\nOrchestrates the process of:\n1. Pausing inference\n2. Preparing training data from source documents\n3. Running QLoRA training\n4. Loading the resulting adapter\"\"\"\nclass TrainingConfig():\n    \"\"\"Configuration for a training run.\"\"\"\n\nclass TrainingResult():\n    \"\"\"Result of a training run.\"\"\"\n\nclass StudyModeManager():\n    \"\"\"Manages GAIA's self-study capabilities.\n\nCoordinates the process of training LoRA adapters from sour\"\"\"\n    def __init__(self, config: Dict[str, Any], adapter_base_dir: str = '/models/lora_adapters'): ...\n    def validate_content(self, content: str) -> Tuple[bool, str]: ...\n    def prepare_training_data(self, source_documents: List[str], output_format: str = 'instruction') -> Tuple[List[Dict[str, str]], Dict[str, Any]]: ...\n    def _create_instruction_samples(self, content: str, doc_name: str) -> List[Dict[str, str]]: ...\n    def _create_completion_samples(self, content: str) -> List[Dict[str, str]]: ...\n    def _split_into_sections(self, content: str) -> List[str]: ...\n    def start_training(self, config: TrainingConfig, model_pool: Any = None) -> TrainingResult: ...\n    def _run_qlora_training(self, samples: List[Dict[str, str]], config: TrainingConfig, model_pool: Any) -> Tuple[Path, float, int]: ...\n    def _run_real_qlora_training(self, samples: List[Dict[str, str]], config: TrainingConfig, adapter_dir: Path, base_model_path: str) -> Tuple[Path, float, int]: ...\n    def _run_simulated_training(self, samples: List[Dict[str, str]], config: TrainingConfig, adapter_dir: Path) -> Tuple[Path, float, int]: ...\n    def _get_tier_directory(self, tier: int) -> Path: ...\n    def _count_adapters_in_tier(self, tier_dir: Path) -> int: ...\n    def _save_adapter_metadata(self, config: TrainingConfig, adapter_path: Path, training_metadata: Dict[str, Any], final_loss: float, steps: int, duration: float, samples: int) -> Path: ...\n    def get_status(self) -> Dict[str, Any]: ...\n    def cancel_training(self) -> bool: ...\n    def list_adapters(self, tier: Optional[int] = None) -> List[Dict[str, Any]]: ...\n    def delete_adapter(self, adapter_name: str, tier: int) -> bool: ...\n\n\n## FILE: gaia-study/training_utils.py\n# AST Summary (substitute with actual source at training time)\ndef get_base_model_name(config: Config) -> Optional[str]: ...\ndef check_for_training_delta(config: Config) -> Tuple[bool, int, str]: ...\ndef get_next_model_version(config: Config, full_retrain: bool) -> str: ...\ndef convert_to_gguf(model_path: str, output_path: str): ...\ndef update_training_log(config: Config, new_entries: list, new_model_name: str): ...\n", "fidelity": 0.65, "weight": 1.0}
{"pair_id": "7e0f48a7-22e3-4231-aea0-5ac9efe58526", "service_id": "gaia-web", "pair_type": "retroactive", "instruction": "You are GAIA's code-architect. Generate Python source code that faithfully\nimplements the following blueprint. Your output must satisfy all contract,\ndependency, failure mode, and intent specifications exactly.\n\nBLUEPRINT:\narchitecture:\n  components:\n  - consumes_interfaces: []\n    description: 'FastAPI application serving the Mission Control dashboard, REST\n      API, and proxy endpoints. Routes user input to gaia-core, serves static files,\n      proxies orchestrator/core status for the dashboard, and handles presence updates\n      from gaia-core for Discord status sync.\n\n      '\n    exposes_interfaces:\n    - process_input\n    - health\n    - root\n    - presence\n    - output_route\n    - queue_status\n    id: http_gateway\n    key_classes: []\n    key_functions:\n    - process_user_input(user_input)\n    - update_presence(body)\n    - output_router(packet)\n    - system_status_proxy()\n    - system_sleep_proxy()\n    - dashboard_redirect()\n    - queue_status()\n    label: HTTP Gateway\n    source_files:\n    - gaia-web/gaia_web/main.py\n  - consumes_interfaces: []\n    description: 'Discord.py bot integration. Manages the WebSocket connection to\n      Discord, handles incoming messages, splits long responses to fit Discord''s\n      2000-char limit, and updates bot presence based on GAIA''s cognitive state (active/asleep/drowsy).\n\n      '\n    exposes_interfaces: []\n    id: discord_gateway\n    key_classes:\n    - DiscordInterface\n    key_functions:\n    - start_discord_bot()\n    - stop_discord_bot()\n    - is_bot_ready()\n    - DiscordInterface._split_message()\n    label: Discord Gateway\n    source_files:\n    - gaia-web/gaia_web/discord_interface.py\n  - consumes_interfaces: []\n    description: 'Buffers incoming messages when GAIA is asleep or drowsy. Sends wake\n      signals to gaia-core, waits for active state, then drains the queue. Prevents\n      message loss during sleep transitions and provides queue depth visibility via\n      status endpoint.\n\n      '\n    exposes_interfaces: []\n    id: message_queue\n    key_classes:\n    - QueuedMessage\n    - MessageQueue\n    key_functions:\n    - MessageQueue.enqueue()\n    - MessageQueue.dequeue()\n    - MessageQueue.wait_for_active()\n    - MessageQueue._send_wake_signal()\n    label: Sleep-Aware Message Queue\n    source_files:\n    - gaia-web/gaia_web/queue/message_queue.py\n  - consumes_interfaces: []\n    description: 'Read-only REST endpoints exposing the blueprint graph, service list,\n      detail JSON, and rendered markdown. Powers the Mission Control dashboard''s\n      graph visualization and blueprint detail panel. Loads from both live and candidate\n      blueprint directories.\n\n      '\n    exposes_interfaces:\n    - blueprint_graph\n    - blueprint_list\n    - blueprint_detail\n    - blueprint_markdown\n    id: blueprint_api\n    key_classes: []\n    key_functions:\n    - get_graph()\n    - list_blueprints()\n    - get_blueprint_detail()\n    - get_blueprint_markdown()\n    label: Blueprint API\n    source_files:\n    - gaia-web/gaia_web/routes/blueprints.py\n  edges:\n  - data_flow: bot_token, core_endpoint\n    from_component: http_gateway\n    label: starts/stops bot on app lifecycle\n    to_component: discord_gateway\n    transport: function_call\n  - data_flow: user messages, sleep state\n    from_component: http_gateway\n    label: creates queue, checks state before routing\n    to_component: message_queue\n    transport: function_call\n  - data_flow: QueuedMessage (content, channel, author)\n    from_component: discord_gateway\n    label: enqueues messages when asleep/drowsy\n    to_component: message_queue\n    transport: function_call\n  - data_flow: user_input text\n    from_component: discord_gateway\n    label: forwards messages to process_user_input\n    to_component: http_gateway\n    transport: function_call\n  - data_flow: \"QueuedMessage \\u2192 process_user_input\"\n    from_component: message_queue\n    label: drains queued messages after wake\n    to_component: http_gateway\n    transport: function_call\ndependencies:\n  external_apis:\n  - name: discord\n    purpose: bot_messaging_gateway\n    required: false\n  services:\n  - fallback: null\n    id: gaia-core\n    required: true\n    role: cognitive_processing\n  volumes:\n  - access: ro\n    mount_path: /gaia-common\n    name: gaia-common\n    purpose: Shared library (CognitionPacket, protocols)\n  - access: ro\n    mount_path: /knowledge\n    name: knowledge\n    purpose: Static knowledge base, blueprints, semantic codex\nfailure_modes:\n- auto_recovers: false\n  condition: gaia-core unavailable\n  response: HTTP 503/504 returned to client; timeout after 300s\n  severity: fatal\n- auto_recovers: false\n  condition: Discord bot token missing or invalid\n  response: Discord integration disabled; web input processing still works\n  severity: degraded\n- auto_recovers: true\n  condition: Discord bot websocket down\n  response: /output_router returns 503 for Discord sends; bot auto-reconnects via\n    discord.py\n  severity: degraded\n- auto_recovers: true\n  condition: Sleep/wake queue timeout\n  response: wait_for_active times out; user gets timeout error\n  severity: degraded\n- auto_recovers: false\n  condition: Discord message send fails (permissions, deleted channel)\n  response: /output_router returns 500; response lost (not retried)\n  severity: degraded\nid: gaia-web\nintent:\n  cognitive_role: The Face\n  design_decisions:\n  - FastAPI + uvicorn for concurrent Discord and web traffic\n  - Discord bot runs in background thread (separate event loop) to avoid blocking\n    HTTP handlers\n  - Sleep-aware message queue holds incoming messages while GAIA sleeps, sends wake\n    signal, waits for active\n  - 'Direct Discord bot control: /presence and /output_router directly manipulate\n    discord.py bot'\n  - Message splitting respects Discord's 2000-char limit while preferring newline/word\n    boundaries\n  - gaia-core is the single source of truth for sleep state; web polls rather than\n    caches\n  - 'Graceful degradation: Discord unavailability doesn''t block web API'\n  open_questions:\n  - Should message queue persistence survive service restart, or is in-memory-only\n    acceptable?\n  - Is MessageQueue polling interval (1.5s default) optimal, or should it be configurable?\n  - Should /output_router handle async retries for failed Discord sends?\n  purpose: 'The user-facing gateway of GAIA. Handles all external interactions: web\n    form submissions, Discord messaging (mentions + DMs), and output routing back\n    to endpoints. Integrates sleep/wake coordination: enqueues messages while GAIA\n    sleeps, wakes on first message, polls for active state before returning response.\n\n    '\ninterfaces:\n- description: Container health check endpoint.\n  direction: inbound\n  id: health\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /health\n    type: http_rest\n- description: \"Root endpoint \\u2014 returns service info and available endpoints\\\n    \\ for discovery.\"\n  direction: inbound\n  id: root\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /\n    type: http_rest\n- description: \"Message queue telemetry \\u2014 queued count, wake signal state, oldest\\\n    \\ message age.\"\n  direction: inbound\n  id: queue_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /queue/status\n    type: http_rest\n- description: 'Primary user input entry point. Receives text from web/Discord, converts\n    to CognitionPacket, sends to gaia-core. Sleep-aware: enqueues + wakes + polls\n    if GAIA is asleep.'\n  direction: inbound\n  id: process_user_input\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /process_user_input\n    type: http_rest\n- description: Update Discord bot presence (status dot + activity text). Called by\n    gaia-core's SleepCycleLoop.\n  direction: inbound\n  id: presence\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /presence\n    type: http_rest\n- description: Route completed packets to output destinations (Discord channels, DMs,\n    logs). Called by gaia-core after response generation.\n  direction: inbound\n  id: output_router\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /output_router\n    type: http_rest\n- description: Send CognitionPacket to gaia-core for cognitive processing.\n  direction: outbound\n  id: core_process_packet\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /process_packet\n    type: http_rest\n- description: Check gaia-core sleep state before enqueueing messages.\n  direction: outbound\n  id: core_sleep_distracted_check\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /sleep/distracted-check\n    type: http_rest\n- description: Send wake signal to gaia-core. Triggers ASLEEP -> WAKING transition.\n  direction: outbound\n  id: core_sleep_wake\n  status: active\n  transport:\n    input_schema: null\n    method: POST\n    output_schema: null\n    path: /sleep/wake\n    type: http_rest\n- description: Poll gaia-core sleep status in wait_for_active loop.\n  direction: outbound\n  id: core_sleep_status\n  status: active\n  transport:\n    input_schema: null\n    method: GET\n    output_schema: null\n    path: /sleep/status\n    type: http_rest\n- description: \"discord.py bot instance \\u2014 sends/receives Discord messages in\\\n    \\ background thread.\"\n  direction: outbound\n  id: discord_bot\n  status: active\n  transport:\n    symbol: gaia_web.discord_interface.DiscordInterface\n    type: direct_call\nmeta:\n  blueprint_version: '0.2'\n  confidence:\n    contract: high\n    dependencies: high\n    failure_modes: medium\n    intent: medium\n    runtime: high\n  created_at: '2026-02-21T15:55:35.806601Z'\n  divergence_score: null\n  generated_by: manual_seed\n  genesis: true\n  last_reflected: null\n  promoted_at: null\n  reflection_notes: null\n  schema_version: '1.0'\n  status: candidate\nrole: The Face (Web/Discord Gateway)\nruntime:\n  base_image: python:3.11-slim\n  compose_service: gaia-web\n  dockerfile: gaia-web/Dockerfile\n  gpu: false\n  gpu_count: null\n  health_check: curl -f http://localhost:6414/health\n  port: 6414\n  security: null\n  startup_cmd: uvicorn gaia_web.main:app --host 0.0.0.0 --port 6414\n  user: ${UID}:${GID}\nservice_status: live\nsource_files:\n- file_type: python\n  path: gaia-web/gaia_web/main.py\n  role: entrypoint\n- file_type: python\n  path: gaia-web/gaia_web/discord_interface.py\n  role: discord_gateway\n- file_type: python\n  path: gaia-web/gaia_web/queue/message_queue.py\n  role: sleep_wake_queue\n- file_type: dockerfile\n  path: gaia-web/Dockerfile\n  role: build_config\nversion: '0.5'\n\n\nAVAILABLE GAIA IDIOMS (from reference implementations):\n\n### gaia-orchestrator\n  config.py: classes=['OrchestratorConfig'], functions=['load_yaml_config', 'get_config', 'reset_config']\n  docker_manager.py: classes=['DockerManager'], functions=[]\n  gpu_manager.py: classes=['GPUManager'], functions=[]\n  handoff_manager.py: classes=['HandoffManager'], functions=[]\n  health_watchdog.py: classes=['HealthWatchdog'], functions=[]\n  main.py: classes=[], functions=['lifespan', 'health_check', 'root', 'get_status', 'get_gpu_status', '_acquire_gpu_inner', 'acquire_gpu', 'release_gpu', 'wait_for_gpu', 'get_container_status', 'stop_live_stack', 'start_live_stack', 'stop_candidate_stack', 'start_candidate_stack', 'swap_service', 'handoff_prime_to_study', 'handoff_study_to_prime', 'get_handoff_status', 'gpu_sleep', 'gpu_wake', 'notify_oracle_fallback', 'websocket_notifications', 'main']\n  models/schemas.py: classes=['GPUAcquireRequest', 'GPUAcquireResponse', 'GPUMemoryInfo', 'GPUStatus', 'ServiceHealth', 'ContainerStatus', 'ContainerStartRequest', 'ContainerSwapRequest', 'HandoffRequest', 'HandoffStatus', 'OracleNotification', 'Notification', 'OrchestratorState'], functions=[]\n  notification_manager.py: classes=['NotificationManager'], functions=[]\n  state.py: classes=['StateManager'], functions=['get_state_manager', 'reset_state_manager']\n\n### gaia-mcp\n  approval.py: classes=['ApprovalStore'], functions=[]\n  main.py: classes=[], functions=['startup_event', 'shutdown_event']\n  server.py: classes=[], functions=['get_study_service', 'health_check', 'create_app', 'dispatch_tool', '_send_discord_message_impl', '_ai_write_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_write_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl', 'jsonrpc_endpoint', 'read_root', 'request_approval', 'pending_approvals', 'approve_action']\n  tools.py: classes=[], functions=['list_tools', 'describe_tool', 'execute_tool', '_ai_write_impl', '_write_file_impl', '_list_dir_impl', '_list_files_impl', '_list_tree_impl', '_read_file_impl', '_memory_status_impl', '_memory_query_impl', '_memory_rebuild_index_impl', '_find_files_impl', '_find_relevant_documents', '_fragment_write_impl', '_fragment_read_impl', '_fragment_assemble_impl', '_fragment_list_pending_impl', '_fragment_clear_impl', '_get_study_client', '_study_start_impl', '_study_status_impl', '_study_cancel_impl', '_adapter_list_impl', '_adapter_load_impl', '_adapter_unload_impl', '_adapter_delete_impl', '_adapter_info_impl']\n  web_tools.py: classes=['SourceTrustConfig', '_RateLimiter'], functions=['_get_config', 'web_search', '_ddg_search', 'web_fetch', '_extract_content', '_extract_title_bs4', '_domain_from_url']\n\nGenerate the implementation for: gaia-web", "output": "## FILE: gaia-web/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia-web: The Face - UI and API gateway.\n\nThis service handles all user-facing interactions:\n- HTTP/REST API endpoints\n- Server-Sent Events (SSE) streaming\n- WebSocket connections\n- Static file servin\"\"\"\n\n## FILE: gaia-web/discord_interface.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Discord Interface for GAIA Web Gateway\n\nThis module handles Discord bot integration as part of the Unified Interface Gateway.\nDiscord messages are received here, converted to CognitionPackets, sent to\"\"\"\nclass DiscordInterface():\n    \"\"\"Discord interface for the GAIA Web Gateway.\n\nHandles:\n- Receiving messages from Discord (mentions an\"\"\"\n    def __init__(self, bot_token: str, core_endpoint: str, message_queue = None, core_fallback_endpoint: str = ''): ...\n    def start(self): ...\n    def _handle_message(self, content: str, channel_id: str, user_id: str, guild_id: Optional[str], author_name: str, message_id: str, is_dm: bool, message_obj: Any): ...\n    def _send_response(self, message_obj: Any, content: str, is_dm: bool): ...\n    def _split_message(self, content: str, max_length: int = 2000) -> list: ...\n\ndef _run_on_bot_loop(coro, timeout: float = 30.0): ...\ndef send_to_channel(channel_id: str, content: str, reply_to_message_id: Optional[str] = None) -> bool: ...\ndef send_to_user(user_id: str, content: str) -> bool: ...\ndef start_discord_bot(bot_token: str, core_endpoint: str, message_queue = None, voice_manager = None, core_fallback_endpoint: str = '') -> bool: ...\ndef stop_discord_bot(): ...\ndef is_bot_ready() -> bool: ...\ndef get_discord_status() -> Dict[str, Any]: ...\ndef change_presence_from_external(activity_name: str, status_str: str | None = None): ...\n\n## FILE: gaia-web/main.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"gaia-web FastAPI application entry point.\n\nProvides the HTTP API gateway for the GAIA system.\nThis is The Face - UI and API gateway.\"\"\"\ndef _load_constants() -> dict: ...\ndef health_check(): ...\ndef queue_status(): ...\ndef root(): ...\ndef dashboard_redirect(): ...\ndef system_status_proxy(): ...\ndef system_services(): ...\ndef system_sleep_proxy(): ...\ndef process_user_input(user_input: str): ...\ndef process_audio_input(body: Dict[str, Any]): ...\ndef update_presence(body: Dict[str, Any]): ...\ndef output_router(packet: Dict[str, Any]): ...\ndef startup_event(): ...\ndef shutdown_event(): ...\n\n## FILE: gaia-web/queue/message_queue.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Message queue for the sleep/wake cycle.\n\nMessages arrive via Discord (or other sources) while GAIA is sleeping.\nThey are held here until gaia-core wakes and pulls them.  The first\nenqueue triggers a w\"\"\"\nclass QueuedMessage():\n    \"\"\"A message waiting to be processed.\"\"\"\n    def to_dict(self) -> Dict[str, Any]: ...\n    def from_dict(cls, d: Dict[str, Any]) -> QueuedMessage: ...\n\nclass MessageQueue():\n    \"\"\"Thread-safe async message queue for sleep/wake cycle.\n\nPersists messages to a JSON file so they surv\"\"\"\n    def __init__(self, core_url: str = 'http://gaia-core:6415', queue_file: str | None = None) -> None: ...\n    def _load_from_disk(self) -> None: ...\n    def _persist_to_disk(self) -> None: ...\n    def enqueue(self, message: QueuedMessage) -> bool: ...\n    def dequeue(self) -> Optional[QueuedMessage]: ...\n    def peek(self) -> Optional[QueuedMessage]: ...\n    def get_queue_status(self) -> Dict[str, Any]: ...\n    def wait_for_active(self, poll_interval: float = 1.5, timeout: float = 120.0) -> bool: ...\n    def _send_wake_signal(self) -> None: ...\n\n\n## FILE: gaia-web/routes/__init__.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Route modules for gaia-web.\"\"\"\n\n## FILE: gaia-web/routes/blueprints.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Blueprint API endpoints for gaia-web.\n\nExposes the blueprint graph, service list, detail, and markdown rendering\nover HTTP. All data sourced from gaia_common.utils.blueprint_io.\"\"\"\ndef get_graph(include_candidates: bool = True): ...\ndef list_blueprints(): ...\ndef get_blueprint_detail(service_id: str, candidate: Optional[bool] = None): ...\ndef get_blueprint_markdown(service_id: str, candidate: Optional[bool] = None): ...\ndef get_component_topology(service_id: str, candidate: Optional[bool] = None): ...\n\n## FILE: gaia-web/routes/files.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"File browser routes for Mission Control dashboard.\n\nProvides directory listing and file reading with path traversal\nprotection and configurable root directories.\"\"\"\nclass WriteRequest(BaseModel):\n\ndef _parse_roots() -> dict[str, Path]: ...\ndef _safe_path(root: Path, subpath: str) -> Path: ...\ndef list_roots(): ...\ndef browse(root: str, path: str = ''): ...\ndef read_file(root: str, path: str): ...\ndef write_file(root: str, path: str, req: WriteRequest): ...\n\n## FILE: gaia-web/routes/hooks.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Hook/Command proxy routes for Mission Control dashboard.\n\nProxies sleep/wake, GPU management, and semantic codex operations\nfrom the browser to gaia-core, avoiding CORS issues.\"\"\"\nclass CodexSearchRequest(BaseModel):\n\ndef sleep_status(): ...\ndef sleep_wake(): ...\ndef sleep_shutdown(): ...\ndef gpu_status(): ...\ndef gpu_release(): ...\ndef gpu_reclaim(): ...\ndef codex_search(req: CodexSearchRequest): ...\n\n## FILE: gaia-web/routes/terminal.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Terminal routes for Mission Control dashboard.\n\nProvides container listing and WebSocket-based Docker exec bridge\nfor interactive shell access to GAIA containers.\"\"\"\ndef _get_client(): ...\ndef list_containers(): ...\ndef terminal_ws(ws: WebSocket, container: str = ''): ...\n\n## FILE: gaia-web/routes/voice.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Voice auto-answer API endpoints for gaia-web.\n\nManages the Discord voice whitelist and exposes voice connection status.\nThe VoiceManager instance is accessed via request.app.state.voice_manager.\"\"\"\nclass WhitelistAdd(BaseModel):\n\ndef _get_vm(request: Request): ...\ndef list_users(request: Request): ...\ndef list_whitelisted(request: Request): ...\ndef add_to_whitelist(request: Request, body: WhitelistAdd): ...\ndef remove_from_whitelist(request: Request, user_id: str): ...\ndef voice_status(request: Request): ...\ndef force_disconnect(request: Request): ...\n\n## FILE: gaia-web/utils/retry.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Async retry helper for gaia-web HTTP calls.\n\nProvides retry-with-backoff for outbound requests to gaia-core and other\nGAIA services. Retries on transient network errors; does NOT retry on\nclient error\"\"\"\ndef _is_maintenance_mode() -> bool: ...\ndef post_with_retry(url: str, *, json: dict, headers: dict | None = None, timeout: float = 300.0, max_attempts: int = 3, base_delay: float = 2.0, fallback_url: str | None = None) -> httpx.Response: ...\n\n## FILE: gaia-web/voice_manager.py\n# AST Summary (substitute with actual source at training time)\n\"\"\"Discord Voice Manager — auto-answer, VAD, and audio pipeline.\n\nManages voice connections for GAIA's Discord bot. When a whitelisted user\njoins a voice channel, GAIA auto-joins and enters a listen-tran\"\"\"\nclass VoiceWhitelist():\n    \"\"\"Persistent whitelist of Discord users whose calls GAIA auto-answers.\n\nAlso tracks all users GAIA has\"\"\"\n    def __init__(self, data_dir: str = '/app/data') -> None: ...\n    def _load(self) -> None: ...\n    def _save(self) -> None: ...\n    def add(self, user_id: str) -> None: ...\n    def remove(self, user_id: str) -> None: ...\n    def is_whitelisted(self, user_id: str) -> bool: ...\n    def get_whitelisted(self) -> list[str]: ...\n    def record_seen(self, user_id: str, name: str, guild_id: str | None = None) -> None: ...\n    def get_seen_users(self) -> list[dict]: ...\n\nclass SimpleVAD():\n    \"\"\"Energy-based voice activity detection with webrtcvad fallback.\n\nSegments continuous audio into utter\"\"\"\n    def __init__(self, silence_threshold_ms: int = 800, min_speech_ms: int = 300, max_utterance_seconds: int = 30) -> None: ...\n    def _init_vad(self) -> None: ...\n    def feed_frame(self, frame_16k_mono: bytes) -> bytes | None: ...\n    def _detect_speech(self, frame: bytes) -> bool: ...\n    def _flush(self) -> bytes: ...\n    def reset(self) -> None: ...\n\nclass GaiaVoiceSink(_DiscordSinkBase):\n    \"\"\"py-cord voice sink that streams decoded audio into an asyncio.Queue.\n\nEach ``write()`` call from the\"\"\"\n    def __init__(self, queue: asyncio.Queue, loop: asyncio.AbstractEventLoop, target_user_id: int | None = None) -> None: ...\n    def _enqueue(self, data: bytes) -> None: ...\n    def write(self, data: bytes, user: int) -> None: ...\n    def cleanup(self) -> None: ...\n\nclass VoiceManager():\n    \"\"\"Manages GAIA's Discord voice connections and the audio pipeline.\n\nHandles auto-joining when whitelis\"\"\"\n    def __init__(self, core_endpoint: str, audio_endpoint: str, whitelist: VoiceWhitelist, voice_config: dict | None = None) -> None: ...\n    def get_status(self) -> dict: ...\n    def handle_voice_state_update(self, member: discord.Member, before: discord.VoiceState, after: discord.VoiceState) -> None: ...\n    def _notify_core_voice_state(self, connected: bool) -> None: ...\n    def _get_core_state(self) -> str | None: ...\n    def _send_wake_signal(self) -> None: ...\n    def _join_channel(self, channel: discord.VoiceChannel) -> None: ...\n    def disconnect(self) -> None: ...\n    def _recording_finished(self, sink, *args) -> None: ...\n    def _process_audio_loop(self) -> None: ...\n    def _process_utterance(self, pcm_16k_mono: bytes) -> None: ...\n    def _transcribe(self, pcm_16k_mono: bytes) -> str | None: ...\n    def _get_response(self, text: str) -> str | None: ...\n    def _get_lite_stalling_response(self, text: str) -> str | None: ...\n    def _speak(self, text: str) -> None: ...\n\ndef pcm_48k_stereo_to_16k_mono(pcm_data: bytes) -> bytes: ...\ndef pcm_48k_stereo_to_16k_mono_fast(pcm_data: bytes) -> bytes: ...\ndef pcm_to_wav_base64(pcm_16k_mono: bytes, sample_rate: int = 16000) -> str: ...\n", "fidelity": 0.45000000000000007, "weight": 1.0}
